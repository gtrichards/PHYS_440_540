{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 7: Problems\n",
    "## Due Wednesday 9 November, by 11:59pm\n",
    "\n",
    "### PHYS 440/540, Fall 2022\n",
    "https://github.com/gtrichards/PHYS_440_540/\n",
    "\n",
    "This week you will work through the various regression techniques that we talked about this week using a single example.  The assignment is to fill in the blanks in the code cells below.  Where I have asked a question that requires an answer in words, I'm just looking for ~1 sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a 2-D data set that is close to, but isn't quite linear.  We'll generate 500 sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=500\n",
    "D=2\n",
    "\n",
    "X = 10*np.random.random((____,____)) # N points in D dimensions\n",
    "\n",
    "dy = np.random.normal(loc=0,scale=2.0,size=____) # add heteroscedastic errors\n",
    "\n",
    "#Simulate a distribution that isn't quite linear, but close.  Adding some noise\n",
    "y = 50 + 0.5*X[:,0]**2 + -0.8*X[:,1]**2 + dy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see what the data look like.  We have 2 inputs so we need 2, 1-D plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(10, 5))\n",
    "ax[0].scatter(X[:,0],y,s=50)\n",
    "ax[0].set_xlabel('X[0]')\n",
    "ax[0].set_ylabel('y')\n",
    "\n",
    "#add the 2nd plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try fitting it with plain vanilla linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = ____()\n",
    "____.____(____, ____)\n",
    "slopes = ____.coef_\n",
    "intercept = ____.____\n",
    "print(intercept,slopes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some of those trends.  We first need a grid of X values for the sake of plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xgrid0 = np.linspace(X[:,0].min(),X[:,0].max(),50)\n",
    "ypred0 = slopes[0]*Xgrid0 + intercept\n",
    "\n",
    "Xgrid1 = ____ #Complete\n",
    "ypred1 = ____ #Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the data and the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig,ax = plt.subplots(1,2,figsize=(15, 5))\n",
    "ax[0].scatter(X[:,0],y,s=50)\n",
    "ax[0].plot(____,____,\"r-\")\n",
    "ax[0].set_xlabel('X[0]')\n",
    "ax[0].set_ylabel('y')\n",
    "\n",
    "#add the 2nd plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why might the lines not go through the data?  The slopes look about right, but not the intercepts...  (Hint: Save this for after you've made the 3-D plot below.)\n",
    "\n",
    "\n",
    "**Put 1-sentence answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead let's make a new two-dimensional prediction meshgrid `Xgrid` over the $2$-D feature space, use your best-fit model to predict the labels, and make a $3$-D interactive plot showing the data and your best-fit over the entire space.\n",
    "\n",
    "**Note that the 3-D plotting will NOT work in Jupyter Lab.  Just Jupyter Notebooks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uu, vv = np.meshgrid(Xgrid0, Xgrid1)\n",
    "Xgrid = np.array([uu.flatten(), vv.flatten()]).T\n",
    "print(Xgrid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = ____.____(____)\n",
    "print(ypred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Makes the plot interactive.  You can rotate it\n",
    "#Sometimes I have to run this cell twice\n",
    "%matplotlib notebook  \n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(X[:,0], X[:,1], y, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "ax.scatter(uu.flatten(), vv.flatten(), ypred, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "ax.set_xlabel('X[0]', fontsize=12)\n",
    "ax.set_ylabel('X[1]', fontsize=12)\n",
    "ax.set_zlabel('Target', fontsize=12)\n",
    "\n",
    "ax.view_init(elev=28, azim=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn off \"notebook\" plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now attempt a `GridSearchCV` procedure on the data to find the best `Ridge` regression hyper-parameters. Use the following `param_grid`:\n",
    "```\n",
    "ridge_params = {'alpha': [0.05,0.1,0.2,0.5,1.0], 'solver': ['svd', 'lsqr']}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_params = {\n",
    "____,\n",
    "____\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeGrid = GridSearchCV(____, param_grid = ____)\n",
    "ridgeGrid.fit(____,____,sample_weight=1/dy**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the `best_params_` and the `best_score_` from this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(____.____)\n",
    "print(____.____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the data with the best options, and visualize interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeReg = Ridge(alpha=____,solver=____)\n",
    "ridgeReg.____(____,____,sample_weight=1/dy**2)\n",
    "ypredRidge = ____.____(____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(____, _____, ____, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "ax.scatter(uu.flatten(), vv.flatten(), ____, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "ax.set_xlabel('X[0]', fontsize=12)\n",
    "ax.set_ylabel('X[1]', fontsize=12)\n",
    "ax.set_zlabel('Target', fontsize=12)\n",
    "\n",
    "ax.view_init(elev=28, azim=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn off \"notebook\" plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try `RidgeCV` also; what cross-validated `alpha_` does it return, and does this agree with `GridSeachCV`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ____\n",
    "\n",
    "ridgeCv = ____\n",
    "ridgeCv.fit(____,_____,sample_weight=1/dy**2)\n",
    "ridgeCv.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Ridge Regression in this case doesn't really make sense.   Why not?  \n",
    "\n",
    "**Put ~1 sentence answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform Nadaraya-Watson regression on the data, using a Gaussian kernel with a bandwidth of $0.2$. Visualize interactively as before. How does this look compared to previous models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.linear_model import ____\n",
    "nwreg = ____(____,____)\n",
    "nwreg.____(____,____)\n",
    "ypredNW = nwreg.____(____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "#Fill in from 3-D plot cells above and modify as needed.\n",
    "\n",
    "ax.view_init(elev=28, azim=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn off \"notebook\" plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this look compared to previous models?\n",
    "    \n",
    "***Put ~1 sentence answer here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now model the data using Gaussian process regression and the default kernel. Check what the fit looks like with and with and without uncertainties accounted for. *(Hint: GPR in `sklearn` takes an `alpha` parameter equal to `(dy/y)**2`.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "gp = ____(alpha=(dy/y)**2)  #Do once with errors\n",
    "#gp = ____()  #And once without\n",
    "gp.fit(____,____)\n",
    "ypredGP, dypredGP = gp.____(____, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(X[:,0], X[:,1], y, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "ax.scatter(uu.flatten(), vv.flatten(), ypredGP, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "ax.set_xlabel('X[0]', fontsize=12)\n",
    "ax.set_ylabel('X[1]', fontsize=12)\n",
    "ax.set_zlabel('Target', fontsize=12)\n",
    "\n",
    "ax.view_init(elev=28, azim=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What difference do you see when including/excluding the uncertinties?\n",
    "    \n",
    "***Put ~1 sentence answer here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's at polynomial fitting. Fit a $4$th degree polynomial to the data. Predict on the 2D meshgrid, and visualize interactively as before. How does this look compared to the linear model and GPR models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.linear_model import PolynomialRegression\n",
    "order = 4\n",
    "poly = ____(____) # fit Nth order polynomial\n",
    "poly.____(____,____)\n",
    "ypredpoly = poly.____(____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(X[:,0], X[:,1], y, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "ax.scatter(uu.flatten(), vv.flatten(), ypredpoly, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "ax.set_xlabel('X[0]', fontsize=12)\n",
    "ax.set_ylabel('X[1]', fontsize=12)\n",
    "ax.set_zlabel('Target', fontsize=12)\n",
    "\n",
    "ax.view_init(elev=28, azim=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!  Note that it does the polynomial regression in both dimensions of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's figure out the best fit polynomial model, the coefficients of that model and how well we might expect to do with unknown data.\n",
    "\n",
    "First do a train-test split with a `train_size` of 80%.  Also split out a cross-validation set from the training set (leaving a somewhat smaller set to use for training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtestcv, ytrain, ytestcv = train_test_split(____, ____, train_size=____, random_state=42)\n",
    "\n",
    "____, ____, ytest, ycv = train_test_split(____,____, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import PolynomialRegression\n",
    "from astroML.linear_model import PolynomialRegression\n",
    "\n",
    "degrees = np.arange(1, 12)\n",
    "training_err = np.zeros(degrees.shape)\n",
    "crossval_err = np.zeros(degrees.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for i,d in enumerate(degrees):\n",
    "    #print(i,d)\n",
    "    poly = PolynomialRegression(degree=____)\n",
    "\n",
    "    poly.____(____,____)\n",
    "    \n",
    "    ypredTrain = poly.predict(____)\n",
    "    ypredCV = poly.predict(____)\n",
    "    \n",
    "    training_err[i] = np.sqrt(np.sum((____ - ytrain) ** 2)/len(ytrain))\n",
    "    \n",
    "    crossval_err[i] = np.sqrt(np.sum((____ - ycv) ** 2)/len(ycv))\n",
    "    \n",
    "\n",
    "BIC_train = np.sqrt(len(y)) * training_err + degrees * np.log(len(y))\n",
    "BIC_crossval = np.sqrt(len(y)) * crossval_err + degrees * np.log(len(y))\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "ax.plot(degrees, crossval_err, '--k', label='cross-validation')\n",
    "ax.plot(degrees, training_err, '-k', label='training')\n",
    "#ax.plot(degrees, 0.1 * np.ones(degrees.shape), ':k')\n",
    "\n",
    "ax.set_xlim(0, 12)\n",
    "\n",
    "#ax.set_xlim(0, 14)\n",
    "#ax.set_ylim(0, 0.8)\n",
    "\n",
    "ax.set_xlabel('polynomial degree')\n",
    "ax.set_ylabel('rms error')\n",
    "ax.legend(loc=2)\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.plot(degrees, BIC_crossval, '--k', label='cross-validation')\n",
    "ax.plot(degrees, BIC_train, '-k', label='training')\n",
    "\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 200)\n",
    "\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('polynomial degree')\n",
    "ax.set_ylabel('BIC')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use an 8th order model (so, not the best fit) to predict the error on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialRegression(____)\n",
    "poly.____(____,____)\n",
    "\n",
    "ypredTest = poly.____(____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_err = np.sqrt(np.sum((ypredTest - ytest) ** 2)/len(ytest))\n",
    "print(test_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the training error for that model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does that mean in terms of the importance of doing train-test splitting?\n",
    "\n",
    "**Put ~1 sentence answer here.**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
