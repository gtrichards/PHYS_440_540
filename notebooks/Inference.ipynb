{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inference: Classical and Bayesian\n",
    "\n",
    "G. Richards \n",
    "(2016, 2018, 2020),\n",
    "with material from Ivezic [Sections 4.0, 4.1, 4.2, 4.3, 4.5, 5.0, 5.1, 5.2], Bevington, and Leighly.  With updates to my own class from [Stephen Taylor's class at Vanderbilt](https://github.com/VanderbiltAstronomy/astr_8070_s22); note that we'll cover in just 2 lectures, what they cover in 7, so see those notebooks if you'd like more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents\n",
    "* [Statistical Inference](#one)\n",
    "* [Maximum Likelihood Estimation](#two)\n",
    "* [MLE Applied To A Homoscedastic Gaussian](#three)\n",
    "* [Quantifying Estimate Uncertainty](#four)\n",
    "* [Confidence Intervals](#five)\n",
    "* [MLE Applied To A Heteroscedastic Gaussian](#six)\n",
    "* [Aside: Cost Functions](#seven)\n",
    "* [Goodness of Fit](#eight)\n",
    "* [Bayesian Statistical Inference](#nine)\n",
    "* [Bayesian Priors](#ten)\n",
    "* [Heteroscedastic Gaussian with Bayesian Priors](#eleven)\n",
    "* [Stuff that there simply won't be time for!](#twelve) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Inference <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "Statistical **inference** is about drawing conclusions from data, specifically determining the properties of a population by data sampling.  \n",
    "\n",
    "Three examples of inference are:\n",
    "* What is the best estimate for a model parameter?\n",
    "* How confident we are about our result?\n",
    "* Are the data consistent with a particular model/hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some Terminology\n",
    "\n",
    "* We typically study the properties of some ***population*** by measuring ***samples*** from that population. \n",
    "* A ***statistic*** is any function of the sample. For example, the sample mean is a statistic. But also, \"the value of the first measurement\" is also a statistic.\n",
    "* To conclude something about the population from the sample, we develop ***estimators***. An estimator is a statistic based on observed data.\n",
    "* There are ***point*** and ***interval estimators***. Point estimators yield single-valued results (example: the position of an object), while with an interval estimator, the result would be a range of plausible values (example: confidence interval for the position of an object).\n",
    "* Measurements have **uncertainties** (not errors) and we need to account for these (sometimes they are unknown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Frequentist vs. Bayesian Inference\n",
    "\n",
    "There are two major statistical paradigms that address the statistical inference questions: \n",
    "- the **classical**, or **frequentist** paradigm,\n",
    "- the **Bayesian** paradigm.\n",
    "\n",
    "While most of statistics and machine learning is based on the classical paradigm, Bayesian techniques are being embraced by the statistical and scientific communities at an ever-increasing pace...especially in astrophysics.\n",
    "\n",
    "For more insight see [Jake VanderPlas's blog \"Frequentism and Bayesianism: A Practical Introduction](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Key differences\n",
    "- **Definition of probabilities**:\n",
    "    - In ***frequentist inference***, probabilities describe the ***relative frequency of events*** over repeated experimental trials. \n",
    "    - In ***Bayesian infernece***, probabilities instead quantify our ***subjective belief about experimental outcomes, model parameters, or even models themselves***. \n",
    "    \n",
    "In short, classical (frequentist) statistics is concerned with the frequency with which $A$  happens in identical repeats of an experiment, i.e., $p(A)$. Bayesian statistics is concerned instead with $p(A|B)$, which is how plausible it is for $A$ to happen given the knowledge that $B$ has happened (or is true).     \n",
    "    \n",
    "- **Quantifying uncertainty**:\n",
    "    - In ***frequentist inference*** we have ***confidence levels*** that describe the distribution of the measured parameter from the data around the true value.\n",
    "    - In ***Bayesian inference*** we have ***credible regions*** derived from posterior probabilitiy distributions (we'll meet these later). These encode our \"***belief spread***\" in model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "My colleague, Karen Leighly, dug up the following article, which might help one to understand the differences in these approaches in a relatively simply way.  The first 4 sections are what is relevant here.\n",
    "[Efron 1978](http://www.jstor.org/stable/2321163?seq=1#page_scan_tab_contents)\n",
    "\n",
    "I'll briefly (and perhaps too cavalierly) summarize it.\n",
    "\n",
    "Let's say that you get the results of an IQ test.  Any given test result might not give you your \"real\" IQ.  But it gives us a way to *estimate* it (and the possible range of values).  \n",
    "\n",
    "For a frequentist, the best estimator is just the average of many test results.  So, if you took 5 IQ tests and got a sample mean of 160, then that would be the estimator of your true IQ.\n",
    "\n",
    "On the other hand, a Bayesian would say: \"but wait, I know that IQ tests are normed to 100 with a standard deviation of 15 points\".  So they will use that as \"prior\" information, which is important here since 160 is a 4$\\sigma$ outlier. \n",
    "\n",
    "There's nothing mysterious about priors. It simply encodes any previous knowledge or information we have about our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Complete and execute the following cell. It will show the simple average of IQ tests, the prior distribution one would use in a Bayesian analysis, and the final Bayesian posterior \"belief\" distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Define the distributions to be plotted\n",
    "sigma_values = [___, 6.7, 1] #complete with the prior width of IQ distribution\n",
    "linestyles = ['--', '-', ':']\n",
    "mu_values = [____, 148, 160] #complete with the prior mean of IQ distribution\n",
    "labeltext = ['prior dist.', 'posterior dist.', 'observed mean']\n",
    "xplot = np.linspace(50, 200, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(10, 7.5))\n",
    "\n",
    "for sigma, ls, mu, lab in zip(sigma_values, linestyles, mu_values, labeltext):\n",
    "    # create a gaussian / normal distribution\n",
    "    dist = norm(mu, sigma)\n",
    "\n",
    "    if (sigma>1):\n",
    "        plt.plot(xplot, dist.pdf(xplot), ls=ls, c='black',label=r'%s $\\mu=%i,\\ \\sigma=%.1f$' % (lab, mu, sigma))\n",
    "    else:\n",
    "        plt.plot([159.9,160.1],[0,0.8], ls=ls, color='k', label=r'%s $\\mu=%i$' % (lab, mu))\n",
    "        \n",
    "plt.xlim(50, 200)\n",
    "plt.ylim(0, 0.1)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p(x|\\mu,\\sigma)$')\n",
    "plt.title('Gaussian Distribution')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The end result (skipping over the detailed math) is that the Bayesian estimate of the IQ is not 160, but rather 148, or more specifically that $p(141.3\\le \\mu \\le 154.7 \\, | \\, \\overline{x}=160) = 0.683$.\n",
    "\n",
    "That's actually fine, where the controvery comes in is when the Bayesian wants to do the same things but doesn't actually know the prior distribution, or when the parameter is fixed but we are trying to experimentally verify it (e.g., the speed of light)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Likelihood Estimation (MLE), Ivezic 4.2  <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "Let's not worry about classical vs. Bayesian right now and talk about maximum likelihood estimation (Ivezic, 4.2), which is relevant to both.\n",
    "\n",
    "Maximum likelihood estimation follows this blueprint:\n",
    "\n",
    "1. **Hypothesis**: Formulate a model, a *hypothesis*, about how the data are generated. For example, the data are a measurement of some quantity with Gaussian random uncertainties (i.e., each measurement is equal to the true value, plus a deviation randomly drawn from the normal distribution). Models are typically described using a set of model parameters $\\boldsymbol{\\theta}$, and written as $\\boldsymbol{M}(\\boldsymbol{\\theta})$.\n",
    "\n",
    "\n",
    "2. **Maximum Likelihood Estimation**: Search for the \"best\" model parameters $\\boldsymbol{\\theta}$ which maximize the ***likelihood*** $L(\\boldsymbol{\\theta}) \\equiv p(D|M)$. This search yields the MLE *point estimates*, $\\boldsymbol{\\theta^0}$.\n",
    "\n",
    "\n",
    "3. **Quantifying Estimate Uncertainty**: Determine the confidence region for model parameters, $\\boldsymbol{\\theta^0}$. Such a confidence estimate can be obtained analytically (possibly with some approximations), but can also be done numerically for arbitrary models using general frequentist techniques, such as bootstrap, jackknife, and cross-validation (we'll come to these later).\n",
    "\n",
    "\n",
    "4. **Hypothesis Testing**: Perform hypothesis tests as needed to make other conclusions about models and point estimates. Possibly GOTO #1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Measuring the Position of a Quasar\n",
    "\n",
    "Let's assume we wish to estimate the position, $x$, of a quasar from a series of individual astrometric measurements.\n",
    "\n",
    "1. We adopt a model where the observed quasar does not move, and has individual measurement uncertainties \n",
    "2. We derive the expression for the likelihood of there being a quasar at position $x_0$ that gives rise to our individual measurements. We find the value of $\\hat x_0$ for which our observations are maximally likely.\n",
    "3. We determine the uncertainties (confidence intervals) on our measurement.\n",
    "4. We test whether what we've observed is consistent with our adopted model. For example, is it possible that the quasar was really a misidentified star with measurable proper motion?\n",
    "\n",
    "Note: in the text to come, I will use $\\mu$ instead of $x_0$ to denote the true position of the quasar. This is to avoid potential confusion with the first (or zeroth) measurement of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we know the distribution from which our data were drawn, then we can compute the **probability** of our data.  \n",
    "\n",
    "For example if you know that your data are drawn from a model with a Gaussian distribution, then we've already seen that the probablity of getting a specific value of $x$ is given by\n",
    "$$p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "#------------------------------------------------------------\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(10, 7.5))\n",
    "dist = norm(5, 1)\n",
    "x = np.linspace(0, 10, 1000)\n",
    "plt.plot(x, dist.pdf(x), c='black',label=r'$\\mu=5,\\ \\sigma=1$')\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 0.5)\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p(x|\\mu=5,\\sigma=1)$')\n",
    "plt.title('Probability of $x$')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we want to know the total **likelihood** of our *entire* data set (as opposed to one measurement) then we must compute the *product* of all the individual probabilities:\n",
    "$$L \\equiv p(\\{x_i\\}|M(\\theta)) = \\prod_{i=1}^n p(x_i|M(\\theta)),$$\n",
    "where $M$ refers to the *model* and $\\theta$ refers collectively to the $k$ parameters of the model, which can be multi-dimensional.\n",
    "\n",
    "In words, this is *the probability of the data given the model*.  \n",
    "\n",
    "Note:\n",
    "- while the components of $L$ may be normalized pdfs, their product is not.\n",
    "- the product can be very small, so we often take the log of $L$. \n",
    "- we're assuming the individual measurements are independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can write this out as\n",
    "$$L = \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right),$$\n",
    "and simplify to\n",
    "$$L = \\prod_{i=1}^n \\left( \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right) \\exp\\left( -\\frac{1}{2} \\sum \\left[\\frac{-(x_i-\\mu)}{\\sigma} \\right]^2 \\right),$$\n",
    "\n",
    "where we have written the product of the exponentials as the exponential of the sum of the arguments, which will make things easier to deal with later.\n",
    "\n",
    "That is, we have done this: $$\\prod_{i=1}^n A_i \\exp(-B_i) = (A_iA_{i+1}\\ldots A_n) \\exp[-(B_i+B_{i+1}+\\ldots+B_n)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you have done $\\chi^2$ analysis (e.g., doing a linear least-squares fit), then you might notice that the argument of the exponential is just \n",
    "$$\\exp \\left(-\\frac{\\chi^2}{2}\\right).$$\n",
    "\n",
    "That is, for our gaussian distribution\n",
    "$$\\chi^2 = \\sum_{i=1}^n \\left ( \\frac{x_i-\\mu}{\\sigma}\\right)^2.$$\n",
    "\n",
    "So, maximizing the likelihood is the same as minimizing $\\chi^2$.  In both cases we are finding the most likely values of our model parameters (here $\\mu$ and $\\sigma$).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Key Idea Behind Maximum Likelihood Estimation\n",
    "\n",
    "Let's say that we know that some data were drawn from a Gaussian distribution, but we don't know the $\\theta = (\\mu,\\sigma)$ values of that distribution (i.e., the parameters), then MLE is about varying the parameters until we find the maximal value of $L$ (i.e., the **maximum likelihood**).  Those model parameters will also minimize $\\chi^2$, and will be our *Maximum Likelihood Estimators* for for the true values of the model. \n",
    "\n",
    "Simple as that.  Stop and make sure that you understand this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's [an animation of linear least squares fitting](https://yihui.org/animation/example/least-squares/).\n",
    "\n",
    "They are trying to fit a line to some data.  They start by fixing the interecept and then trying 50 different values of the slope.  The red dashed lines show the difference (*residual*) between the predicted value and the actual value.  These are squared and summed (residual sum of squares, or $\\chi^2$) and plotted as the y axis in the right hand plot.  You see $\\chi^2$ going down as the slope changes, bottoming out at the best slope (presumably with $\\chi^2 \\sim 1$, but we can't tell from the scale shown).  Then $\\chi^2$ goes back up after we have passed through the best slope.\n",
    "\n",
    "With the best slope determined, we then try different values of the intercept, choosing the one that minimizes $\\chi^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A simple example would be the likelihood of rolling a certain combination of numbers on a six-sided die.\n",
    "The probability of rolling a 3 is $1/6$ (as is the probability of *any* roll).  So, what is the probability of rolling (in no particular order): {1,1,2,3,3,3,4,5,6,6}?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print((1./6)*(1./6)*(1./6)*(1./6)*(1./6)*(1./6)*(1./6)*(1./6)*(1./6)*(1./6))\n",
    "print((1./6)**10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, even for 10 rolls of the die, the likelihood is pretty small.  That's just because there are *lots* of possible combinations of rolling a die 10 times.  This particular series of numbers is just as likely as any other.  \n",
    "\n",
    "Students who took PHYS 114 with me will recall that the result is related to the number of *combinations* ($n$ choose $r$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Write some code to compute the probability for N rolls\n",
    "import numpy as np\n",
    "N=____ #Number of rolls\n",
    "L=____ #Likelihood, initialize to unity\n",
    "rolls = np.array([])\n",
    "for i in np.arange(____): #Loop over each roll\n",
    "    #Append a single new roll to \"rolls\" between 1 and 6 (careful) to the rolls array\n",
    "    rolls = np.append(____,np.random.randint(low=____,high=____,size=____))\n",
    "    L = L*(1./6) #The likelihood of each roll is 1/6\n",
    "print(L,rolls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLE applied to a Homoscedastic Gaussian (Ivezic 4.2.3)  <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "Let's take a look at an example using a Gaussian model where all the measurements have the same error ($\\sigma$).  This is known as having **homoscedastic** errors.  Don't be intimidated by the fancy word, statisticians just like to sound smart, so they say \"homoscedastic\" instead of \"uniform errors\".  Later we will consider the case where the measurements can have different errors ($\\sigma_i$) which is called **heteroscedastic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For an experiment with data $D=\\{x_i\\}$ in 1D with Gaussian errors, we have\n",
    "$$L \\equiv p(\\{x_i\\}|\\mu,\\sigma) = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right).$$\n",
    "\n",
    "Note that that is $p(\\{x_i\\})$ not $p(x_i)$, that is the probability of the full data set, not just one measurement.\n",
    "\n",
    "If $\\sigma$ is both constant and *known*, then this is a one parameter model with $k=1$ and $\\theta_1=\\mu$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we found above, likelihoods can be really small, so let's define the **log-likelihood function** as ${\\rm lnL} = \\ln[L(\\theta)]$.  The maximum of this function happens at the same place as the maximum of $L$.  Note that any constants in $L$ have the same effect for all model parameters, so constant terms can be ignored.  \n",
    "\n",
    "In this case we then have $${\\rm lnL} = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma^2}.$$\n",
    "\n",
    "Take a second and make sure that you understand how we got there.  It might help to remember that above, we wrote\n",
    "$$L = \\prod_{i=1}^n \\left( \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right) \\exp\\left( -\\frac{1}{2} \\sum \\left[\\frac{-(x_i-\\mu)}{\\sigma} \\right]^2 \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then determine the maximum in the same way that we always do.  It is the parameter set for which the derivative of ${\\rm lnL}$ is zero:\n",
    "$$\\frac{d\\;{\\rm lnL}(\\mu)}{d\\mu}\\Biggr\\rvert_{\\hat \\mu} \\equiv 0.$$\n",
    "\n",
    "That gives $$ \\sum_{i=1}^N \\frac{(x_i - \\hat \\mu)}{\\sigma^2} = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since $\\sigma = {\\rm constant}$ (at least in this case), that says \n",
    "$$\\sum_{i=1}^N x_i = \\sum_{i=1}^N \\hat \\mu = N \\hat \\mu.$$\n",
    "\n",
    "Thus we find that\n",
    "$$\\hat \\mu = \\frac{1}{N}\\sum_{i=1}^N x_i,$$\n",
    "which is just the arithmetic mean of all the measurements.\n",
    "\n",
    "As promised last week, that's where the formula that you know and love for the mean comes from.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Sample Mean is an ML Estimator\n",
    "\n",
    "So the **sample mean** ($\\overline{x} = \\hat \\mu$) of observations drawn from a $\\mathscr{N}(\\mu, \\sigma=const)$ distribution is a maximum-likelihood **estimator** of the distribution's $\\mu$ parameter.  We'd intuitively guess that, but this derivation clarifies our choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we just discussed, in an experiment with data $D=\\{x_i\\}$ in 1D with Gaussian errors, we have\n",
    "$$L \\equiv p(\\{x_i\\}|\\mu,\\sigma) = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right).$$\n",
    "\n",
    "Let's create some data and see what the resulting likelihood looks like for some example points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Load up the algorithms we are going to need.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are going to draw a homoscedastic sample of ${x_i}$ from a Gaussian and compute the likelihood.\n",
    "\n",
    "First generate a sample of `N=3` points drawn from a normal distribution with `mu=1.0` and `sigma=0.2`: $\\mathscr{N}(\\mu,\\sigma)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N = ___ #Complete\n",
    "mu = ___\n",
    "sigma = ___ \n",
    "np.random.seed(42)\n",
    "sample = norm(___,___).rvs(___)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Think back to Lecture 2 when we did a Kernel Density Estimate.  Treat each of those observations as an estimate of the true distribution.  So, we are going to center a Gaussian (with the known $\\sigma$) at each point, this is the likelihood $p(x_i|\\mu,\\sigma)$.\n",
    "\n",
    "Plot each of the likelihoods separately.  Also plot their product.  Make the $x$ axis a grid of 1000 points uniformly sampled between $x=0$ and $x=2$.\n",
    "\n",
    "Note that, according to [scipy.stats.norm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html), `norm.pdf(x, loc, scale)` is identically equivalent to `norm.pdf(y)/scale` with `y=(x-loc)/scale$`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Make the plot and see if you get the same as me.\n",
    "xgrid = np.linspace(___,___,___)\n",
    "L1 = norm.pdf(___,loc=___,scale=___) #This is a Gaussian PDF sampled uniformly, centered at a specific location.\n",
    "L2 = norm.pdf(___,loc=___,scale=___)\n",
    "L3 = norm.pdf(___,loc=___,scale=___)\n",
    "L = ___ #Total L is ???\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "plt.plot(____, ____, ls='-', c='green', label=r'$L(x_1)$')\n",
    "plt.plot(____, ____, ls='-', c='red', label=r'$L(x_2)$')\n",
    "plt.plot(____, ____, ls='-', c='blue', label=r'$L(x_3)$')\n",
    "plt.plot(____, ____, ls='-', c='black', label=r'$L(\\{x\\})$')\n",
    "\n",
    "plt.xlim(0.2, 1.8)\n",
    "plt.ylim(0, 8.0)\n",
    "plt.xlabel('$\\mu$') #Leave out or adjust if no latex\n",
    "plt.ylabel(r'$p(x_i|\\mu,\\sigma)$') #Leave out or adjust if no latex\n",
    "plt.title('MLE for Gaussian Distribution')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {
    "MLEexample.jpg": {
     "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/2wBDAQICAgICAgUDAwUKBwYHCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgr/wAARCAFoAfgDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD9/KKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKK+X9P/wCCnPgrSv2wfjd+zH8V/AR8MaN8H/Cg1+38cS6yJotbtbfS7DUdVzB5KfZmtI9UsePMl81ZWb5NhFAH1BRXyV+yF/wU68R/tMWHwki8Y/s0zeCdX+Jeu+OdJ1XRrnxSLyTw9c+G72W0lRyLWMTtK8TZGI/KPGZMZrsfif8AtSftNJ+1xrP7Kn7OH7OngTxNJ4d+HGieLNX1zxv8Vb3QBjUr/V7OK2hhtdE1DzCn9ku7Ozp/rlAU7SSAfQlFeHft7/tZ63+xb+xf4o/aPuPBjahr+kaTGtjpOn6Zf6rarqUxWOMSm0g842yStlpCkWVUDMbOorF8Pf8ABQ74DfDzQ/D3hP8AaJ+NIfxZd6XZ3mvX9n8JPEGhWOlx3tw8VnJqVvdrct4fWVl2Iuo3EZdkYjg4AB9F0V5f4X/bI/Z88bfH7Xf2YvB/inVdT8Z+F9SFh4msbHwhqkltpFwbCG/Rbq9W2+y24kt542jaSVVlbfHGWkjdF5T4ifta/F3U/wBojW/2Zf2UfgDo/jbXPB2jWGo+OtY8XeOX8P6TpH27zWtLNJYbC+nubp44XlKLAI0jaMtKDIq0Ae90V5F41/a98K/Az4d6F4r/AGmvBPiTwvq2rW11LeaH4Y8Map4r+wi2IE0ry6RZzbLdVeN/PlSJdsg3BGDKu94f/al+APizWrbQfC3xJtNSnvfAMPjayksIJpoLnQJWKxX0cyIYpEYjhVYuRg7cEGgDv6K8Vt/+ChH7KmpXPhO08OeMPEGuN408MaV4j0ZvDfw81zU0g0rUgxsLy+e1spF0yKba+1rwwf6uTONjY19A/bI+A3jH4o6p8G/CGu69e6xpN1e2V7qcHgHWZNFhvLVGa4tjqotRYPPFscPAtx5gZGQgMCAAep0V8/J/wUV/Zr8B/AzwJ8W/il8Vb3UtL8WeAdO8THxv4c+FevrorafcW6SDU7jZBcjRbV9xkC306mJMh5GKM1aPxi/4KO/sb/ATx3qXw3+KHxXubPVdE0Ox1vXFsfCeq39vpel3bTJBqF1c2lrJDbWha3lD3MrrFFhfMZN6bgD3CivLP2zf2jNV/ZX/AGb9a+OvhrwJa+Kb7T73S7TT9Eu9bbToLqW+1G1sYy9ykFwYkVrkOWEUhwpAXmvP7X9t34neAfibL8C/2ufg5ofw117VPBGseI/B/ifw94on8U6DfQ6YsbXsbk2en3YngSeKYweQolj3+XLuQgAH0nRXimgft3fs+yal4T8CT+OdW8R6/wCItA0fUTeeEfhnrtzYxRaio+yXF08FvcR6RHOQzol7MjKn3mIUtXA/DP8A4K5fs2+IvAnxG+JPxf0zxT8P9D+HHxDuvC+p614g8C69HYS41mLSLSYXUunRReZNcTwh7dWd7cOWlIRGcAH1TRXyR+07/wAFKtD+GHwd8f8Axd+Fvi6wE/hLwV4c8QHwr49+HGtaLd6dZ33iC90uTULxr1rZvJkFncCKDyopIzaNO7yxXMIHqHhn/goL+yb4r0K91+x+I9/Zrp3irQ/Dl/Ya54P1XTb+31DWbuGz0tXs7u1juFhup541iuTH5DAs/mbEdlAPZ6K+c/2r/wDgpX8Gv2X5V0yLQNf8WapYfFDw54N8U6V4d8M6teT6RJq6RzRzhbOyuGuCLaRZFijB81ysIYSuq1ut+3J8CfDcPjPxj47+Lsdpo3hxfD+/RbrwBq9hq+my6rbxyWdpLBMpnvLu4aWMJaw2yTxs3kvGZQaAPb6K8N1X/gpD+xvonw50b4par8Ur6HTtf8bSeDtNsm8Hav8A2p/wkKWtxdnSpdNFr9tt7ww20rLBLCjuTGqhmmiV/Rfgj8dPhd+0Z8PoPih8H/Ej6no815c2bPcadcWVxbXVtO8FxbXFtdRxz208Usbo8UqI6spBUUAdbRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQByXjL4+fAr4c+N9F+GfxC+NPhLQfEniR1Tw94f1nxHa2t9qjF9gFtBLIsk5LkKAinnjrXW18A/t+/C34n+KP2rfEV74W8IfEF31zw/4Dt9E0Xw74Pl1Hw/46k0/X7y8kttZv1tpP7IjtDLvDLcWJZZ2dnu1Agj+nP27fiF8e/gz+zH40+OXwC8VeELC/wDA3hDV/EF3Z+MPCV1qsOopZ2Utwtun2bULNrcs0eDITJwfucUAexUV4z/wg/8AwUL/AOjoPgz/AOGH1b/5qKP+EH/4KF/9HQfBn/ww+rf/ADUUAezUV4z/AMIP/wAFC/8Ao6D4M/8Ahh9W/wDmoo/4Qf8A4KF/9HQfBn/ww+rf/NRQB7NRXjP/AAg//BQv/o6D4M/+GH1b/wCaij/hB/8AgoX/ANHQfBn/AMMPq3/zUUAezUV4z/wg/wDwUL/6Og+DP/hh9W/+aij/AIQf/goX/wBHQfBn/wAMPq3/AM1FAHs1FeM/8IP/AMFC/wDo6D4M/wDhh9W/+aij/hB/+Chf/R0HwZ/8MPq3/wA1FAHs1FeM/wDCD/8ABQv/AKOg+DP/AIYfVv8A5qKP+EH/AOChf/R0HwZ/8MPq3/zUUAezV8hfEb/glhofx9+OPxE+Inxl8WyWmleIPi3onizQYvDV3i4vNOtfDdhpF9pGoiWHabW7NtKJI42bdGIm3o64X1f/AIQf/goX/wBHQfBn/wAMPq3/AM1FH/CD/wDBQv8A6Og+DP8A4YfVv/mooA8G039iD9sr4M+LPAvxO+D+jfDLxTq3hT4sfFXxFd6P4l8c6jo1vNp/ijWrq+swlzBpN43nxQzoJYzCEDhgkjgBjR+NH7Cnx++OP7VT/tUfGr9gH9lv4oTar8JdD8Mz+GfiH4+urqLw7fWGra5dSyWVxP4VuTPFPBqVpuYxWzB4GUq6qjn6H/4Qf/goX/0dB8Gf/DD6t/8ANRR/wg//AAUL/wCjoPgz/wCGH1b/AOaigBn7Y/7Pvjz9pT9ivxD+z/4Sg8P6Fr+u6NZQQwS3sp02ykjngleNZUgDtGojZVYQqThcomSB5B+1R+wj+0n8UNc+O/gL4T634G/4Qj9pTQrLTPGur+I767i1XwsF01dKupbG2itpYtQ32aRtEkk1qIZ9zkyhto9i/wCEH/4KF/8AR0HwZ/8ADD6t/wDNRR/wg/8AwUL/AOjoPgz/AOGH1b/5qKAJ/wBmL9nvxR8Evid8cfGviO+02e2+JnxUg8SaF9inkkmhso/DeiaX5dyXRcS+fptw4Cl12SId25mVfPPi5+yh8c9A/aD+Ifxc+CfgX4Y/ETwr8ZPD2m6d8Sfht8VtWudOtnnsoZbZLmG4h0+/SaKa1kSGW1mttp8hGEg3Mtd5/wAIP/wUL/6Og+DP/hh9W/8Amoo/4Qf/AIKF/wDR0HwZ/wDDD6t/81FAHzZof/BNX9rj4XfBX4XfBT4Z+P8Aw+PDXhuLxVJ4k8G+GviX4h8DWFnfatrTalaXFrc6FGLq8gsY5p7VbJ2tY5VYPuiOFST4Pf8ABO79tD9m/wCF/wALdG+F3iH4aaz4i0P9nBPhX4yl8Qa3qMFrZSRSrLBqlkY7OR71UZpla1lFsXHl4mTBB+j/APhB/wDgoX/0dB8Gf/DD6t/81FH/AAg//BQv/o6D4M/+GH1b/wCaigD5wP8AwTq/a08NfDn4PeCvhSPBHhjxd4G+Fng/wrrHxh0D4o67p9/A2lRxrcQvpENj9k1+0/4+PJjvpIgv2lzsQmu90L9jz9pTSv2xLj4seDbLwp8PvBd1r2t33iq38MfE3W76PxtFd2tzFbi60G4s49P0y6FxLb3c17byyTSPbshyszEepf8ACD/8FC/+joPgz/4YfVv/AJqKP+EH/wCChf8A0dB8Gf8Aww+rf/NRQB8k/ED/AIJb/t2a3+yv4B/Zc0j4s6Dd6PoH7MGi/Du6sLb4veJvDdho/iK1sZbW71cQ6TAja/bzq1vGLa8eBES1zsbzpEr1G+/4J3fGi9+GXx58HyeJvCzX/wAU/wBlzw98NtEne9uSkWq2Gma5azTXLfZ8ratJqcLKyh3KrKTGpChvZf8AhB/+Chf/AEdB8Gf/AAw+rf8AzUUf8IP/AMFC/wDo6D4M/wDhh9W/+aigDM/bF/Zv+Kvxt/Yfn+APw0udAfxZEPDs1m2v6nPa6fPNp2pWN5Ikk8VvPLGrrauocQuQWUletcB45/ZK/a1/al+JL/F/9pFfh54Ul8M/DfxN4e+H/g/wZ4jvtYgbUdZto7ebUL7ULmws22pFCI0hjtePPlcu5CrXqX/CD/8ABQv/AKOg+DP/AIYfVv8A5qKP+EH/AOChf/R0HwZ/8MPq3/zUUAeCap+wL+1VpeofDOP4TxeDPCOteFvC/hHR9f8AitoHxP1yzvp4NLEAu7WfQ4rL7DrkLotzFC15NGY1uNwVSoB0fE/7CX7TGs/Dv4ifAG3j8Av4Y8R/tC6P8StB8RXHiW9F5JDH4x0vXr2wubEaeY42WG1uY4pUuJBK5iDpCGZ19q/4Qf8A4KF/9HQfBn/ww+rf/NRR/wAIP/wUL/6Og+DP/hh9W/8AmooA8i/bn/4J+/HH9pXxV8aPEPw08XeE9PPxD+EHgnwt4efXnuHFrf6P4i1jU7iW5jjhIMDQ6hAqBWLO6SKwjXa7ZfxR/YS/a1/aB1Xx58eviLffDnw/8RL6X4ef8IP4e0PW7++0YJ4S8Ry6/C19eSWcE268uLiaFglu32aIIVM7bt3uP/CD/wDBQv8A6Og+DP8A4YfVv/moo/4Qf/goX/0dB8Gf/DD6t/8ANRQB4Frf7Cn7aPje++I/xk8XTfDG38ZeIvjP4G8e+GPC+n+JdQfS449BhsIpNOudQfTxMpkFrLtuUtW5dSYVGUGx8Qv2Ef2lfHnxF8V/tD21/wCBtN8X3njrwN408N+HpNbvLrTTf6PpjWl9p91dfY45BC/nXCw3SQM4IimaAEGGvZf+EH/4KF/9HQfBn/ww+rf/ADUUf8IP/wAFC/8Ao6D4M/8Ahh9W/wDmooA8g8NfsFfH3Wvi94a/aK+JureDLXxHdftLr8TPGWgaJqV1cWGnWEPga88L2tnZTy2sb3lwC1nPJNLFbht02Avlxo3tX7JHwF8YfAS0+I1v4v1LTbn/AIS74t694p0v+zZpHEVneyo8Ucm9E2yjB3Ku5QTwzVU/4Qf/AIKF/wDR0HwZ/wDDD6t/81FH/CD/APBQv/o6D4M/+GH1b/5qKAPZqK8Z/wCEH/4KF/8AR0HwZ/8ADD6t/wDNRR/wg/8AwUL/AOjoPgz/AOGH1b/5qKAPZqK8Z/4Qf/goX/0dB8Gf/DD6t/8ANRR/wg//AAUL/wCjoPgz/wCGH1b/AOaigD2aivGf+EH/AOChf/R0HwZ/8MPq3/zUUf8ACD/8FC/+joPgz/4YfVv/AJqKAPZqK8Z/4Qf/AIKF/wDR0HwZ/wDDD6t/81FH/CD/APBQv/o6D4M/+GH1b/5qKAPZqK8Z/wCEH/4KF/8AR0HwZ/8ADD6t/wDNRR/wg/8AwUL/AOjoPgz/AOGH1b/5qKAPZqK8Z/4Qf/goX/0dB8Gf/DD6t/8ANRR/wg//AAUL/wCjoPgz/wCGH1b/AOaigD2aivnH4s+Kv27/AIFaPoPjvxR8b/hJr+l3HxC8K6HqmlWHwf1SwuJbXVNesNMmaK4fxFOsUiR3jOrNFIu5ACpBNfR1ABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXjP/BRv/lHp8eP+yM+KP8A003NezV4z/wUb/5R6fHj/sjPij/003NAHs1FFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABWFc6RpeueMbq31rToLuO30y2aCK5iDqjPJOGIDZAJCLz7Vu1xvxC8P8AjfxTF4h0D4cfEH/hFdbudGsVsPEH9kxX32NhPcEt5EpCSZUMuCeN2e1TJuMW0r+Xf7yZycYNpXa6Ld+Wtl97N7/hBvBP/Qn6V/4L4/8A4mj/AIQbwT/0J+lf+C+P/wCJrwv/AIZv/b7/AOklH/mHNK/+OUf8M3/t9/8ASSj/AMw5pX/xyuH67if+gaf3w/8Akzzf7Qxf/QJU++n/APLD3T/hBvBP/Qn6V/4L4/8A4muf8fax8D/hh/Yo8caXpVkfEXiC20TRlGi+a1zfz7vKhAjjYjIRyWbCqFJYgAmqfwG+Hfx2+H9pqUPxw/aL/wCFhS3UkTadN/wiNrpP2JVDb1xbsRJuJU5bpt46153+2vpHxjuPix8DvGnw9/Z78S/ELQ/BPjjUfEHiKw8K6po9vcxS/wBh3+n2mV1W/s45E36i8nyuxVoFOOldlKcqlNSlFxfZ2uvubX4nfRqTq01KUHFvo7XX3Nr7meq6Td/BjXfHWs/DXSNI0qfWfD9nZ3Os2qaQNtql153kBpNnll2EEjGMMXVTGzKqyxltv/hBvBP/AEJ+lf8Agvj/APia+PPGHwt+Mfxb0X9oz4I6f8IL+y8Q/Fvxt4e1rS7/AF66t0XwhYXPhrSbSLVJZIJ2Wa506/0W8KQWckha5ityHSOVrlOj+Bei/GX9nLwx8AvBPiPQfFun+IfEXirUofjTDpfh7+2rPW719PvzNrl9qFvbTG1E+oRWMsJeeAJDcrC8arCY4dDU+hfAt38GPiVp97qngvSNKvItN1q90m/B0gRPBeWk7wTxMkiKww6HBxtdSroWR1Y7X/CDeCf+hP0v/wAF8f8A8TXlH7IWh6zH48+O3xAuLCe20jxZ8aZrnw9FcRGMtDZaHo+j3MgVuQr32mXrKejqRIM78n2ygDwT9r0eX+zrolupOyH46eA4ogTnaiePtJVV+gUAD2Fe914L+2B/yb1pH/Ze/A3/AKsDSq96oAKKKKACiiigAooooAKKKKACiiigAooooAK8Z/4KN/8AKPT48f8AZGfFH/ppua4L9vL4fQeMvjH8IPBvgn4g+P8ARPF/jPx9YxTzeGfidrmm2troGlF9U1KWSxtbyO0kE0cKae0jxM2dSiyflUruf8FVPGniTwl+wF8X7XQPhH4h8UR6n8K/Ette3OhXOnRppER0q4BurgXl3AzRLkkiBZpcKcRk4BAPoeiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArLtP+R21D/sFWf/oy5rUrjfiDr/jfwtH4h1/4cfD/AP4SrW7bRrFrDQP7VisftjGe4BXz5QUjwpZskc7cd6mUlCLk+nz/AAW5rQozxNeNKDScmkrtRV27ayk0ku7bSS1bsdlRXzx/w0f+3z/0ja/8zFpX/wAbo/4aP/b5/wCkbX/mYtK/+N1w/wBp4b+Wf/guf/yJ9X/qPnX/AD9w3/hZhf8A5cfQ9FcB8B/iH8dfiBaalN8b/wBnb/hX0trJEunQ/wDCXWurfbVYNvbNuoEe0hRhuu7jpXjX/BRX40fB74a/Fj9nTw38bvix4a8H6Bd/Fe41zVdT8V67b6datBpmiajLCnm3Dom/+0JtOYDOTtOOa7KVSNampxvZ900/udmfNY/BVsuxcsPVcXKO7hONSOqvpODlF79G7PTdM+pKK+IfFH7QOq2Fj+0p8XfhD8QoLnx7q/jfw54f+Cb6RFb6j/wkcH/CM6TqmlafbK+6Oezubq/1Z5J1IWKGe7m82IQNMnoH7N37Vfxf8S/Cz4XeNfFMeleLNQ+KmpyzappUGofYb7wrJ5xS702CySzInh0sq0NzPc3Echlik/jlht60OQ+nqK8N/Y1uLq08efHzwbYZHh/Q/jhcJ4bUfcRbvQ9H1O/CHuP7VvtSZsdHZ16qa9yoA8F/bA/5N60j/svfgb/1YGlV71Xgv7YH/JvWkf8AZe/A3/qwNKr3qgAooooAKKKKACiiigAooooAKKKKACiiigDPufCPhS98VWfjq88MafLrenWFxZafrEtlG11a2tw8LzwRykb0jke2t2dAQrmCIsCUXHlP/BRv/lHp8eP+yM+KP/TTc17NXjP/AAUb/wCUenx4/wCyM+KP/TTc0AezUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUV8eft8/FH4uftCftBeGP8Aglt+zD8QNS8Kan4l0VvEvxo+IGgyhL7wp4QWUwrBaSdIdQ1CcNBDJy0Mcc8wXKqwAPrfT/EOgatfXWmaVrlnc3Ni4S+t7e5R3t2PIV1ByhPocVcr4d+K/wDwRL/Z3+Fvw9tfiF/wTP8ACFh8GfjX4Ltjc+CfGmj3E2NZnT52sNc3ux1O0uiuyVp98i7vMVsrhvoD9g79rfR/22P2Z9E+N9v4bm0DWzNcaT428J3bZuPDuv2UrW+oadKOu6KeNwpOC6FHwA4oA9iooooAKKKKACiiigAooooAKKKKACsu0/5HbUP+wVZ/+jLmtSuY8S+NPB3w81HWvGfj/wAWaZoej2OkWTXuq6xfx2ttbqZrhQXlkIVAWYAEkckDvV06dSrNQgm5N2SSbbb2SSTbb6JJt9gbSV2dPRXlX/Ddn7EP/R5Hwq/8OHpv/wAfo/4bs/Yh/wCjyPhV/wCHD03/AOP16v8Aq7xD/wBAVb/wTW/+VGftqP8AMvvX+Z6rRXL/AAz+N/wX+NMF3dfBz4veF/FkWnui38nhnX7a/W2ZwSgkMDtsLBWwDjO046VzH7RXxq8U/CzxZ8KfBPguw0+4v/iJ8S4vD8o1CJ3ENlHpmoandyoEdcOLfTpArNlQzrlWyAfNr4fEYWq6VeDhJbqScWvVSUWvmkWmpK6Z3dj4L8K6Z4v1Hx7p+hwQ6xq9la2ep38a4e5htmmaBH7HYbibB6/ORnAGM/xF8HPhF4v8b6X8TPFnwr8N6p4k0SPZoviDUdDt576wXeH2wTuhkiG8BsKw5APWvMNZ/bEXwhffHDxx4n8MXFx4J+DGoadpepNotn52oGc6da6nqV6Q8qo9rBaalZtsQeaPst2R5rNHEOl+GP7XfwP+J+j6NrsHiu30KHxXqM8HgWPxNf21nP4qt42CpfafC0plntpc7omKq0iFZAmx42bEZ3fhPwX4W8C6dcaV4R0OCwt7rU7vUbmOBcebdXU73FxMxPJZ5ZHcn1b0wK0684/Z++Mmu/E7WviP4K8XWNnDrHw8+Ilz4fu30+N0hnt5LOz1SxkAdmO/7BqVmshzgzJKVCqQi+j0AeC/tgf8m9aR/wBl78Df+rA0qveq8F/bA/5N60j/ALL34G/9WBpVe9UAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFeM/8FG/+Uenx4/7Iz4o/wDTTc1wP7Qn7deofD79rlf2ctI+IHhvwjpugaBoeseKtd8SeAdW1uK4XVL+7tYLbz7GeCDRkH2Jwb29doi9zEqoxSSt3/gqp8XPhR8MP2Avi/pvxL+J3h7w7c+IvhX4l0/w/b67rUFo+p3j6VcBbe3WV1M0pLKBGmWJIwOaAPoeiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAKXiPxFofhDw9f+LPE+qQ2Om6XZS3eoXtw+2O3gjQvJIx7KqqST6Cvkj/gjj4X1r4ifCXxh/wUT+IulyweKv2lvFb+LIIrtMTaf4YiX7L4esM91TT0jn93vJT3q1/wWp8UeIdS/Y8tv2U/Aepy2niX9ofxtpXwx0u5gPzW9rqUpOp3BHXbHpcN+xPbA69D9VeEvCvh/wAC+FdM8EeEtLisdK0bT4bHTLKAYS3t4YxHHGo7BVUAewoA0K+L/A8K/saf8FiNe+G9uBa+CP2q/C0vijRoAMRW/jbRYoodSRFHCtd6a1tcsTy72Ep/vGvtCvkH/gtV4e1rw3+yfpf7Y3grTZbnxF+zn470r4kWcVuMSXGnWUhi1e23dQkml3F8GHQlV6YBAB9fUVT8Pa/o3ivQLHxT4c1GO80/UrOK6sLuFspPDIgdHU9wVII+tXKACiiigAooooAKKKKACiiigArmPEvgvwd8Q9R1rwZ4/wDCema5o99pFkt7pWsWEd1bXCia4YB4pAVcBlBAIPIB7V09Zdp/yO2of9gqz/8ARlzV06lSlNTg2pJ3TTaaa2aaaaa6NNNdwaTVmeff8MJ/sQ/9Gb/Cr/w3mm//ABij/hhP9iH/AKM3+FX/AIbzTf8A4xXqtFer/rFxD/0G1v8AwdW/+WmfsaP8q+5f5HL/AAz+CHwX+C0F3a/Bz4Q+F/CcWoOjX8fhnQLawW5ZAQhkECLvKhmwTnG4461yX7RX7LQ/aA8WeCvHumfHfxr4D1rwHeX1zouoeDotIlLvdWxtpDLHqmn3kbYiaRVKorDzX5Oa9VrA8ffFHwL8MP7FHjjXPsR8ReILbRNGUWssrXN/Pu8qECNWIyEclmwqhSWIAJrza+IxGKqurXm5ye7k3Jv1cnJv5tlpKKskePt+wppuu6n418O/EL4mazrnhDx1rWia74q0yUW0M3iTULPTIdMuodR8mBI3srmGw0ySS3t1gWR4p42BgmeF7J/Yqm0XwL8NPg94I+LV9beD/htrVje2Nlq8E91eTQWd6lza2PmRXMNs8EMccdvH9ptrl0SOORXFwizj1my+JngbUfGWueALPxBG+qeGrC0vNdh8pxHZxXPnGHfKV8vcVgkYoGLouxmVVkjLVfg38YfAXx9+Gmk/F/4XX99eeHtdtvtOkX1/ol3p7XUBJ2TJFdxRSmJxh0k27ZEZXQsjKxxGY/wJ+DN38KtR8e+Ktc1eG91jx/4+uvEWqTW0ZWNEFtbafZRKDzmPT7Cyjc9GkSRhgMBXf1yunfGr4c6n8YNQ+AsGrXcfirTdHj1WbTrvR7qCOeydwnnW88kSw3Sq7Kj+S7mJnVX2lgDoeBfiJ4O+JWn3uqeC9ZF5FputXuk34MEkTwXlpO8E8TJIqsMOhwcbXUq6FkdWIB4/+2B/yb1pH/Ze/A3/AKsDSq96rwX9sD/k3rSP+y9+Bv8A1YGlV71QAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB4x8e/2IPhz+0D40vfF2veO/FWjQ+INDsdE8c6JoFxZpaeK9Ms7me5trO8M9tLMiI91djdayW8jJcyIzsNoVf+Cjf/KPT48f9kZ8Uf8Appua9mrxn/go3/yj0+PH/ZGfFH/ppuaAPZqKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA+OPiMp/aF/4La/D3wH/rdF/Z6+EGpeML8p8yDXtfmOl2KSDoHSxtdTde4E4Pfj7Hr45/4JUD/hb3xQ/aa/bcuR5i/Eb44Xfh/w5dL9ybQvDUKaLbOh/utdQ6jJ6ZlJ6k19jUAFZHj/AMD+Gvid4D1v4a+M9PW70fxDpFzpmrWjdJraeJopUP1R2H41r0UAfJ3/AARS8b+JtY/4J/8Ahz4N/EHUDc+KvgvrWq/DHxO7dTcaFeSWELknk+ZaRWsue/m5yep+sa+Of2Sx/wAKI/4Kx/tLfs3yfuNM+I+jeHfi34XtU+4ZZoW0XV2H+0Z9Os5D3zcZOARn7GoAKKKKACiiigAooooAKKKKACuB+MXwt/4XXoXif4Xf8LG8VeEv7X0Swj/4SLwTq/2DVbLbczvut7ja3lsdu0nacqzDvXfVi3d5/Yviq41C8srt4LnT4I45LWzkmw8bzFgRGrEcSLgkYPPpQB8s/wDDob/rJ/8Atgf+Hr/+5KP+HQ3/AFk//bA/8PX/APclfVn/AAmGk/8APpqv/gju/wD41R/wmGk/8+mq/wDgju//AI1QB5z+yp+yd/wyxp+tWH/DS3xa+I/9szQSef8AFXxl/bElj5YcbbY+VH5Stvyw5yVXpiuV/bX0j4x3HxY+B3jT4e/s9+JfiFofgnxxqPiDxFYeFdU0e3uYpf7Dv9PtMrqt/ZxyJv1F5PldirQKcdK9w/4TDSf+fTVf/BHd/wDxqj/hMNJ/59NV/wDBHd//ABqgD5k8OeG/jlqVj+1PofhHTW8I+NvG1/p3ifws3ivRBqaWtpdeFNL09beWOzulilmjutK1GApDcOiuEkzKrgSeOeKPhf4+/bG/ZD/Z41f9kvwX9q8NeB/AelT61qFn40tkv9e017eHT9X8EwzxyxGGeSCOX7TNOIEjuLS1RcSGR7T7+/4TDSf+fTVf/BHd/wDxqj/hMNJ/59NV/wDBHd//ABqgD511RLK4/wCCnvgHwR8Eda8P6fa+AvgvrVp440e3sRcLZ2Fze6QdOsEihkjGnTMbd5Y3cOrQQyIsB3LLF2X7IWh6zH48+O3xAuLCe20jxZ8aZrnw9FcRGMtDZaHo+j3MgVuQr32mXrKejqRIM78n1j/hMNJ/59NV/wDBHd//ABqk/wCEw0n/AJ89V/8ABHd//GqAPGv2wP8Ak3rSP+y9+Bv/AFYGlV71Xg37YcNxD+zvobXNu8TTfHLwDOsci4ZVk8e6RIuR2O1hkdq95oAKKKKACiiigAooooAKKKKACiiigAooooAK8Z/4KN/8o9Pjx/2RnxR/6abmtPxJ+1CdK/aXH7MPhf4EeNfE2pW3h/SNb17xBo0ukRabo1jqN5fWkEs5vNQguJNraddO628MzBFXAZmCVmf8FG/+Uenx4/7Iz4o/9NNzQB7NRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFec/tf/HfT/wBl79lL4k/tHak8Yi8DeBtU1xVk6SSW1rJKkeO5Z1VQO5YCvRq+O/8Agtrnx1+y/wCDP2UIzz8dPjb4S8EXYX739nyail/qBx3X7DYXQb/ZYjvQB6L/AMEq/gRqH7NX/BOX4NfBzXEkXV9P8BWN14h837x1S7T7ZfMc8km6uJySeTnJr3+gAKAqjAHQCigAooooA+Ov22x/wpX/AIKZfso/tOx/6NYeJtQ8RfCjxPdL/wAtV1SxGpaajewvdICj3n9cCvsWvkb/AILi6Dqyf8E6vFHxr8K2Lza78HNe0P4k6IYx80cmh6nb385H1tYrlfo57V9W+H9e0nxToNj4n0C9S5sNSs4rqyuYz8ssMiB0cexUg/jQBcooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDxn9vD/AJIhof8A2Wb4cf8Aqa6JXs1eM/t4f8kQ0P8A7LN8OP8A1NdEr2agAooooAKKKKACiiigAooooAKKKKACivNPix+2f+x78BfE8fgn45ftW/DbwZrMsayR6T4r8c6fp9yyNjDCKeZH2nIwcYOR616BoWvaF4p0a18R+GdatNR0+9hWayv7C5WaG4jYZV0dCVdSOQQSDQB86ftg/sk+Nf2gvitoniXwV8GfhhYajYT6M1v8ar/V54/FugQ2eom7ltrGGLT23oyGVADfRRt9rlEkbpuSS/8A8FVPhj4b+If7AXxf1DX9S8Q28mhfCvxLfWS6F4u1HS0llXSrghbhLOeJbuLgZhnEkTDIKEE5+h68Z/4KN/8AKPT48f8AZGfFH/ppuaAPZqKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAr47/AGmP+Lw/8FjP2bPg5AfMtfhn4G8X/ErW4H5TzZY7fQtPYjpuDX18wz3TI+6a+xK+O/2QR/wuD/gq/wDtU/HuYfaLPwTp/hL4X+HrsfdQ21lLrOoxg+0+rQqR2MXuKAPsSiiigAooooA5z4wfDTQfjR8JfFPwd8VJu0vxZ4cvtG1Jduc291A8Egx3+VzXgH/BFz4l698Tf+CYPwhk8Yv/AMT/AMLeHG8H+Ikdsul/olxLpE+/PO4vZFjnruB719RV8d/8ExP+LX/tIftcfspz8Dw38dj4x0pBwq6f4m0621IBR/dF4NQHpkMO1AH2JRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHjP7eH/JEND/AOyzfDj/ANTXRK9mrxn9vD/kiGh/9lm+HH/qa6JXs1ABRRRQAUUUUAFFFFABRRRQAUUUUAfEn7S9z8Q/2ZP2svEXi7wT8Yf+FU/D/wASeBNOvLCKz+DD69omteKl1LWJdTk1BrJBNFPNFc6eT+8ilusHZIzQuK+nf2X4NNm+AnhXxZa/BWy+Hl74n0O013XvCFnpyWp03UbuCOa5hlRUTMyyMyOzKGLKS3NYv7Rn7W3wW+AMd54V+I3xZtfAupXWgyXWmeKPFHhq9k0KyZvNSOWe7AitTsePc9ubmKTZtJKCRHPVfAXxTrfjn4M+GvHGveOdB8Tza1pEOoQ+IvDGjT6fYalbzjzYLiG2nnnkhV4WjbY0rkEnmgDrq8Z/4KN/8o9Pjx/2RnxR/wCmm5r07xf8RPh/8Pm0pPHvjnR9DbXdYh0nQxq+pxWx1HUJQxitIPMYedO4RysSZdtrYBwa8x/4KN/8o9Pjx/2RnxR/6abmgD2aiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAbLLFBE080ioiKWd2OAoHUk9q+P/wDgh5FL4t/Yr1D9py+jZLv45fFXxZ8QZVYYIt77Vp47LHt9ht7MD0UAdq9A/wCCrnxtvP2d/wDgm18bfi1pMjrqdh8OtSttDMedx1K6hNpZgAckm5nhAA5OcCvQP2SfglZ/s1/ssfDf9nmxjRY/A/gTSdCzH0ZrWzigZs9yzIWJ7kk96APQqKKKACiiigAr47u/+LM/8F17O4Y/ZtN+Of7OUsGBwLvWPDerBwfdhZay30EQ9a+xK+O/+Cpf/FsPjd+yh+1hbDa3hD9oC18M6tO3CR6Z4ksbnSJS57KLqWwPPAKg/wAIoA+xKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA8Z/bw/5Ihof/AGWb4cf+prolezV4z+3h/wAkQ0P/ALLN8OP/AFNdEr2agAooooAKKKKACiiigAooooAKKKKAPjv9sb4sftQaX4rudW8LaV8XPAOlrK+h6dNHrHw3k0HXJfMnMdyItYuxdmSVMlYxLCSiKGiDBq9J/wCCc2l/tA+Dv2dNM+Ffx88H3GnHwjaWekeFLu4sLK3kvdIgs4YoGlFprOqLLMPLYvMZIg+9cQrgs3N/traT/wAFAx8RTr37HHh/RdS0M+D0g8W6d42uobuxvgLmYhdK0/fEzaokbOS1zc29nMskEblym+D239nfw/4f8M/ArwlpXhjTp7a0OgW0+288NRaNcySzRiWWa4sYYoUtLiSR3kliWOPbI7javSgD5R/4KI/BP9svxr+0P4I+K3gb4P8AhHxn4Y8OeO/By+E4JPGGo219ozjWre41O9mtIdIuY9riK3Rroz/6PbwSEJ+9lD+s/wDBVTTfivqH7AXxfk+GnjTw9pNtB8K/Er+IItd8MT6i97ZjSrjdDbvFe2wtZSNwErrOoJBMbYwfoevGf+Cjf/KPT48f9kZ8Uf8AppuaAPZqKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigD47/wCCv3/Fx7L9n/8AZGg/fj4rftGeHItZ08c/aNH0dpdfvcjuuNMiU+nmA19iV8d/Ez/i8f8AwXH+F/gt/msvgr8Btf8AF0pTkJqOuX9vpVsG9G+zWWokdwGPZq+xKACiiigAooooAK+X/wDgtF8Mtc+Kf/BLr4zad4TU/wBt+H/CbeKtAZFy63+jTR6tb7Mc7jLZKB65x3r6gqn4h0DSfFWgX3hfX7JLmw1Kzltb23k+7LDIhR0PsVJH40AY3wZ+Juh/Gv4P+FPjJ4ZYHTfFvhqx1rTyGyDBdW6Tx89/lkFdLXyP/wAEONf1dv8Agm94O+EHim9abXPhJq2tfDnW1kPzRS6Fqlzp0Sn0P2eC3bHYMK+uKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA8Z/bw/5Ihof/ZZvhx/6muiV7NXjP7eH/JEND/7LN8OP/U10SvZqACiiigAooooAKKKKACiiigAooooA/Ov9u0/Bv4b/ALYd78PNP+PHgj4L/a/BcPjG6h8SfGfxR4PtPG19c31/Hdwo2k6vZWlnMn2WOWW6aC6nlN4W8siKQt9kfsdf8Ku1D9mbwX41+D/w8bwvo3irw9Z68uj3DmS6hlvIEnkF1KzM81yC+2WR2Z2dTuJNehazomi+I9Mm0XxDpFrf2dwm24tL23WWKVfRlYEMPqKxvhn8HfhH8FtKvNB+Dnws8OeE7HUdRfUNQsvDOh29hFdXbokbXEiQIoeUpHGpcgsVjUE4UYAOjrxn/go3/wAo9Pjx/wBkZ8Uf+mm5r2avGf8Ago3/AMo9Pjx/2RnxR/6abmgD2aiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoorP8WeJ9F8EeFdT8aeJLxbbTtI0+a9v7hukUESGR2P0VSfwoA+S/+Cen/F2P27P2wP2oM77Q/EfR/hxojPyUg8PaVGbkKf7pv9SvAcd0weVr7Er5K/4IeeGNb0//AIJq+A/id4uszFr/AMVLnVfiLr0j/emn17UbjVEZj3IguYVz3CCvrWgAooooAKKKKACiiigD47/4J9f8Wo/b3/bB/ZjH7uzf4gaJ8SNEjfgvDr+kxx3RUf3RfaZdZx3fJ5avsSvjv4of8Wd/4Li/Crxu3y2Xxo+BHiDwfMF4Dahol9b6talvVvs15qIHfCt2WvsSgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAPGf28P8AkiGh/wDZZvhx/wCprolezV4z+3h/yRDQ/wDss3w4/wDU10SvZqACiiigAooooAKKKKACiiigAooooAKKKKAPlb/go/ZeOfA8vhP9oH4eeJdW0pNF8W+H7PxVrNp8VNXtF02xk1q0TbF4eizp2syXCXE8En2gxSIjI0fnukcQ6L/gqp408SeEv2Avi/a6B8I/EPiiPU/hX4ltr250K506NNIiOlXAN1cC8u4GaJckkQLNLhTiMnAPqviP9nT9nzxj8TdO+Nfi74E+DdV8ZaQsa6T4t1LwxaT6nZCMsyCG6eMyxhS7kbWGCxx1NcX/AMFG/wDlHp8eP+yM+KP/AE03NAHs1FFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXyx/wWu+IeveAP+CYPxZ0/wdLjX/GmhxeCfDsakh5L7XLqHSIgmOdwN4W45AQntX1PXx1/wUlx8Wf2vf2RP2TU5h1X4vXXxA1oDkCy8M6dLdRhx/dOoXWnY/2gvvQB9T/C34e6D8I/hl4c+FPhWLy9L8MaDZ6TpqYxtt7aFIYxgdPlQVvUUUAFFFFABRRRQAUUUUAfHX/BYb/i2vh34FfthwfuB8IP2hvDd5rV+v3odF1aSTQb8f7uzU0c/wDXIds19i14b/wUy+Ak37T/APwT5+MvwIsLdpdQ8Q/DvVItFVM7l1GO3aazYY5ytzHC3HPHFb/7Dnx7h/al/Y0+Ff7Rsdwsknjb4f6TrF3tx8lzPaRvPGcd1lLqfdTQB6nRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAeM/t4f8kQ0P/ss3w4/9TXRK9mrxn9vD/kiGh/8AZZvhx/6muiV7NQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAV4z/wUb/5R6fHj/sjPij/ANNNzXs1eM/8FG/+Uenx4/7Iz4o/9NNzQB7NRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAV8dfDU/8Lu/4Lh/Ezxsn7zTvgX8DtF8IQB+VTVtevJNWumT0YWlnpoPcCUdmr7Fr45/4Ivf8XK+DPxP/bNuD5rfHX45eJfE2lXD8udGtrkaRpaE91+yabE69sSkjrQB9jUUUUAFFFFABRRRQAUUUUAFfHX/AARb/wCLc/A/4l/scTjy2+BXxz8UeFtMgfhxo893/a2mOR2X7HqUKL2xFgfdr7Fr45+F3/FjP+C3fxS8Bv8AJp3x3+DGh+M7Bn+VW1XQrh9IvUTsXNpdaWx74j9FNAH2NRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAeM/t4f8kQ0P8A7LN8OP8A1NdEr2avGf28P+SIaH/2Wb4cf+prolezUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAeD/ta/Gb9oL4DeJPC/i7wZ4g8EXvh7WvF2heHbbwLfeHLyTXNcub2+WG5a0vY71YoTBatJdbGtJh5dpMzvGgLx0P8Agqp8XPhR8MP2Avi/pvxL+J3h7w7c+IvhX4l0/wAP2+u61BaPqd4+lXAW3t1ldTNKSygRpliSMDmuh+JP7HK/ET9p3RP2prf9o74gaFqvh/QxpWmaBpkOiXGlwQPN5ty0aX2mXE0MtyBHHNLFKjtHDGisgHJ/wUb/AOUenx4/7Iz4o/8ATTc0AezUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAeAf8FUfj/qv7MP8AwTu+Lvxk8MtJ/btn4NubHwusJ+d9Yvttjp6r6k3dzAAByc13H7HnwA0r9lT9lH4cfs16MIzD4F8E6ZojSxDiaS3to45Jfcu6s5PcsTXxX/wVk/ad0v49/tLfAf8A4J8fs0aLD8S/FsHxXg8bePfC+laokVtbWGgIbuK3vrpgYoEbUGsS6nc4WIqULOit9Ew/si/tFfHwf2p+2d+0zqMdhMMt8OfhNczaLpManrFPegi9vQRjJ3wrnOEFcNTGty5aEHUfdNKK1trJ6brZKT8kfU4PhqEaTr5tiY4SCt7soylWleKknChFKdnGSanUlSpu6tKR6t8T/wBqP9mv4KTNa/F74/eDfDU6/wDLrrfiS2tpj34jdw5/AV53F/wVS/4J2TXf2JP2vfBgcnG59RKp/wB9ldv612Xwt/Ys/ZK+CsKp8Mf2dPCGlTKctfpocMt3IfV7iQNK592YmvRrnTdJksWs7uwt2tgp3RSRKUC98gjGKi2bS15qcfK05fjeP5G7qeH1D3VTxdX+854el81BU633SnfuzmPhn+0D8CPjTGZfhB8aPCnikBdzDw94gtrwqO+RE7Fce9dfXwl+0bN+x5+0lreqeDv2ZP2N/CHxI8Q6MzjW/ibCy6DoPhl0GWe41y2CTSvGPnMNqzsADlkwa+Ubbwb+2n+0rc2vw3/ZA+N+uftH+G9DN1F4o1DxZqWraT8O4pwDttLXU01JLzWmicBNoM0WCC0iAkDuyTBcR59Op9Sw8alKnb2lbnVOhTu7fvKlVKK2dowdWcrNQpzei8bEVuDMxozeS4uoqsU7QxEEoTa05YYihzwcv8VKMejqxe36n/Er9s79kf4O3UmnfFD9pfwNod3ESJLC/wDFFqlyMdf3O/zP/Ha8u1X/AILP/wDBK/RJDDqf7cvgKJ1bBj/tNmYfgFJ/Gvkn9i7/AIJ3/sZfE/xcfhf+3Je+IE+KEMTPefBq/wBKi8IaCyL/AMtLK10twuswjDDz5bq6ZlXLquSD+hXw4/Y1/ZG+EGkLoPws/Zf+H3h60VceTpHg+ygDe7FIwWJ7k5J7mvpMLR4PoR5sRjZ4qS0aw1NU6aa3XtcT+8lbq1h6fdKzR4eOyji/KMX7DMqUKMmlJK0pc0XtKM1JwnF9JQlKL6PdLy3Qf+Czv/BKrxJerp+n/t5/DiORjgNf68ton4vOEUfia96+HHxc+FHxj0T/AISb4RfE7w94q03IH9oeG9agvoMnkfvIXZf1ryz4uf8ABNL9hv40F7vxP+zp4f0/UmB2a54XtzpF8jH+Lz7MxsxHo5YdiCOK+Tfin/wRPi+EWtn4i/ADwtZ+MPsgLQPpWqnwZ42se++01zShDFfSf7N/E2cAGTpTliOCakrVKeKoL+dOhiYrzlTUcNVt39m5NdIvY8mrjM3wkr1aKnDvBttesWub/wAB5vQ/Savjn/gqH/xZ34/fssftqQfJF4M+My+D/EkzcJHo3ii2bTHeRuyJfDTH54BQE9OfNPgD+0/+258Nru+0T4Y+Mbz4/wCm+HUB8TfCX4m2UHhr4o+HIRj5o5AEs9ZQKd28rGZSVCzEmvQv2kfib8D/APgrh/wTt+M/wJ+A3iS9g8cReE7jd4L12wk07xB4b8QWu2709buymAlgZbyCAhwCjFSUdsZoxfDleGXvMsvrQxeEW9Wi21C7slVpyUatGV9LVYKLeiqSej7sJj8NjIKVOW/6b/NdVo11ij7Pory39iD9ozTv2uv2Pfhn+03prRgeN/BOnatdRRdLe6lgQ3EHsY5vMjI9UNepV86doUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAeM/t4f8kQ0P/ss3w4/9TXRK9mrxn9vD/kiGh/9lm+HH/qa6JXs1ABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXjP8AwUb/AOUenx4/7Iz4o/8ATTc17NXjP/BRv/lHp8eP+yM+KP8A003NAHs1FFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUVwP7RH7Snwu/Zj8Fx+L/AIkajcPNe3ItNB0DS7c3Opa3etwlpZ26/NNKxIGBwucsVUEjOrVp0KbqVHaK3bOvAYDG5pjIYTB03UqzdoxirtvyXybbbSSTbaSbXTfED4g+CPhV4M1H4h/EjxTZaJoek2xn1HVNRnEcMEY7lj6nAAHJJAAJIFfOEN7+0T/wUJCz6Td698J/gnPzHeR5tfFHjODsyZ+bSrJxyG/4+JUxjy1etH4f/s1/FL9pvxnp3x//AG6dNt4LbTblbvwN8Gre5E+naC4+5d6gw+W/vwDxkeTDk7AWO4ewftKfHbwf+y/+z342/aM8fy7dG8D+Fr7W9RUOFaSO2geUxrn+N9oRR1LMAMk153JXzHWpeFL+XaUv8XWMf7qak/tNL3T7J4nK+DFyYNwxGPW9XSdGg+1FNONaquteSlSg1ahCpJe2Pkb/AIJ+/Bv4Xar/AMFLvjn8Uvg94G0/RPBHwX8O6V8H/BUGlw7YZL4hNZ16ZmOWkm86406B5GJZjancSRk/d9fNX/BI34F+MfgX+wb4NHxWiP8Awnnjk3fjn4iyum2R9d1q4fUbpHH96I3CwfSAdetd/wDtMftZeDv2dodO8L2ehXvizx54kZovB3w/0Eq1/q0o6uc/Lb2ydZLmTEcagnk4U9054fBULytGEdPLskkvuSSu3okfBZjmUpTqYzHVXKUneUpNylKTe7bblKUm/OTbsrnT/HH48fCv9nH4fXPxM+L/AIrh0rS4HWKLKtJNdzvxHbwRIC88znhY0BY88YBI+PP2oPi9rHjTwBF8XP2+NU1/4cfCrVbxLPwZ8BfDZeTxb8QLqQ/ubW8W3Pml5cjGnwEbQ2Z5QEOKnxI8b6z8GPjFofiv41aHF8b/ANq3xHaSS/DP4OeGZ8aR4ItGO1rovJlbO3U4E2pzjzJSNkIwCK9t/ZV/YW1nwZ8RH/a1/a/8c2/xG+N+o2jQ/wBtrbsmleErV/vabols+fs0AB2tOf38/LSMNxQfSZbw1QngoZtxLzUsLPWlh4vlrYm2l29fZ0Lq0qjTTs401WqX9n89KGKzaVqq5af8nfzqWet+lJO3/PyT+E8y+G/7Dvxb/bI0jTL/APbc8KWvw6+EOmiNvBv7Lvg+6WKz8lDujk8QXFvtF5JnD/YYiLaMhd/mtvr7R8O+HfD/AIR0Kz8LeE9Cs9L0zT7dLew07TrVIYLaJRhY440AVFAAAUAACrlFGc8Q43OIwoNRpYel/Do01y0qa292N9ZNJc1SbnUnb3p2tGPt0cPToRtFf1+iXRJJLojh/j1+zj8HP2l/CC+DfjD4Oh1KG3mE+mX0btDeaZcDBW4tbiMiS3lUgEOjA8YORkV4lD8Xvj3+wXcxaF+1Hq994/8AhOJBFp/xchtS+qeH0JwketwRL+9iAwv2+IdRmVAW3V9S0y6tba9tpLK9t0mhmQpLFKgZXUjBUg8EEcYr5TEYJVKntqT5KnddfKS+0vxX2ZLZ/XZRxLPCYX+zswp/WMG237OTs4N7zoTs3RqdXa9OdkqtKa96NfQte0TxRotp4k8Naxa6hp1/bpPZX9jcLLDcRMAVkR1JVlIIIIOCDVuvlzX/AIH/ABa/YX1u7+Jv7Hfhu48SfDm6uHuvFfwUhkHmWJY7pbzQSxxE+SXaxJEcnzeXsYqK90+B3x2+Fn7Rnw9tfid8IfFUOq6Xcs0chClJrSdeJLeeJsPDMh4aNwGHpggl4fFupU9jWXLUXTo13i+q79Y7SS0bWc8OxwmFWZZdU9vg5Oyna0qcnqqdeCb9nU0dnd06qTlSnJc0IYP7Rn7JXwi/aXtLK+8X2V3pfiXRWMnhjxx4duTZ6xokvZ7e5X5gMnmNt0bfxKa+QP2ifhN9h8c6Ha/t2a1P4N8babItp8K/2xfh1AunSpKTiOy1hBmOHeeGgn32U4dwpibJr9DKoeKvCvhnxz4cvfB/jPw/Z6rpOpWzW+oabqFss0FzEwwyOjAhlI7EV3YLE5lk2YLMcqrOhXWl18M01ZxqR1jOMl7slKMrrRqS90+DxmWRqzdag1Co99PdlbZTStfykrTj0k17p+Y//BJn9r+x/wCCf/xN8d/8Eiv22vE9np2r+EviLPdfD3x9aWZt/DmrWOvF9Vs7JXJK2UzyS3nlRSNsby5IY3doTu/UevyT/bI/Zw+Hf/BM/wDbW8G/Ev4j+FdR+JH7Nvxy8M3Hwq8YeCtWtm1Gbw68bSappQi3EyXUURS/WEE+dCjsqM5WID3n4bfG/wAf/wDBOzwhpfjGTx/qXxp/ZJ1OFZfD/wAQ7OV9S134dWxOBFf7AZNR0uPoLgA3FsFZJVYIpr6vLsHl3F1FUsGlRzJXcqDa5K95S1w70UZ62VBu00o+xkp/uZOhjpRkqddcsrXavdpXte/2o3XxJaXXOkz7zorO8IeMPCnxA8Laf448C+JbHWdG1a0jutL1XTLpJ7e7gdQySRyISrqQQQQSDWjXys4TpTcJpqSbTTTTTWjTTs000000mmrNHqJpq6CiiipAKKKKACiiigAooooAKKKKACiiigDxn9vD/kiGh/8AZZvhx/6muiV7NXjP7eH/ACRDQ/8Ass3w4/8AU10SvZqACiiigAooooAKKKKACiiigAooooAKKKKAPNviF+0rpXw2/aD8B/ADW/hf4rm/4WFNc22j+L7SGzOkW95DY3t8bScvcrciU29hOwMcEiDKBnUsKwv+Cjf/ACj0+PH/AGRnxR/6abmsv9qX4ZftP+Ov2gfg145+DXgbwFqPh74e+LLnW9cn8T+O73Tb2Y3GkalpbwwQQaVdI+xL8Th3mTc0RjKoG80Uv+Cqnwx8N/EP9gL4v6hr+peIbeTQvhX4lvrJdC8XajpaSyrpVwQtwlnPEt3FwMwziSJhkFCCcgH0PRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUV83/ABJ/as+I3xx8aaj+z3+wbDZajqthcG08YfFK/h87Q/Cbfxxx9tQv1HS3Q7EYgysAGWubE4qlhYpy1b0SWrk+yX57JLVtI9rJchx+e15RoWjCC5qlSb5adKN7c1Sdmkr6RSUpzlaNOE5Oy6z9pP8Aa50/4Qa7ZfBr4V+EpvHXxU163MmgeCNNnCGGLODe383K2Vmp6yvyx+VAxzin+zx+yPqHhDxpJ+0X+0h4uh8cfFjULYxPrXkFLDw/bt1sNJgbP2eAZw0h/ezcs7fMVHTfs2/ss/Df9mfQr2LwzJe6v4h12cXXi3xpr0/2jVNdusY824mPOByEjXEcY4VRkk+lVz0sLVr1FWxW61jFaxj5/wB6X97ZbRS+J+zj89wGV4OeWZDdQmuWrXkuWrXXWKV26NC60pJ89SylXnLSlAr4x/4Kpt/w0l8T/gl/wTH0tvOtvil4yXxL8TYU5Efg3QJIr26ilxyi3d7/AGfaA9GEso6A4+yb6+sdLsZtT1O8itra3iaW4uJ5AiRIoyzMx4UAAkk8ACvyj/ZZ+KXxx/b9/aY+On7fvw18QRfD74Z6yw8HeHvjhrzpEuk+BtHklMx0gTYXzr69a6upLmT91bxpB96Rdq9daq6doxi5zk0oxiryk3skv12X3J/BYvFwwlPm5XKT0UVu3+SXeTailq30f3N8ev2tvEo8fy/sw/si+GrPxf8AE8wq2qzXUjDR/B8D/dutTmTo2MlLVP3smOirhj856F4g8V+Hfix4k/Z1/YH1eH4n/H/U2WH41/tGeLrYT6T4KB6wEJ8jzp0t9IgIRCqtOQEbLfgt4e8W/tdeCx8B/wDgnxb618Jv2dPtUjeK/ji6vH4k+IczHE50h5x5qpKciTVphubO2BcJmvtj4C/AD4O/sxfC3TPgx8CPAVj4c8N6RGVtNOsUPzMeXlkdiXmlc/M8rlndiSxJOa+zw+SYLhCaxmfQjXzBawwz1pYftLEWfv1V/wA+L3/5/unH9xLy8PhMRjKyxGId2trfDHyhfeVtHVau9VBRjq+P/ZE/Yr+Fv7Ifh/U5vD99qPiTxn4ouVvPHnxG8TTC41nxJeAY8y4m/hjX7scCYiiXhVyWJ9hoorwMyzLH5vjZ4vG1HUqT3k/JWSWySSSUYxUYxilGMYxSS9yEI048sVZBRRRXCUFFFFABXz/8cP2UfGmgfEK6/ai/Y01qy8N/EKZVPiTQL7K6L41hTpDfIv8AqrkDIjvEG9c7X3qePoCiufE4aliocs+mqa0afdPo/wDhmmm0evk2d4/IsU62GaakuWcJLmp1IPeFSD0lF22dmmlKEoTjGa8s/Zo/au8F/tF2mo+H5NFvfC3jjw26w+MvAGvYTUNHmPQkDie3frHcR5jkUggg5Uep15H+0t+yV4e+O93p3xF8JeJ7rwX8SfDaN/winxA0aJTc2mTk286H5bu0c/ft5MqQTjaTmuf+B/7X3iGDx7bfs0/tf+FrXwV8S5EI0i4t5WbRPF6JwZ9MuHxl+ha0kxNHuHDjLDlp4qrhpqjiuukZ7KXZPpGXl8Mvsu/unvYzI8BnWFnmOQJ+6nKrh2+apSS1lOm3rWoLfmSdWktK0XFKs4P+Co/7NXin9qf9iPxl4D+GTmHx5osVv4o+Gl9Gv7y18R6VMl/pzIf4S08CxE/3JXHevHf2f4vEHjn4C+Fv+Cjf/BOvRrOXSfiX4fg1r4g/Ay7uBFpuq3TptvBaMw22GpRSrLA/HlTNERIARuP3LXxZ+xXIf2Mf2/8A4tf8E+tXH2bwp4/nufiz8FC3ESxXcyp4g0mLoqm31Blu0iXkRaiTwFrqxGFp4mKvdSWsZLRxfdP81qmtGn0/P8XgqOMgua6lHWMlpKL7p/g07xktJJrbh/hIde+Alnqn7Tn/AAS20O/8S/Dk6pK3xb/ZWvk+yal4bvixa5m0aCUj7Beq25n044t7j5jCVYoT9nfs2/tNfBb9rX4W2nxf+BXjKLV9JuJXguY2jaK6066TiW0uoHAktriM8PE4DDg8ggni/wBor9kXUfGHjSP9oz9m3xdD4H+LOn2wij1ryC9h4ht16WGrQLj7RAcYWQfvYThkb5dp+aJdC8T+P/jvqfxd/Zihtvgd+1hplmknj/4WeJZD/wAI98TLOLIDyGPC3ScMIdSgHnw5KSqVwB9nhs+wfFTjgeIqio43RUsW7qFaysoYq12pWSSr6zjp7X21Nc9Py6OKxGBrKhiVq9mtIz/w/wAs+9Nuz1dNte6v0Iorw79kX9uv4fftRXep/DXXvDOo+Afip4XjUeNvhV4qKpqelscDz4iPkvbNiQY7uHdG6sudrHaPca8TNMqzHJcbLCY2m4VI20dtU1dSi03GUZKzjOMpRlFpxk1t7kJwqR5ou6CiiivPLCiiigAooooAKKKKACiiigDxn9vD/kiGh/8AZZvhx/6muiV7NXjP7eH/ACRDQ/8Ass3w4/8AU10SvZqACiiigAooooAKKKKACiiigAooooAKKKKACvGf+Cjf/KPT48f9kZ8Uf+mm5r2avGf+Cjf/ACj0+PH/AGRnxR/6abmgD2aiiigAooooAKKKKACiiigAooooAKKKwfiR8Uvht8HfCs/jj4rePNI8OaPbD99qWtahHbQqey7nIBY44UcnoAamU4wi5Sdkur0RrQoV8VWjRoQc5ydlGKcpNvZJJNtvokm/I3q5D42fHr4Q/s6eCZfiH8Z/HdjoWlxsI4pLpyZbmU/dhgiUGSeVu0casx7CvF5P2vvjr+0rnSf2E/gxJJo8x2n4tfEa0m0/RUQ/8tLK0IW61E4zghYotwGZCK6n4L/sPeC/AvjeP43/ABo8Y6l8UPiUqYTxl4rjTbpwPWPTrNB5Onx5zxGN5ydztmvOeNq4rTBxuv53dQXps5/9u2j/AHj7JcMYHI/3nEdV05L/AJh6bjLES8p6yp4dd3Vcqq6UE9uJk8PftNft8n/iurPXPg/8HZ/+ZfSf7P4q8WQn+G7dDnSrVhwYUJuHXcGaMNivon4c/DbwF8IfBWn/AA5+GPhGw0LQtKgEOn6XptuI4oU68AdSSSSxyWJJJJJNbdcd8aP2gvgn+zt4YPjH43/E/R/DOnkkQyapeKj3DD+CGMZeZ/8AYjVmPpWtLD0MGpV6s7ytrOVlp2WyjHyVl35nqeNn3FNXHYVYdRjhsHTfNGlF2gna3POUnzVajWjq1G5fZgqcbQOxrzv9oX9qb4M/sx6Fban8TvEb/wBoanL5OgeGtJt2u9V1q4PCwWlpHmSZySBkDauRuZRzXy9+0t/wVD8QadoEN/4d1XS/gn4R1Ntmn/EL4rWLvrWrgnA/sTw3Hm8vXOVKPMqqc/6tq+afi18ZPjR8Bfhzd/tCfDr4b+J/h9f+LrmPSNH+K3xgsI9Z+KnxE1GfKwaV4e0E5j00SNwvn7UiiPmNb/uya+iyzh7PM7wqxlGMcPhHp9ZxF6dLTdUk17TESX8tGE1e15I+EqZrWxcvZ4GN/wC+07f9ux0cvJvkh5yW+H/wWx/aS/aV+MeleDf2RPiDN4h8L6z8ZtSSHRfgT8NHGo+Io/DiPm81TVGhOHnkRWgtrMMluJXMksrC1lx9a/A7/gnT8Q/j74X8K237c/hvS/C/wv8ABtjZ23w8/Zc8K3/n6LpdvbIq2763crgavcoET9yP9FQrnEpZjV3/AII7f8ExtV/Yt8Ea3+0H+0rqc/ij9oH4pyLffEHxVrOqHU7vToDgw6PFduMvHCoXzXXCzSgsB5aQqn2rXoZbm+H4WpVYZPd4iUnfFTX73l2SpRu1h7q95RvVSlyxnT95y68Nl0aSbqScm9ZXd2357Ky2UUlBdnu47OztNOtItP0+1jgggjWOCCFAqRoowFUDgAAAADpUlFFfONtu7PSCiiikAUUUUAFFFFABRRRQAVyPxt+BPwo/aK8BXHw1+MXg221nSZ3WRI5spLbTL9yeCVCHglU8rIhVh2PJrrqKipTp1YOE0mno09UzowmLxWAxUMThqkqdSDTjKLcZRa2aaaaa8n+qfy3H45/aX/YLzp3xgXXPi38I7f8A49vHVlbG48S+GoB21K3jGdQgQYJuoR5oCsZI24NZP7f3w9u/2tf2dfCX7Zn7CfiLSvEvxK+D2tDxl8Kr7S7xXi1sJGY9R0J5E5Ed9aGW2ZDgrL5JbBj4+uq+f/iL+wtaaV4zvfjT+yB8Q5/hR41vpPO1VNNs1n0LX36/8TDTSRG7nkefEY5huJ3MeK89UsXgf4P7yn/K37y/wyfxL+7J37T6H2Msdw9xT/yMbYTFv/l9CP7io+9alBXpSfWrQi4N6zoJ3kd9+yn+0x8MP2xf2efCn7Snwe1JrjQfFmlJd28cwAms5QSk1pOo+5PDKskMifwvGw7Un7RP7MHwn/ac8M22ifETTbqC/wBKuPtXhzxPot0bTVdDuhjFxaXKfNE4IGRyrYAZWHFfnPbftGfHn/gjx+2BrXxL/aX/AGfLzw18BPi7rX2v4l6t4QMmp+HfC/ieUrGfEVmyL51taXpCi7tJ0EiSqs8bShnSv1H8FeOfBXxK8LWPjn4deL9L1/RNTgWfTtY0W/jurW6iYZDxyxsyOpHQgkV0RnhcfRcZK6ejjJWfzT1Vv8mnsz5bO+H8TgG8PjIRnCSVpRlGpTmns4zg3Fp201jJNWcYSVl8HftTfC7WtEXR7H9v1tYVvCs+fhp+138MrU2mr+GZG4C6rFCGNujZ2yNtks5gw3pG3zDt/h1/wUE+J37Lq6V4N/4KOSaReeFdUMUXhD9pnwVCG8K66jkCH+0kjLf2NcvlfmJa0dixSRAMV9l3NtbXttJZ3luksMqFJYpUDK6kYKkHggjjFfM3jz/gn/qPw/Ora3+xR4t0vwnb6ysv/CQ/CrxRpx1DwZrwkBEiPZdbFnBwz22FIGGibJr6PLOI8VgMFHLc0pPG4GN+WLly4ihd3bw9Vp6X1dKpzU5veKk/aHyEsNmGXS56DdSHb7a+9pVEul3Goukp7H0rpup6brWnW+saPqEF3aXUKzWt1bSiSOaNgCroykhlIIII4INT1+W+mXXxU/YN8VrafBTUZf2e57y7LTfB74sXMuqfC/XZmJZjo2tQ5fQ5Hyz+WdkYLKGg4r6d+Fn/AAVj+DL65pvw2/bC8H6n8CPGOqKo02DxxdQvoOtMRndpmuQk2V6hyMAvHKc48uvSjw3TzelLEcOV/rlOKvKCi44mkt37XDNupp1nR9tTdr3ituzCZthMVePNaS3T0t6ppNekkvJs+q6KZbXNveW8d3Z3CSxSoHiljcMrqRkEEcEEd6fXyrTTsz0wooooAKKKKACiiigDxn9vD/kiGh/9lm+HH/qa6JXs1eM/t4f8kQ0P/ss3w4/9TXRK9moAKKKKACiiigAooooAKKKKACiiigAooooA5jxZ8bfgx4C8baH8NPHXxd8MaL4j8TuyeGvD+ra/bW19qzKQGW2gkcSTkEjIQNjIrgP+Cjf/ACj0+PH/AGRnxR/6abmvBP21fAHxB1L43fE/wlpnwn8Ua1qnxT8MeArD4c+I9J8M3V5Y6Zc6brV7PMbm8ijaLTRZvPHqAed4hJvIhMkqFB65/wAFT/Cvxv8AGH7A3xd0T4J+MNG0uef4W+JYtSt9R8IT6tc38L6VcKIbQRXluIJyThXdLhckfum6EA+haK+Hf+GBP+CyP/SfjVv/ABGXwp/hR/wwJ/wWR/6T8at/4jL4U/wrXkp/zr7mB9xUV8On9gP/AILIEYP/AAX41b/xGXwp/hVa4/4J3/8ABYe5BWT/AIL+6+M/88/2cfDKf+gkUnCH8y+5/wCRUVFvVn3XRX5/33/BL7/grjqGfP8A+Dgnxouf+eHwO0SL/wBAlFc/q/8AwRz/AOCp+t5+2/8ABw38VEz1+x/Dq0t//RV4uKykpr4bP5tf+2s7qNDLJv8Ae15R9KTl/wC5In6QVT1vxDoHhqzOo+I9cs9Pt1+9Pe3KRIP+BMQK/I34Rf8ABEP/AIKIftLfAfwV8YPiP/wXO+MFleeLfCOm6xe+H9W8KfaW06W6tY53tWMt8pYxs5QkopO37o6DXs/+Dar9om1uPtd1/wAFXItSn73Gu/s3eHtQkP1e6kkJ/E1zSePb92MF6yn+Sh+p7VLCcEwhzVsZiJPtDD04/wDk1TEtL/wFn6A+O/8Agoj+w18OJjaeJv2qfBTXQbb/AGfpWtx6hdbv7vkWpkkJ9tua5o/8FBp/HP8Ao/7N37Inxb8etJ/x66nN4Y/sDS5Pc3WrNbnHusbfSvlnw3/wQ7/4KEeCIPsvw/8A+C4XiDw1Ht27PDHwG0PTAB6AWsseKh1b/gh7/wAFN9aJN3/wcVfGtM9fsnhnyP8A0VqK1dPCYqt/FxUYL+5TlJ/fOX/toPM+D8Frhstq133r4mMI/OGHpJv09qvU+qm0L/gpl8auNc8ZeAfgppE3Jg8PWz+JtcVf7hnuVhs4j/tLFLip/D/7FH7JPwd16D4t/HbxLN428U23zx+M/i/4jW/mtz1zBHOVtrUA9PJiTHrxXw94k/4Nvf20fGm4eM/+C9nxk1cP98ajo1zLn651auEn/wCDRDWL+/8A7U1n/gpxrWo3Bfc0uo/Dczlz6tv1QhvxzXqYbIOFZTU8Xjakmu9CVSz8k68IL5R+ZyYrjni32MqGW06ODpyTTjQvSbT3U6kVKtNPqpV0n1i9j9JfiZ/wVM/4J3/CNXHjH9r3wS0kYO+30XVRqcq46jy7MSsD7Yr5p+IH/Byh+xZFqDeG/wBnrwd4n+I2rGUxww6fAsKOc4z5aedeEZ67bVv6V8/fsvf8G8HxY1XwhdeMNO/bC8D6Je2Pi/X9HhTXP2SfC+rTPHpusXmnxXQe/Dn9/HapcLwcCYAMwAY/Reg/8Eof+CpPhHRz4f8AA/8AwXPvfDticf6J4b/Zh8KadGMdMC2VMV60qXBlHepiq3pHD0E/m3iZr5I+Ilh88rfFXhBf3IXf3zb/APSTjLj9sT/gs7+145sfgz+yV4s8E6LM2BcxaVb+HxLC38R1LXwbhcDHMGllucqe40fCP/BMP4+eFpLj46/tf/tm+EfhUIo86x4n8LznVPEBj5O6TxP4i3/YmAyCbO1t144xzm1qP/BGT/gpX4kuWk8a/wDBfn4qapC3DWtv4Vm0lD/wLStVtXH4NVvwf/wQM8OaB4it/GvjnXfg78Sdet23LrXxg+F/ijxbMX67tmq+MJ4lOQD8iLzTp53lmXzU8sy6jTmtqlXmxVVPvGWIvSg/OGGTXRk08jwvtFUrylVktnN3t6LRL5JPzOf+D3xq/Ya+HfjfUNK/4I8/span+0x8YLl2t9Y+MF9qM97pVjMRh5tS8W6j5ilcHebexaVpNpVUU4x9J/sof8E+vFXhf4vf8No/tx/FG2+KPxznspLXTNRtbJrfQfA9nL/rNP0KzckwIRhJLuQm4nC/MyhmQ9joHwm/bs8KaNbeHPC/7QvwN03TrOIRWdhYfs/apDDAg6KiJ4oCqB6AYq5/wg//AAUL/wCjoPgz/wCGH1b/AOaivKzLNczznFPE4+vOrUenNOTk7LZK70S6RioxS0UUj14U4U48sVZHs1FfNfwT1L/go/8AFfwbe+KNY+Ofwe0SW18XeINGSzm+BesOZItO1i80+K5y3iZTieO1ScDBAEwAZwAx67/hB/8AgoX/ANHQfBn/AMMPq3/zUVwFns1FeM/8IP8A8FC/+joPgz/4YfVv/moo/wCEH/4KF/8AR0HwZ/8ADD6t/wDNRQB7NRXjP/CD/wDBQv8A6Og+DP8A4YfVv/moo/4Qf/goX/0dB8Gf/DD6t/8ANRQB7NRXzX8N9S/4KP8Ajrxl8QPC+ofHP4PabF4M8XQ6NY3knwL1hhqsT6PpuoG5UHxMoUCS/kgwC4zbE7gSVXrv+EH/AOChf/R0HwZ/8MPq3/zUUAezUV4z/wAIP/wUL/6Og+DP/hh9W/8Amoo/4Qf/AIKF/wDR0HwZ/wDDD6t/81FAHs1FeM/8IP8A8FC/+joPgz/4YfVv/moo/wCEH/4KF/8AR0HwZ/8ADD6t/wDNRQB7NRXzXo2pf8FH9U+PHiT4PTfHP4PQ2WheEdE1m314/AvWCl5Lf3WqwSWwX/hJgAYV06JyQxJF2uVXAL9d/wAIP/wUL/6Og+DP/hh9W/8AmooA9morxn/hB/8AgoX/ANHQfBn/AMMPq3/zUUf8IP8A8FC/+joPgz/4YfVv/mooA9c1zQ9F8T6LeeG/Emj2uoadqFrJbX9hfW6yw3MMilXikRgVdGUlSpBBBINfl98W/wDgkJ+0P/wT/wDiTqXx5/4JQeJfE7+CdSumvNe+C+g+LE06+0uQklpNGe7Elldw5O46fepj5NsM8W4IPuX/AIQf/goX/wBHQfBn/wAMPq3/AM1FH/CD/wDBQv8A6Og+DP8A4YfVv/morGvh6GJhy1Y3X4p90000/NNHp5XnGaZLiHWwVVwk1Z7OMk94yjJShOLtrGcJLrZNJr5E/Z8/4K6/FbVfEv8AwqzUvH3w+8WeLbQrHe/DX4oRS/DLx5DIT9w2V+JLK8YY+/bSeW3Zhnj6WT/go74N8HgR/tEfs7fFn4blBie/1jwRNqOnBv8AZvNLNzGR7tt9cAV57+0h+zp+0l+0b8UfDf7Nf7RKfs4/EDw9rfhDXNZfUfFX7NeoX0OmzWV1pMCQIsviNgkkw1GR1dXRl+yHAbJKeRj/AIIDfEDwid37OX7aw+DXOfI+FVl4z021HsLSTxjLbKv+ysQX2rk+p4ml/BryS7SSmvvfLL/yZnvviTJcd/yM8rpSl1nQlPDS9eWPtaLf/cKC8j7C8Pftr/sF/HrRpvDNh+0b8ONctr+MxXOh6rrlqrzKeCklrcEOR2IZK888d/8ABLj9nzxj4Yv7T9nbxzfeBdL1jc194b0sW+s+FNQLHJ8/Rb5ZrNhz0iERHUEV8067/wAEFP25/FbN/wAJr/wWq1rxGjf8s/F/wG0TW+PQtqM07N/wIk1zTf8ABth+0eLw31l/wVmfTpW6yaB+zxoWlN/31ZTQsPwNZTo5j7WNXlg5xd4yjKpTnF73jJe9F3192S1OLFZZ4ZZtZ1XiqTWzcKFa3kpwnh6n+fVHSv8A8E5P+Ck37HFw+ofsVfEIWFjG5KaR4B8RkaTICcvJJ4c19p7SN29LO9tQOigYFWrb/gsb/wAFAP2Ztth+2l+wJfatYxy7D4h8NafeaHcMg6yNBdrPYM2Mk7NRx6AZAFfQP+CDv7f3hQBfC/8AwXn+KenKvRLPwrLGuPTaNVwR+FdXpf8AwSK/4KmaOALH/g4E+I5x0+1fDW1uP/Rt81fWw4w4nrq2aU6WMXfEJTqf+FFNYfEP1nOs/N9fnq3DeSYZ/wDCdm9VLpGphnJfhWv+LOw+F3/Bw/8A8EzfHubbxp8QPEvgC7XG+38ZeErkRKe+bqyFzagD1M2K+i/hh+3h+xL8aUi/4VN+1z8NvEMkuNltpXjWxlnBPZohLvU+xANfnx8ef+CO/wC3beeNfAXh3xv/AMFR4fGo8b+L59H1HUtd/Zm8N3jaXGmj6lqAunM3mFgZLGODlk5uVO7ICtyvjX/g1K8X/EEMPE3/AAUX0ptzbj9h/Zx0S059f9HnTFdUMdwriv4+BrUH/wBOsRTqx/8AAcRShO3/AHEb8zidDMKLsq1Kou/LVpv7nzr8T9h4pYp4lnglV0cAo6NkMPUHvTq/GXwt/wAGlvxA8CTfaPAX/BWPxVoEmc79B+HYsjn/ALYakteiaD/wb3ft/wDhaMQeG/8Ag4V+O1lEv3YILS9EY/4AdXI/StqmD4PlC9HG1k+08NH86eJa/A1Xt1H3uW/k5frFH6rUV+aui/8ABFr/AIKhaAQbD/g4f+Lz46fbfBMVz/6Ov2zXUaX/AMEuv+Ct+j4+yf8ABwV42fHT7V8EdFn/APRkzZry6uFyuP8ADxN/WlNf+3MFKp1j+P8AwD6c/bw/5Ihof/ZZvhx/6muiV7NX53fE79iT/gp98O18FeL/AIw/8FctZ+LHhfTvjF4Dm1rwFP8AArRNOGpQjxXpXzG7sz58IhOLgsvGIMNlCwr9Ea4KkYQlaMuZejX5lq4UUUVmMKKKKACiiigAooooAKKKKACiiigAr4//AGcf2i/j/wCJ/jR4E8XeNPinPq3h34seIvHWlx+B5NIsYrfwyuj306WMlvNFAty7eRaPHc/aJZg01wrRiBV8tvsCvKtD/Yp/Zx8MfEXVfit4c8IapYa5q8t3LNcWnjDVY4rR7u7ivL1rOBbkQ2BuriGOW4NskRuWUmbzNzZAOJk8XfGHQP29NE+FnhL9oHW/GWm39jqOr/EHwdqGiaWmn+D9JeN10ySG4trWO5S4lukWJI7ieYzRJeSBV8kEfRdeWeAP2NfgR8LvjLr3x68DWviyy8ReKNan1bxCD8Sddm0/UL2WBbdppdOkvWs2KxJGiDycRLFGIwmxMep0AFFFFABXx/qv7Rfx/g/aJv8Ax3B8U508J6Z+0Lp/wz/4VuNIsTbT2NxpNtI1+1wYPtgvBd3XnjEwg+zRBDCXYzV9gV5fr/7Gv7O/iX4wT/HjVPBuoL4muZDPLdWfinUra3+2fYW08X4tIbhbdb5bNjbreiMXKRBUWUBVAAPnb9p3/gpFo/hD/goV8Nv2Y/CX7S/gvwtp+lfEKy0H4jeHdV1XTl1PXLjUdFvLy2t44rg+dBbwldPYzxBfOnv4IVkzFNG/2zXOeKPhL4B8aTeFrnxVo0t9L4L1pNW8NzT6hOXtr1LWe0EzNvzMfJuZ1IlLgmTcQWAYdHQAUUUUAFfH+q/tF/H+D9om/wDHcHxTnTwnpn7Qun/DP/hW40ixNtPY3Gk20jX7XBg+2C8F3deeMTCD7NEEMJdjNX2BXl+v/sa/s7+JfjBP8eNU8G6gvia5kM8t1Z+KdStrf7Z9hbTxfi0huFt1vls2Nut6IxcpEFRZQFUAA8N+Jf7Xfj2w/bd8QeC9d8S/ETw18Pfh9r/hTRb+88KeHtEn0q7vtYMLRnVZ75JL3yJZLq2tFGnqpgKySzyKsiNH9hV5l4x/Y9/Z88f/ABE0z4p+LvB9/eaxpf8AZxQt4n1FLW9fT5jPYy3tolwLfUJbeY+bFLdRyvG4DKwYAj02gAooooAK+P8A9rX9ov4/+DPiz8RvEvw9+Kc+iaF8GfDvhHVJPCMWkWM0XittS1G6W8jupZ4HnjU21ukVv9mkhKzmR5DMoEQ+wK83+Jv7JH7P/wAYfiZpfxd+IXgme913Slsljlh16+tre8WzumvLNLy1gmSC/W3uWeeFbmOUQyOzxhWYkgHzb/wUx/ae/ah+DfxC1S2+A3izXLTS/B/wnl8Wa1J4c0vR7q20eT7ZIiX3iBdQja4bSfKtrg7NKzfMLa8KqSsZH2zHJHNGssThlZQVZTkEHvXkfxO/YW/Zm+Mk1nc/EvwdrGqSWuinR7l5PG2rxnVtNMplNjqXl3S/2pbb2c+ReedHiSQbcOwPrtABRRRQAV4R+0J4o+L3gf8Aa9+A/wDwjHxi1K18KeMvFGpeH/EfghdKsHtL0ReHta1FLozvbtdpKs1nbACOZI9qMGRtxx7vXmfxk/ZF+Cvx6+I3hX4rfEdfFza14KuvtXhmXQ/iTrukW9nPsljaU21heQwSyNFPNEzyIzPFK0bEoStAHiv7ePx5+P8A4N8X+PLb4PfF1/CFv8JvgLefEZ4E0mxuYvEt4k115VjdtdQStHZqmnyiX7MYZz9rRhMmwBvqzRNS/tnRrTWPsrwfa7WObyZR80e5Q20+4zivNPij+xV+zl8aV0cfFDwhqmrtotpcWcMk/jDVVe9sp5o5prK+ZLkNqNo8kUZa1ujNCdgBTAxXqtABRRRQAV4R+3X4o+L3w80n4d/EH4W/GLUvDkEXxc8J6N4g0W00qwuINestU1/TrCaGd7q3lliVYZ59rW7RPucEvhQD7vXnP7Rf7Knwc/ar0fSdB+M0Piia00PVYdS06Hw58QNZ0HbdwzRTwTudLu7czPFLDFJGZC3lugZNpyaAMP8Abb1++8C/B25+JK/tL+KPhva6KrHzPCGhaXqF7rN5Ltis7GKHULO686SWdljjghRZZZJERXGcHufgdf8AxT1T4KeD9T+Oek2Wn+Nrnwtp8vjGw01t1tbaq1tGbuOI5OY1mMiqcngDk9a4/wCKn7EvwB+NMHhyL4iweM7uTwlrk2seHLy0+KXiCzurG9ktzbNMk9tfRy58lnRQWIQSybQpkct6Z4c0Gx8K+HrDwxpk97LbabZRWtvLqWpT3lw6RoEUy3Fw7yzyEAbpJHZ3OWZmYkkAu0UUUAFeEf8ABS/xR8Xvh3+xF8Q/i78DvjFqXgrxD4J8L3/iC01HTdKsLxro2tpNKtrIl9bzxiJ3CbyqCTCkK6E5r3euH/aH/Z1+Ff7VHwtv/gv8abHWbvw3qqlNUsNF8Walo7XkTIyNDLNp1xBLJCyuwaJnMb8blOBgA5T9uT9qfwx+yd8Fo/FWs+O/DfhvVPE2t2/hzwrq/jDUorPS7TUrlZGW4uppnRFhghinuXQurSLbmJCZJEBzP+CZ3x7v/wBpf9hv4efF7Xvi5YeOdY1DRBHrviXT3syt3exuySl1s1WCOTK/MiKoU8bR0r1X4d/DDw38L9LGkeGtT8Q3MQt4YQ/iLxdqOsS7YlKqfNv7iZ9xB+Z925zguWIBp/wv+GPgf4MfD/Svhb8NdD/s3QtFtRb6ZY/aZZvJjBJ275WZ25J5ZiaAN6iiigArmPjRofi3xH8Ldb0jwL8StS8H6rJZl7TxHpFlaXFzZFGDsY47yGaBmZVZP3kbgb84JArp6xPiN8P/AA/8VPBOofD7xVc6vDp2qQiK7k0HxDe6VdhQwbEd3YyxXEJJABMcikglTkEggHzleftc/FXwP/wR68NftialcDWfHF/8GfD2rT3raUJFl1W/s7RWumtbcIHVZ7gymGMIGClF2ZBHf/sV/EbWPFvhzxT4K8c+NPiLqfivwp4jW08Q2XxR0zQrbU9OM1nbXEMa/wBgxpZS27xSrKjo0jAyOjvujKJq/D/9i79nr4afBj/hnfw94d1268E/8Iy/h5fDniLxzrGr266Yy7Psyi/upiqqmEQgho0ARCqgCui+C/wB+F3wA0nUtK+GmlajG2tal9v1nUtb8Q32r3+oXIhjgWS4vL+aa4mKwwxRLvkYIkaquFUCgDsqKKKACqPibTdV1nw3qGkaF4im0i+urGWGz1a3gjlkspWQqk6pKrRuyMQwV1KkrggjIq9VXXdGtPEWiXnh/UJrqOC+tZLeeSwv5bWdUdSpMc0LJJC4BOJI2V1OCpBANAHz9+yz8ZfifL+wdr/xQ+KXxYt9Z8Q+FtU8dWcnjHxXZW9rFJFpGu6tZ21xeJYQwxKqwWkPmGGJMhWIXJrkP+CaXx3/AGl/iN4x8T+Bv2ltX8XLdweAfCviDTdK+IOl6Jb6n5l9/aK3dzbnRY1tzprvbRJBHP8A6dE8Vx9oRA8O72D4QfsQ/s8fA3wfefD7wHpPimXQdRutRuNQ0XxL8R9d1u1unvzI155kWo3s6uszyyyOrAqZJXkxvZmOr8CP2U/gd+zZNqF58JvDmpwXWp2VnZXd/rninUtYufsdp5v2W0SbULieSK2h86YxwIyxIZpCqguxIB6LRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABWN8R/HOifDD4ea98S/EsuzTvDui3Wp6g+cbYLeFpXOf91DWzWH8Tvhv4M+Mfw38QfCL4j6OdR8PeKdEutI17Txcyw/arO5iaGaLzImWRN0bsu5GVhnIIPNAHy1+wL+2n4g/aY+IXgf4fv8ftE8V3mhfs76XrXxOXR1syt34mv54o3P7lAYXtWsr1Ht49ojN7GJU3eVj7BpFVUUIigADAAHAFLQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAH//2Q=="
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You should get something that looks like this:\n",
    "![MLEexample.jpg](attachment:MLEexample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can just read off the maximum likelihood solution.  Use `np.argsort()` to figure out the argument of the largest value and print that index of `xgrid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "indices = ____.____(____)\n",
    "index_max = ____[____]\n",
    "print(\"Likelihood is maximized at %.3f\" % ____[____])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantifying Estimated Uncertainty (Ivezic 4.2.5)  <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "Our ML estimate of $\\mu$ is not perfect. The uncertaintly of the estimate is captured by the likelihood function, but we'd like to quantify it with a few numbers.\n",
    "\n",
    "We *define* the uncertainty on our MLEs as second (partial) derivatives of log-likelihood:\n",
    "\n",
    "$$\\sigma_{jk} = \\left( - \\frac{d^2}{d\\theta_j} \\frac{\\ln L}{d\\theta_k} \\Biggr\\rvert_{\\theta=\\theta_0}\\right)^{-1/2}.$$\n",
    "\n",
    "The marginal error bars for each parameter, $\\theta_i$ are given by the diagonal elements, $\\sigma_{ii}$, of this **covariance matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our example, the uncertainly on the mean is \n",
    "$$\\sigma_{\\mu} = \\left( - \\frac{d^2\\ln L(\\mu)}{d\\mu^2}\\Biggr\\rvert_{\\mu_0}\\right)^{-1/2}$$\n",
    "\n",
    "We find\n",
    "$$\\frac{d^2\\ln L(\\mu)}{d\\mu^2}\\Biggr\\rvert_{\\mu_0} = - \\sum_{i=1}^N\\frac{1}{\\sigma^2} = -\\frac{N}{\\sigma^2},$$\n",
    "since, again, $\\sigma = {\\rm constant}$.  \n",
    "\n",
    "Then $$\\sigma_{\\mu} = \\frac{\\sigma}{\\sqrt{N}}.$$\n",
    "\n",
    "So, our estimator of $\\mu$ is $\\overline{x}\\pm\\frac{\\sigma}{\\sqrt{N}}$, which is the result that we are already familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The result for $\\sigma_{\\mu}$ has been derived by expanding $\\ln L$ in a Taylor series and retaining terms up to second order (essentially, $\\ln L$ is approximated by a parabola, or an ellipsoidal surface in multidimensional cases, around its maximum). If this expansion is exact (as is the case for a Gaussian error distribution), then we've completely captured the error information.\n",
    "\n",
    "In general, this is not the case and the likelihood surface can significantly deviate from a smooth elliptical surface. Furthermore, it often happens in practice that the likelihood surface is multimodal. It is always a good idea to visualize the likelihood surface when in doubt (see examples in 5.6 in the textbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confidence Intervals <a class=\"anchor\" id=\"five\"></a>\n",
    "\n",
    "The $(\\mu_0 - \\sigma_\\mu, \\mu_0 + \\sigma_\\mu)$ range gives us a **confidence interval**.\n",
    "\n",
    "In frequentist interptetation, if we repeated the same measurement a hundred times, we'd find that 68 experiments yield a result within the computed confidence interval ($1 \\sigma$ errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confidence Estimates: Bootstrap and Jackknife (Ivezic 4.5)\n",
    "\n",
    "We often assume that the distribution is Gaussian and our samples are large, but even if that is not the case, we can still compute good confidence intervals (e.g., $a<x<b$ with 95\\% confidence) using **resampling** strategies.\n",
    "\n",
    "Remember that we have a data set $\\{x_i\\}$ from which we have estimated the distribution as $f(x)$ for a true distribution $h(x)$.  \n",
    "\n",
    "In **bootstrapping** we map the uncertainty of the parameters by re-sampling from our distribution (with replacement) $B$ times, such that we obtain $B$ measures of our parameters.   So, if we have $i=1,\\dots,N$ data points in $\\{x_i\\}$, we draw $N$ of them to make a new sample, where some values of $\\{x_i\\}$ will be used more than once.\n",
    "\n",
    "The **jackknife** method is similar except that we don't use a sample size of $N$, rather we leave off one or more of the observations from $\\{x_i\\}$.  As with bootstrap, we do this multiple times, generating samples from which we can determine our uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When to bootstrap or jackknife?\n",
    "\n",
    "- Jackknife estimates are usually easier to calculate, easier to apply for complex sampling schemes, and automtically remove bias.\n",
    "- Bootstrap is better for computing confidence intervals because it maps out the full distribution of the statistic instead of assuming asymptotic normality.\n",
    "- Bootstrap is random resampling so gives different results each time. Whereas jackknifing gives repeatable results.\n",
    "\n",
    "It is generally a good idea to use both methods and compare the results. Use either/both with caution with $N$ is small!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An example of bootstrap is given below, using [astroML.resample.bootstrap](http://www.astroml.org/modules/generated/astroML.resample.bootstrap.html), where the arguments are 1) the data, 2) the number of bootstrap resamples to use, and 3) the statistic to be computed.\n",
    "\n",
    "You'll get some more practice with this in a homework assignment based on Data Camp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 4.3, modified slightly by GTR\n",
    "# %load ../code/fig_bootstrap_gaussian.py\n",
    "\"\"\"\n",
    "Bootstrap Calculations of Error on Mean\n",
    "---------------------------------------\n",
    "Figure 4.3.\n",
    "\n",
    "The bootstrap uncertainty estimates for the sample standard deviation\n",
    ":math:`\\sigma` (dashed line; see eq. 3.32) and :math:`\\sigma_G` (solid line;\n",
    "see eq. 3.36). The sample consists of N = 1000 values drawn from a Gaussian\n",
    "distribution with :math:`\\mu = 0` and :math:`\\sigma = 1`. The bootstrap\n",
    "estimates are based on 10,000 samples. The thin lines show Gaussians with\n",
    "the widths determined as :math:`s / \\sqrt{2(N - 1)}` (eq. 3.35) for\n",
    ":math:`\\sigma` and :math:`1.06 s / \\sqrt{N}` (eq. 3.37) for :math:`\\sigma_G`.\n",
    "\"\"\"\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from astroML.resample import bootstrap\n",
    "from astroML.stats import sigmaG\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "#if \"setup_text_plots\" not in globals():\n",
    "#    from astroML.plotting import setup_text_plots\n",
    "#setup_text_plots(fontsize=12, usetex=True)\n",
    "\n",
    "m = 1000  # number of points\n",
    "n = 10000  # number of bootstraps\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# sample values from a normal distribution\n",
    "np.random.seed(123)\n",
    "data = norm(0, 1).rvs(m)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute bootstrap resamplings of data\n",
    "mu1_bootstrap = bootstrap(data, n, np.std, kwargs=dict(axis=1, ddof=1))\n",
    "mu2_bootstrap = bootstrap(data, n, sigmaG, kwargs=dict(axis=1))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the theoretical expectations for the two distributions\n",
    "xgrid = np.linspace(0.8, 1.2, 1000)\n",
    "\n",
    "sigma1 = 1. / np.sqrt(2 * (m - 1))\n",
    "pdf1 = norm(1, sigma1).pdf(xgrid)\n",
    "\n",
    "sigma2 = 1.06 / np.sqrt(m)\n",
    "pdf2 = norm(1, sigma2).pdf(xgrid)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "ax.hist(mu1_bootstrap, bins=50, density=True, histtype='step',\n",
    "        color='blue', ls='dashed', label=r'$\\sigma\\ {\\rm (std. dev.)}$')\n",
    "ax.plot(xgrid, pdf1, color='gray')\n",
    "\n",
    "ax.hist(mu2_bootstrap, bins=50, density=True, histtype='step',\n",
    "        color='red', label=r'$\\sigma_G\\ {\\rm (quartile)}$')\n",
    "ax.plot(xgrid, pdf2, color='gray')\n",
    "\n",
    "ax.set_xlim(0.82, 1.18)\n",
    "\n",
    "ax.set_xlabel(r'$\\sigma$')\n",
    "ax.set_ylabel(r'$p(\\sigma|x,I)$')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "From Ivezic: \"The bootstrap uncertainty estimates for the sample standard deviation $\\sigma$ (dashed line; see Eq. 3.32) and $\\sigma_G$ (solid line; see Eq. 3.36). The sample consists of N = 1000 values drawn from a Gaussian distribution with $\\mu = 0$ and $\\sigma = 1$. The bootstrap estimates are based on 10,000 samples. The thin grey lines show Gaussians with the widths determined as $s / \\sqrt{2(N - 1)}$ (Eq. 3.35) for $\\sigma$ and $1.06 s / \\sqrt{N}$ (Eq. 3.37) for $\\sigma_G$.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLE applied to a Heteroscedastic Gaussian (Ivezic 4.2.6) <a class=\"anchor\" id=\"six\"></a>\n",
    "\n",
    "Now let's look at a case where the errors are heteroscedastic.  For example if we are measuring the length of a [rod](https://www.nist.gov/image/meter27jpg) and have $N$ measurements, $\\{x_i\\}$, where the error for each measurement, $\\sigma_i$ is known.  Since $\\sigma$ is **not** a constant, then following the above, we have\n",
    "\n",
    "$$\\ln L = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma_i^2}.$$\n",
    "\n",
    "Taking the derivative:\n",
    "$$\\frac{d\\;{\\rm lnL}(\\mu)}{d\\mu}\\Biggr\\rvert_{\\hat \\mu} = \\sum_{i=1}^N \\frac{(x_i - \\hat \\mu)}{\\sigma_i^2} = 0,$$\n",
    "then simplifying:\n",
    "\n",
    "$$\\sum_{i=1}^N \\frac{x_i}{\\sigma_i^2} = \\sum_{i=1}^N \\frac{\\hat \\mu}{\\sigma_i^2},$$\n",
    "\n",
    "yields a MLE solution of \n",
    "$$\\hat \\mu = \\frac{\\sum_i^N (x_i/\\sigma_i^2)}{\\sum_i^N (1/\\sigma_i^2)},$$\n",
    "\n",
    "which is just a variance-weighted mean, with uncertainty\n",
    "$$\\sigma_{\\mu} = \\left( \\sum_{i=1}^N \\frac{1}{\\sigma_i^2}\\right)^{-1/2}.$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aside: Cost Functions <a class=\"anchor\" id=\"seven\"></a>\n",
    "\n",
    "Recall from Lecture 3 that we measured the deviation from expected in three different ways\n",
    "$$d_i = x_i - \\mu,$$  \n",
    "$$|x_i-\\mu|,$$\n",
    "and\n",
    "$$(x_i-\\mu)^2.$$\n",
    "\n",
    "When trying to determine the best model parameters, we have to specify a \"cost function\", which is basically the prescription for evaluating the difference (distance) between our estimator and the true value.  This is otherwise known as the \"norm\", or the total length of the distances.  The first form above represents the $L_0$ norm, while the next two are the $L_1$ norm (the \"taxi-cab\" norm) and the $L_2$ norm (the \"as the crow flies\" norm).  \n",
    "\n",
    "So far we have been using an $L_2$ norm (which comes about simply from the definition of a Gaussian with the $(x_i-\\mu)^2$ term).  Later in the course we will encounter machine learning algorithms that allow us to specify different norms (different cost functions).\n",
    "\n",
    "The [$L_p$ norm](https://www.wikiwand.com/en/Lp_space) can be defined as $\\sum_{i=1}^N (y_i -M(x_i))^p/\\sigma^p$.\n",
    "\n",
    "See [this Medium article](https://medium.com/@montjoile/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c) and Ivezic, 4.2.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Truncated/Censored Data\n",
    "\n",
    "Note that knowing how to deal with missing data points (\"censored data\") is often quite important, but adds complications that we don't have time to get into here.  For more, see Ivezic 4.2.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"Goodness\" of Fit (Ivezic 4.3)  <a class=\"anchor\" id=\"eight\"></a>\n",
    "\n",
    "The MLE approach tells us what the \"best\" model parameters are, but not how good the fit actually is.  If the model is wrong, \"best\" might not be particularly revealing (\"Garbage In = Garbage Out\")!  For example, if you have $N$ points drawn from a linear distribution, you can always fit the data perfectly with an $N-1$ order polynomial.  But that won't help you predict future measurements.\n",
    "\n",
    "We can describe the **goodness of fit** as \n",
    "> whether or not it is likely to have obtained $\\ln L_0$ by randomly drawing from the data.  That means that we need to know the *distribution* of $\\ln L$ and not just the maximum.  \n",
    "\n",
    "For the Gaussian case we have just described, we do a standard transform of variables and compute the so-called $z$ score for each data point (basically the number of standard deviations away from the mean that this point is), writing \n",
    "$$z_i = (x_i-\\mu)/\\sigma,$$ then\n",
    "$$\\ln L = {\\rm constant} - \\frac{1}{2}\\sum_{i=1}^N z^2 = {\\rm constant} - \\frac{1}{2}\\chi^2.$$\n",
    "\n",
    "Here, $\\chi^2$ is the thing whose distribution we discussed last week.\n",
    "\n",
    "So for Gaussian uncertainties, $\\ln L$ is distributed as $\\chi^2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The mean of the $\\chi^2$ distribution is $N-k$ and its standard deviation is $\\sqrt{2(N-k)}$.\n",
    "\n",
    "Let's visualize that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell to make the plot\n",
    "# %load ../code/fig_chi2_distribution.py\n",
    "\"\"\"\n",
    "Example of a chi-squared distribution\n",
    "---------------------------------------\n",
    "Figure 3.14.\n",
    "\n",
    "This shows an example of a :math:`\\chi^2` distribution with various parameters.\n",
    "\"\"\"\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "#from astroML.plotting import setup_text_plots\n",
    "#setup_text_plots(fontsize=12, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define the distribution parameters to be plotted\n",
    "k_values = [1, 2, 5, 7]\n",
    "linestyles = ['-', '--', ':', '-.']\n",
    "mu = 0\n",
    "xplot = np.linspace(-1, 10, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(5, 3.75))\n",
    "fig.subplots_adjust(bottom=0.12)\n",
    "\n",
    "for k, ls in zip(k_values, linestyles):\n",
    "    dist = chi2(k, mu)\n",
    "    idx = np.argsort(dist.pdf(xplot))[-1]\n",
    "    #print(f\"The peak Q value for {k} degrees of freedom is {xplot[idx]}.\")\n",
    "    print(\"The peak Q value for {0:d} degrees of freedom is {1:3.2f}.\".format(k,xplot[idx]))\n",
    "                                                                                       \n",
    "    plt.plot(xplot, dist.pdf(xplot), ls=ls, c='black',\n",
    "             label=r'$k=%i$' % k)\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 0.5)\n",
    "\n",
    "plt.xlabel('$Q$')\n",
    "plt.ylabel(r'$p(Q|k)$')\n",
    "plt.title(r'$\\chi^2\\ \\mathrm{Distribution}$')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We define the $\\chi^2$ per degree of freedom, $\\chi^2_{dof}$, as\n",
    "$$\\chi^2_{dof} = \\frac{1}{N-k}\\sum_{i=1}^N z^2_i.$$\n",
    "\n",
    "For a good fit, we would expect that $\\chi^2_{dof}\\approx 1$.  If $\\chi^2_{dof}$ is significantly larger than 1, then it is likely that we are not using the correct model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also get overly high or low values of $\\chi^2_{dof}$ if our errors are under- or over-estimated as shown below:\n",
    "\n",
    "![Ivezic, Figure 4.1](http://www.astroml.org/_images/fig_chi2_eval_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Statistical Inference (Ivezic 5.0, 5.1)  <a class=\"anchor\" id=\"nine\"></a>\n",
    "\n",
    "Up to now in this lecture we have been computing the **likelihood** $p(D|M)$.  In Bayesian inference, we instead evaluate the **posterior probability** taking into account **prior** information.\n",
    "\n",
    "Recall from the BasicStats lecture that Bayes' Rule is:\n",
    "$$p(M|D) = \\frac{p(D|M)p(M)}{p(D)},$$\n",
    "where $D$ is for data and $M$ is for model.\n",
    "\n",
    "We wrote this in words as:\n",
    "$${\\rm Posterior Probability} = \\frac{{\\rm Likelihood}\\times{\\rm Prior}}{{\\rm Evidence}}.$$\n",
    "\n",
    "If we explicitly recognize prior information, $I$, and the model parameters, $\\theta$, then we can write:\n",
    "$$p(M,\\theta|D,I) = \\frac{p(D|M,\\theta,I)p(M,\\theta|I)}{p(D|I)},$$\n",
    "where we can omit the explict dependence on $\\theta$ by writing $M$ instead of $M,\\theta$ where appropriate.  \n",
    "\n",
    "Note that it is often that case that $p(D|I)$ is not evaluated explictly since the likelihood can be normalized such that the \"evidence\" is unity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Bayesian Statistical Inference process is then\n",
    "* formulate the likelihood, $p(D|M,\\theta,I)$, which is what we have been talking about so far today\n",
    "* chose a prior, $p(M,\\theta|I)$, which incorporates other information beyond the data in $D$\n",
    "* determine the posterior pdf, $p(M,\\theta|D,I)$\n",
    "* search for the model paramters that maximize the posterior pdf\n",
    "* quantify the uncertainty of the model parameter estimates\n",
    "* perform model selection to find the most appropriate description of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does our answer change for our earlier example if we include a Bayesian prior (like we assumed for the IQ problem) and instead maximize the posterior probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Things that we have already done above, but need to reset here\n",
    "N = 3 #Complete\n",
    "mu = 1.0\n",
    "sigma = 0.2 \n",
    "np.random.seed(42)\n",
    "sample = norm(mu,sigma).rvs(N)\n",
    "\n",
    "xgrid = np.linspace(0,2,1000)\n",
    "L1 = norm.pdf(xgrid,loc=sample[0],scale=sigma)\n",
    "L2 = norm.pdf(xgrid,loc=sample[1],scale=sigma)\n",
    "L3 = norm.pdf(xgrid,loc=sample[2],scale=sigma)\n",
    "\n",
    "#New things\n",
    "Prior = norm.pdf(____,loc=____,scale=____) #Prior PDF\n",
    "Post1 = ____*____ #Posterior PDF for the first measurement\n",
    "Post2 = ____*____\n",
    "Post3 = ____*____\n",
    "Post = ____*____*____ #Total posterior PDF for all the measurements\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "plt.plot(xgrid, Post1, ls='-', c='green', label=r'$P(x_1)$')\n",
    "plt.plot(xgrid, ____, ls='-', c='red', label=r'$P(x_2)$')\n",
    "plt.plot(xgrid, ____, ls='-', c='blue', label=r'$P(x_3)$')\n",
    "plt.plot(xgrid, Post/5, ls='-', c='black', label=r'$P(\\{x\\})$') #Scaled for the sake of display\n",
    "\n",
    "plt.xlim(0.2, 1.8)\n",
    "plt.ylim(0, 10.0)\n",
    "plt.xlabel('$\\mu$') #Leave out or adjust if no latex\n",
    "plt.ylabel(r'$p(\\mu,\\sigma|x_i)$') #Leave out or adjust if no latex\n",
    "plt.title('MLE for Gaussian Distribution')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "idx = np.argsort(Post)\n",
    "print(\"Posterior PDF is maximized at %.3f\" % xgrid[idx[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "See what happens when you have just 2 measurements, but one has a much larger error than the other (i.e., the errors are heteroscedastic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "L1 = norm.pdf(xgrid,loc=____,scale=____) #Measurement with small error\n",
    "L2 = norm.pdf(xgrid,loc=____,scale=____) #Measurement with large error (give it a very different location parameter)\n",
    "L = L1 * L2 \n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "plt.plot(xgrid, L1, ls='-', c='green', label=r'$L(x_1): good$')\n",
    "plt.plot(xgrid, L2, ls='-', c='red', label=r'$L(x_2): lousy$')\n",
    "plt.plot(xgrid, L, ls='-', c='black', label=r'$L(\\{x\\}): weighted$')\n",
    "\n",
    "plt.xlim(0.2, 1.8)\n",
    "plt.ylim(0, 9.5)\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.ylabel(r'$p(x_i|\\mu,\\sigma)$')\n",
    "plt.title('Weighted measurements (not normalized)')\n",
    "plt.legend()\n",
    "\n",
    "idx = np.argsort(L)\n",
    "print(\"Likelihood is maximized at %.3f\" % xgrid[idx[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Priors (Ivezic 5.2)  <a class=\"anchor\" id=\"ten\"></a>\n",
    "\n",
    "Priors can be **informative** or **uninformative**.  As it sounds, informative priors are based on existing information that might be available.  Uninformative priors can be thought of as \"default\" priors, i.e., what your prior is if you weren't explicitly including a prior, e.g, a \"flat\" prior like $p(\\theta|M,I) \\propto {\\rm C}$.\n",
    "\n",
    "For the IQ test example, what kind of prior did we use?\n",
    "\n",
    "In a *hierarchical Bayesian* analysis the priors themselves can have parameters and priors (hyperparameters and hyperpriors), but let's not worry about that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While determining good priors is important for Bayesian analysis, I don't want to get distracted by it here.  You can read more about it in Ivezic, 5.2.  However, I'll briefly introduce 3 principles here.\n",
    "\n",
    "### The Principle of Indifference\n",
    "\n",
    "Essentially this means adopting a uniform prior, though you have to be a bit careful.  Saying that an asteroid is equally likely to hit anywhere on the Earth is not the same as saying that all latitudes of impact are equally likely.  Assuming $1/6$ for a six-side die would be an example of indifference.\n",
    "\n",
    "### The Principle of Invariance (or Consistency)\n",
    "\n",
    "This applies to location and scale invariance.  \n",
    "\n",
    "**Location invariance** suggests a uniform prior, within the accepted bounds: $p(\\theta|I) \\propto 1/(\\theta_{max}-\\theta_{min})$ for $\\theta_{min} \\le \\theta \\le \\theta_{max}$. \n",
    "\n",
    "**Scale invariance** gives us priors that look like $p(\\theta|I) \\propto 1/\\theta$, which implies a uniform\n",
    "prior for ln($\\theta$), i.e. a prior that gives equal weight over many orders of magnitude. \n",
    "\n",
    "### The Principle of Maximum Entropy\n",
    "\n",
    "The principle of maximum entropy is discussed in Ivezic, 5.2.2.\n",
    "It is often true that Bayesian analysis and traditional MLE are essentially equivalent.  However, in some cases, considering the priors can have significant consequences. \n",
    "See Ivezic $\\S$5.5 for such an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analysis of a Heteroscedastic Gaussian distribution with Bayesian Priors (Ivezic 5.6)  <a class=\"anchor\" id=\"eleven\"></a>\n",
    "\n",
    "Consider the case of measuring a rod as above.  We want to know the posterior pdf for the length of the rod, $p(M,\\theta|D,I) = p(\\mu|\\{x_i\\},\\{\\sigma_i\\},I)$.\n",
    "\n",
    "For the likelihood we have\n",
    "$$L = p(\\{x_i\\}|\\mu,I) = \\prod_{i=1}^N \\frac{1}{\\sigma_i\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma_i^2}\\right).$$\n",
    "\n",
    "In the Bayesian case, we also need a prior.  We'll adopt a uniform distribution given by\n",
    "$$p(\\mu|I) = C, \\; {\\rm for} \\; \\mu_{\\rm min} < \\mu < \\mu_{\\rm max},$$\n",
    "where $C = \\frac{1}{\\mu_{\\rm max} - \\mu_{\\rm min}}$ between the min and max and is $0$ otherwise.\n",
    "\n",
    "The log of the posterior pdf is then\n",
    "$$\\ln L = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma_i^2}.$$\n",
    "\n",
    "This is exactly the same as we saw before, except that the value of the constant is different.  Since the constant doesn't come into play, we get the same result as before:\n",
    " \n",
    "$$\\hat \\mu = \\frac{\\sum_i^N (x_i/\\sigma_i^2)}{\\sum_i^N (1/\\sigma_i^2)},$$\n",
    "with uncertainty\n",
    "$$\\sigma_{\\mu} = \\left( \\sum_{i=1}^N \\frac{1}{\\sigma_i^2}\\right)^{-1/2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We get the same result because we used a flat prior.  In the case of homoscedastic instead of heteroscedastic, we obviously would get the result from our first example.\n",
    "\n",
    "Now let's consider the case where $\\sigma$ is not known, but rather needs to be determined from the data.  In that case, the posterior pdf that we seek is not $p(\\mu|\\{x_i\\},\\{\\sigma_i\\},I)$, but rather $p(\\mu,\\sigma|\\{x_i\\},I)$.\n",
    "\n",
    "As before we have\n",
    "$$L = p(\\{x_i\\}|\\mu,\\sigma,I) = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right),$$\n",
    "except that now $\\sigma$ is uknown instead of given (meaning we need to move it to the left of the \"pipe\").\n",
    "\n",
    "Our Bayesian prior is now 2D instead of 1D and we'll adopt \n",
    "$$p(\\mu,\\sigma|I) \\propto \\frac{1}{\\sigma},\\; {\\rm for} \\; \\mu_{\\rm min} < \\mu < \\mu_{\\rm max} \\; {\\rm and} \\; \\sigma_{\\rm min} < \\sigma < \\sigma_{\\rm max}.$$\n",
    "\n",
    "That is, all values of $\\mu$ are equally likely (within the range indicated), but we'll down-weight the likelihood of large errors (again limiting $\\sigma$ to some range).  Note that the ranges actually drop out since they are constants.\n",
    "\n",
    "With proper normalization, we have\n",
    "$$p(\\{x_i\\}|\\mu,\\sigma,I)p(\\mu,\\sigma|I) = C\\frac{1}{\\sigma^{(N+1)}}\\prod_{i=1}^N \\exp\\left( \\frac{-(x_i-\\mu)^2}{2\\sigma^2}  \\right),$$\n",
    "where\n",
    "$$C = (2\\pi)^{-N/2}(\\mu_{\\rm max}-\\mu_{\\rm min})^{-1} \\left[\\ln \\left( \\frac{\\sigma_{\\rm max}}{\\sigma_{\\rm min}}\\right) \\right]^{-1}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The log of the posterior pdf is\n",
    "\n",
    "$$\\ln[p(\\mu,\\sigma|\\{x_i\\},I)] = {\\rm constant} - (N+1)\\ln\\sigma - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma^2}.$$\n",
    "\n",
    "Right now that has $x_i$ in it, which isn't that helpful, but since we are assuming a Gaussian distribution, we can take advantage of the fact that the mean, $\\overline{x}$, and the variance, $V (=s^2)$, completely characterize the distribution.  So we can write this expression in terms of those variables instead of $x_i$.  Skipping over the math details (see Ivezic $\\S$5.6.1), we find\n",
    "\n",
    "$$\\ln[p(\\mu,\\sigma|\\{x_i\\},I)] = {\\rm constant} - (N+1)\\ln\\sigma - \\frac{N}{2\\sigma^2}\\left( (\\overline{x}-\\mu)^2 + V  \\right).$$\n",
    "\n",
    "Note that this expression only contains the 2 parameters that we are trying to determine: $(\\mu,\\sigma)$ and 3 values that we can determine directly from the data: $(N,\\overline{x},V)$.\n",
    "\n",
    "Load and execute the next cell to visualize the posterior pdf for the case of $(N,\\overline{x},V)=(10,1,4)$.  Try changing the values of $(N,\\overline{x},V)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# %load code/fig_likelihood_gaussian.py\n",
    "\"\"\"\n",
    "Log-likelihood for Gaussian Distribution\n",
    "----------------------------------------\n",
    "Figure5.4\n",
    "An illustration of the logarithm of the posterior probability density\n",
    "function for :math:`\\mu` and :math:`\\sigma`, :math:`L_p(\\mu,\\sigma)`\n",
    "(see eq. 5.58) for data drawn from a Gaussian distribution and N = 10, x = 1,\n",
    "and V = 4. The maximum of :math:`L_p` is renormalized to 0, and color coded as\n",
    "shown in the legend. The maximum value of :math:`L_p` is at :math:`\\mu_0 = 1.0`\n",
    "and :math:`\\sigma_0 = 1.8`. The contours enclose the regions that contain\n",
    "0.683, 0.955, and 0.997 of the cumulative (integrated) posterior probability.\n",
    "\"\"\"\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "#from astroML.plotting import setup_text_plots\n",
    "#setup_text_plots(fontsize=14, usetex=True)\n",
    "\n",
    "\n",
    "def gauss_logL(xbar, V, n, sigma, mu):\n",
    "    \"\"\"Equation 5.57: gaussian likelihood\"\"\"\n",
    "    return (-(n + 1) * np.log(sigma)\n",
    "            - 0.5 * n * ((xbar - mu) ** 2 + V) / sigma ** 2)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define the grid and compute logL\n",
    "sigma = np.linspace(1, 5, 70)\n",
    "mu = np.linspace(-3, 5, 70)\n",
    "xbar = 1\n",
    "V = 4\n",
    "n = 10\n",
    "\n",
    "logL = gauss_logL(xbar, V, n, sigma[:, np.newaxis], mu)\n",
    "logL -= logL.max()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(5, 3.75))\n",
    "plt.imshow(logL, origin='lower',\n",
    "           extent=(mu[0], mu[-1], sigma[0], sigma[-1]),\n",
    "           cmap=plt.cm.binary,\n",
    "           aspect='auto')\n",
    "plt.colorbar().set_label(r'$\\log(L)$')\n",
    "plt.clim(-5, 0)\n",
    "\n",
    "plt.contour(mu, sigma, convert_to_stdev(logL),\n",
    "            levels=(0.683, 0.955, 0.997),\n",
    "            colors='k')\n",
    "\n",
    "plt.text(0.5, 0.93, r'$L(\\mu,\\sigma)\\ \\mathrm{for}\\ \\bar{x}=1,\\ V=4,\\ n=10$',\n",
    "         bbox=dict(ec='k', fc='w', alpha=0.9),\n",
    "         ha='center', va='center', transform=plt.gca().transAxes)\n",
    "plt.xlabel(r'$\\mu$')\n",
    "plt.ylabel(r'$\\sigma$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The shaded region is the posterior probability.  The contours are the confidence intervals.  We can compute those by determining the marginal distribution at each $(\\mu,\\sigma)$.  The top panels of the figures below show those marginal distributions.  The solid line is what we just computed.  The dotted line is what we would have gotten for a uniform prior--not that much difference.  The dashed line is the MLE result, which is quite different.  The bottom panels show the cumulative distribution.\n",
    "\n",
    "![Ivezic, Figure 5.5](http://www.astroml.org/_images/fig_posterior_gaussian_1.png)\n",
    "\n",
    "\n",
    "Note that the marginal pdfs follow a Student's $t$ Distribution, which becomes Gaussian for large $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The main result here is that **for smallish N ($<$10 or so), $p(\\mu)$ is not Gaussian!** \n",
    "- The code above can be used to compute $p(\\mu)$ for arbitrary values of N, $\\overline{x}$ and V.\n",
    "- For large N, Gaussian is a good approximation of $p(\\mu)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap\n",
    "\n",
    "To review: the Bayesian Statistical Inference process is\n",
    "* formulate the likelihood, $p(D|M,\\theta,I)$\n",
    "* chose a prior, $p(M,\\theta|I)$, which incorporates other information beyond the data in $D$\n",
    "* determine the posterior pdf, $p(M,\\theta|D,I)$\n",
    "* search for the model paramters that maximize the posterior pdf\n",
    "* quantify the uncertainty of the model parameter estimates\n",
    "* test the hypothesis being addressed\n",
    "\n",
    "The last part we haven't talked about yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What if we wanted to model the mixture of a Gauassian distribution with a uniform distribution.  When might that be useful?  Well, for example:\n",
    "\n",
    "![Atlas Higgs Boson Example](https://atlas.cern/sites/atlas-public.web.cern.ch/files/Higgsmass_fig1_comb.jpg)\n",
    "\n",
    "Obviously this isn't exactly a Gaussian and a uniform distribution, but a line feature superimposed upon a background is the sort of thing that a physicist might see and is pretty close to this case for a local region around the feature of interest.  This is the example discussed in Ivezic $\\S$5.6.5.\n",
    "\n",
    "For this example, we will assume that the location parameter, $\\mu$, is known (say from theory) and that the errors in $x_i$ are negligible compared to $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The likelihood of obtaining a measurement, $x_i$, in this example can be written as\n",
    "$$L = p(x_i|A,\\mu,\\sigma,I) = \\frac{A}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right) + \\frac{1-A}{W}.$$\n",
    "\n",
    "Here the background probability is taken to be $0 < x < W$ and 0 otherwise.  The feature of interest lies between $0$ and $W$.  $A$ and $1-A$ are the relative strengths of the two components, which are obviously anti-correlated.  Note that there will be covariance between $A$ and $\\sigma$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we adopt a uniform prior in both $A$ and $\\sigma$:\n",
    "$$p(A,\\sigma|I) = C, \\; {\\rm for} \\; 0\\le A<A_{\\rm max} \\; {\\rm and} \\; 0 \\le \\sigma \\le \\sigma_{\\rm max},$$\n",
    "then the posterior pdf is given by\n",
    "$$\\ln [p(A,\\sigma|\\{x_i\\},\\mu,W)] = \\sum_{i=1}^N \\ln \\left[\\frac{A}{\\sigma \\sqrt{2\\pi}} \\exp\\left( \\frac{-(x_i-\\mu)^2}{2\\sigma^2} \\right)  + \\frac{1-A}{W} \\right].$$\n",
    "\n",
    "The figure below (Ivezic, 5.13) shows an example for $N=200, A=0.5, \\sigma=1, \\mu=5, W=10$.  Specifically, the bottom panel is a result drawn from this distribution and the top panel is the likelihood distribution derived from the data in the bottom panel.\n",
    "![Ivezic, Figure 5.13](http://www.astroml.org/_images/fig_likelihood_gausslin_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A more realistic example might be one where all three parameters are unknown: the location, the width, and the background level.  But that will have to wait until $\\S$5.8.6.\n",
    "\n",
    "In the meantime, note that we have not binned the data, $\\{x_i\\}$.  We only binned Figure 5.13 for the sake of visualizaiton.  However, sometimes the data are inherently binned (e.g., the detector is pixelated).  In that case, the data would be in the form of $(x_i,y_i)$, where $y_i$ is the number of counts at each location.  We'll skip over this example, but you can read about it in Ivezic $\\S$5.6.6.  A refresher on the Poission distribution (Ivezic $\\S$3.3.4) might be appropriate first."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
