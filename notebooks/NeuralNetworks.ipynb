{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Before lecture today, please go to https://playground.tensorflow.org/\n",
    "\n",
    "* Try the default.  Note how the first layer identifies simple patterns and the second layer combines those to make more complex patterns.  It also finds a solution quite quickly.  (Note also that it won't stop on its own.  Hit pause when you are happy with the solution it has converged on.)\n",
    "* Try a ReLU activation function.  Much faster, but with a \"boxy\" solution.\n",
    "* See what happens when there is only 1 hidden layer with 3 neurons.  Reset and run it multiple times.\n",
    "* Compare learning rate 0.001 to 1.0.  \n",
    "* Now try just 2 neurons (with learning rate back at 0.3).\n",
    "* Now try 8 neurons.\n",
    "* Lastly try the spiral dataset with 4 hidden layers, each with 8 neurons.  Run it for at least 1000-1500 epochs -- until it gets the answer \"right\" (make sure that your computer is plugged in and properly cooled!)  This illustrates the vanishing gradient problem.\n",
    "* Do this again after lecture.\n",
    "\n",
    "See Problem 10.1 in Geron\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "381656e7-7071-4f30-8d24-5c50925acedd"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks\n",
    "G. Richards\n",
    "(2016, 2018, 2020)\n",
    "Ivezic 9.8 and Geron\n",
    "\n",
    "I found this video series particularly helpful in trying to simplify the explanation https://www.youtube.com/watch?v=bxe2T-V8XRs. \n",
    "\n",
    "[Artificial Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network) are a simplified computation architecture based loosely on the real neural networks found in brains.  In reality, what we are going to explore is a [multi-layer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron).\n",
    "\n",
    "In the image below the circles on the left represent the **attributes** of our input data, $X$, which here is 3 dimensional.  The circles in the middle represent the neurons.  They take in the information from the input and, based on some criterion decide whether or not to \"fire\".  The collective results of the neurons in the hidden layer produce the output, $y$, which is represented by the circles on the right, which here is 2 dimensional result.  The lines connecting the circles represent the synapses.  This is a simple example with just one layer of neurons; however, there can be many layers of neurons.\n",
    "![Cartoon of Neural Network](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/500px-Artificial_neural_network.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c8a9551b-8231-4274-a9a0-f995a8a6b44a"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In more detail:\n",
    "\n",
    "The job of a synapses is to take input values and multiply them by some weight before passing them to the neuron (hidden layer):\n",
    "\n",
    "$$z = \\sum_i w x_i$$\n",
    "\n",
    "(Each neuron also has a \"bias\", $b$ that we add, but we'll set that to zero for the sake of simplicity here.)\n",
    "\n",
    "The neuron then sums up the inputs from all of the synapses connected to it and applies an \"activation function\".  For example a [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation function.\n",
    "\n",
    "$$a = \\frac{1}{1+e^{-z}}.$$\n",
    "\n",
    "![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/500px-Logistic-curve.svg.png)\n",
    "\n",
    "What the neural network does is to learn the weights of the synapses that are needed to produce an accurate model of $y_{\\rm train}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bc6f4840-b67d-45c4-95e3-b16e574ca650"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Rather than think about the inputs individually, we can write this process in matrix form as\n",
    "$$X W^{(1)} = Z^{(2)}.$$\n",
    "\n",
    "If $D$ is the number of attributes (here 3) and $H$ is the number of neurons in the hidden layer (here 4), then $X$ is an $N\\times D$ matrix, while $W^{(1)}$ is a $D\\times H$ matrix.  The result, $Z^{(2)}$, is then an $N\\times H$ matrix.\n",
    "\n",
    "We then apply the activation function to each entry of $Z^{(2)}$ independently: \n",
    "$$A^{(2)} = f(Z^{(2)}),$$\n",
    "where $A^{(2)}$ is the output of the neurons in the hidden layer and is also $N\\times H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cafa3925-c1d6-43f9-a2ca-df4023cf2909"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These values are then the inputs for the next set of synapses, where we multiply the inputs by another set of weights, $W^{(2)}:$\n",
    "$$A^{(2)} W^{(2)} = Z^{(3)},$$\n",
    "\n",
    "where $W^{(2)}$ is an $H\\times O$ matrix and $Z^{(3)}$ is an $N\\times O$ matrix with $O$-dimensional output.\n",
    "\n",
    "Another activation function is then applied to $Z^{(3)}$ to give\n",
    "$$\\hat{y} = f(Z^{(3)}),$$\n",
    "which is our estimator of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b67de090-530a-4997-b57a-138100035369"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example we might have $N=100$ people for which we have measured \n",
    "* shoe size\n",
    "* belt size\n",
    "* hat size\n",
    "\n",
    "for whom we know their height and weight.  \n",
    "\n",
    "Then we are going to use this to predict the height and weight for people where we only know shoe size, belt size, and hat size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "31662fbe-7216-4291-a3f6-5fa8578b3414"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The neural network then essentially boils down to determining the weights of the synapses, which are usually initialized randomly.\n",
    "\n",
    "We do that by minimizing the cost function (which compares the true values of $y$ to our predicted values).  Typically:\n",
    "$$ {\\rm Cost} = J = \\sum\\frac{1}{2}(y - \\hat{y})^2.$$\n",
    "\n",
    "As we saw above, that would be a cost function for regression (where we have only one output node).  For classification, we'd use one of the examples above (but ideally one that is differentiable as we'll see next time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we just had 1 weight and we wanted to check 1000 possible values, that wouldn't be so bad.  But we have 20 weights, which means checking $20^{1000}$ possible combinations.    Remember the curse of dimensionality?  That might take a while.  Indeed, far, far longer than the age of the Universe.\n",
    "\n",
    "Thus the (first) death of neural networks (which are currently on life #3, which is looking more promising)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Life #2 begins with the realization that we can write an analytic formula for the *gradient* going backwards and use that to update our weights.\n",
    "\n",
    "For example, how about just checking 3 points for each weight and see if we can at least figure out which way is \"down hill\"?  That's a start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can rewrite $J$ as\n",
    "$$ J = \\sum\\frac{1}{2}\\left(y - f\\left( f(X W^{(1)}) W^{(2)} \\right) \\right)^2$$\n",
    "\n",
    "and then compute\n",
    "$$\\frac{\\partial J}{\\partial W}$$\n",
    "in order to determine the slope of the cost function for each weight.  This is the **gradient descent** method, which we encountered before.  Your choice of cost function is important here; specifically you want it to be differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll want $\\partial J/\\partial W^{(1)}$ and $\\partial J/\\partial W^{(2)}$ separately.  This allows us to [*backpropagate*](https://en.wikipedia.org/wiki/Backpropagation) the error contributions along each neuron and to change the weights where they most need to be changed.  It is like each observation gets a vote on which way is \"down hill\".  We compute the vector sum to decide the ultimate down hill direction.\n",
    "\n",
    "Once we know the down hill direction from the derivative, we update the weights by subtracting a scalar times that derivative from the original weights.  That's obviously much faster than randomly sampling all the possible combinations of weights.  Once the weights are set, then you have your Neural Network classifier/regressor.\n",
    "\n",
    "![Cartoon of Neural Network](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/500px-Artificial_neural_network.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scikit-Learn has both [unsupervised Neural Network](http://scikit-learn.org/stable/modules/neural_networks_unsupervised.html#neural-networks-unsupervised) and [supervised Neural Network](http://scikit-learn.org/stable/modules/neural_networks_supervised.html#neural-networks-supervised) examples. \n",
    "\n",
    "Let's try to use the multi-layer perceptron classifier on the Boston House Price dataset (using 75% of the data for training and 25% for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston = load_boston()\n",
    "#print boston.DESCR\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "Xtrain_scaled = preprocessing.scale(Xtrain)\n",
    "Xtest_scaled = preprocessing.scale(Xtest)\n",
    "Xscaled = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "clf = MLPRegressor(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=1, max_iter=5000)\n",
    "clf.fit(Xtrain_scaled, ytrain)\n",
    "\n",
    "# Look at the weights\n",
    "print([coef.shape for coef in clf.coefs_])\n",
    "\n",
    "ypred = clf.predict(Xtest_scaled)\n",
    "#print ypred, ytest\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.scatter(ytest,ypred)\n",
    "plt.xlabel(\"Actual Value [x$1000]\")\n",
    "plt.ylabel(\"Predicted Value [x$1000]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Of course, that only predicts the value for a fraction of the data set.  Again, we can use Scikit-Learn's [cross_val_predict](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html#sklearn.model_selection.cross_val_predict) to make predictions for the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "yCVpred = cross_val_predict(clf, Xscaled, y, cv=3) # Complete\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y,yCVpred)\n",
    "plt.xlabel(\"Actual Value [x$1000]\")\n",
    "plt.ylabel(\"Predicted Value [x$1000]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Just as with other model hyperparameters, we can use cross-validation to determine the optimal number of layers, neurons per layer, etc.  We'll do that in detail later.  For now, let's talk about some guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Number of Layers\n",
    "\n",
    "Ivezic: \n",
    "> \"For data that can be represented by a linear model, no layers are required (McCullagh & Nelder 1989).  A single layer network can approximate any continuous function.  Two layers can represent arbitrary decision boundaries for smooth functions (Lippmann 1987).  More layers can represent non-continuous or complex structure within the data.\"  \n",
    "\n",
    "\n",
    "Think about why no layers are needed for linear regression.  We just connect our input to output where they synapses are the weights (slopes) and the output neurons add the constant (intecept).\n",
    "\n",
    "So you might start with a single layer, then add more layers and use cross-validation to determine when you are overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Geron has a good example of what adding layers does.  Say you had to draw a whole forest, but you couldn't cut and paste anything.  That would be very tedious.  But if you could draw just one leaf and copy that to make a small branch, then scale up a small branch to a big branch and copy that, then attach the branches to a tree trunk and then make copies of the full tree, you wouldn't have to draw all that much.  Each of the layers in a neural network handles  more and more detailed aspects of the problem.  We saw this in the opening pre-lecture experiments.\n",
    "\n",
    "For image recognition, you might need dozens of layers, but also a huge training set to populate those layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Number of Neurons\n",
    "\n",
    "\n",
    "The number of neurons in each layer is also a free parameter.  Typically choose somewhere between twice the number of input nodes and a number between the number of input and output nodes.\n",
    "\n",
    "If there are lots of hidden layers (where \"lots\" is not clearly defined) then we call that a deep neural network or [deep learning](https://en.wikipedia.org/wiki/Deep_learning).  More on that later.\n",
    "\n",
    "\n",
    "Sometimes the number of neurons in each layer goes down.  But it can also be useful to have the same number in each layer so that there is only one hyperparameter (the number of neurons) and not one per layer.\n",
    "\n",
    "In practice a reasonable approach is to simply specify many more layers and neurons than you need and perform regularization.  This can be as simple as just stopping the training when the cross-validation error reaches a minimum, which appropriately (for once) is called [early stopping](https://en.wikipedia.org/wiki/Early_stopping).  Basically, you put your `fit` method into a loop and instantiate with `max_iter=1` and `warm_start=True`.  See Geron (page 141) for an example.  Indeed the examples from the start of class where we had to stop the process is a good illustration of this.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While the number of neurons in the hidden layers are free parameters the number of input and output nodes are constrained by the data and the desired output.  For example, the MNIST digits data requires 784 input neurons (one for each pixel in the 28x28 images) and 10 output neurons (one for each class [digit]).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activation Functions\n",
    "\n",
    "[Activation function](https://en.wikipedia.org/wiki/Activation_function) control how much \"signal\" it takes for a neuron to \"fire\".  The more signal, the more likely the neuron will fire.\n",
    "\n",
    "See\n",
    "https://mlfromscratch.com/activation-functions-explained/#/\n",
    "\n",
    "The cells below show different activation functions, using the same visualization as we used for loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Mathematical formulas for activation functions\n",
    "  \n",
    "def binary(raw_model_output):\n",
    "   return np.where(raw_model_output < 0, 0, 1)  \n",
    "\n",
    "def sigmoid(raw_model_output):\n",
    "   return 1.0/(1+np.exp(-raw_model_output))\n",
    "\n",
    "def ReLU(raw_model_output):\n",
    "   return np.where(raw_model_output < 0, 0, raw_model_output)\n",
    "\n",
    "def LReLU(raw_model_output):\n",
    "   alpha=0.1\n",
    "   return np.where(raw_model_output < 0, alpha*raw_model_output, raw_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a grid of values and plot\n",
    "grid = np.linspace(-3,3,1000)\n",
    "#plt.plot(grid, log_loss(grid), label='logistic')\n",
    "plt.plot(grid, binary(grid), \"k\", label='binary')\n",
    "plt.plot(grid, sigmoid(grid), \"b\", label='sigmoid')\n",
    "#plt.plot(grid, ReLU(grid), label='ReLU')\n",
    "#plt.plot(grid, LReLU(grid), label='LReLU')\n",
    "#plt.plot(grid, l2(grid), label='l2')\n",
    "#plt.plot(grid, l1(grid), label='l1')\n",
    "\n",
    "plt.fill_between([0,3],-1,3,\"b\",alpha=0.4)\n",
    "plt.fill_between([-3,0],-1,3,\"r\",alpha=0.5)\n",
    "plt.xlim([-3,3])\n",
    "plt.ylim([-1,3])\n",
    "plt.xlabel(\"z\",fontsize=12)\n",
    "#plt.ylabel(\"probability\",fontsize=12)\n",
    "plt.title(\"don't fire                 fire\",fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Until the paper describing backpropagation, a step (binary) function was used for activation: that is either *on* or *off*, which makes sense as neurons either fire or they don't.  However, the step function doesn't have gradients, so backpropagation won't work with them.  \n",
    "\n",
    "Moreover on vs. off actually isn't quite true.  The way your eyes work is that you need 1-10 photons to trigger a rod, but several rods must be triggered to send a signal to the brain.  The sigmoid activation function captures the probabilistic nature of neuron firing.  More importantly it is differentiable, so can be used for backpropagation.\n",
    "\n",
    "Another important aspect of non-linear activation function is that they are what allow neural networks to solve non-linear problems.  If we used a strictly linear activation function, then we could only solve linear problems.  That is, you could fit a straight line, but not an exponential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Vanishing and Exploding Gradients\n",
    "\n",
    "However, neural networks suffered their 2nd death at the hands of the [vanishing and exploding gradients](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) problem.  We won't go into detail there except to say that life #3 began around 2010 when suggestions for different activation functions were published.  \n",
    "\n",
    "For example, the Rectified Linear Unit (ReLU) activation function is another commonly used activation function as it solves the vanishing gradient problem (since the gradient is only 0 or 1).\n",
    "\n",
    "$${\\rm ReLU}(z) = max(0,z)$$\n",
    "\n",
    "It isn't ideal both because the derivative is 0 for $z<0$ and it can end up producing dead nodes.  However, it is fast and the resulting sparsity can be good (sort of like regularization).\n",
    "\n",
    "A number of papers since 2015 describe various improvements, including the Leaky RELU, the exponential linear unit (ELU) and scaled exponential linear unit (SELU).\n",
    "\n",
    "Note that the activation functions can be different in different layers.  For example, for regression, one typically doesn't use any activation function in the output layer as including one would restriction the range of possible outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a grid of values and plot\n",
    "grid = np.linspace(-3,3,1000)\n",
    "#plt.plot(grid, log_loss(grid), label='logistic')\n",
    "#plt.plot(grid, binary(grid), \"k\", label='binary')\n",
    "#plt.plot(grid, sigmoid(grid), \"b\", label='sigmoid')\n",
    "plt.plot(grid, ReLU(grid), \"b--\", label='ReLU')\n",
    "plt.plot(grid, LReLU(grid), \"orange\", label='LReLU')\n",
    "#plt.plot(grid, l2(grid), label='l2')\n",
    "#plt.plot(grid, l1(grid), label='l1')\n",
    "\n",
    "plt.fill_between([0,3],-1,3,\"b\",alpha=0.4)\n",
    "plt.fill_between([-3,0],-1,3,\"r\",alpha=0.5)\n",
    "plt.xlim([-3,3])\n",
    "plt.ylim([-1,3])\n",
    "plt.xlabel(\"z\",fontsize=12)\n",
    "#plt.ylabel(\"probability\",fontsize=12)\n",
    "plt.title(\"don't fire                 fire\",fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some general guidance on activation functions (more on this later):\n",
    "    \n",
    "* Use sigmoid needed for output of binary classification (with binary cross entropy loss)\n",
    "* Use ReLU for layers other than output (at least to start with because it is faster)\n",
    "* Use softmax for output with more than 2 classes (with categorical cross entropy loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularization\n",
    "\n",
    "Just as we can use regularization for standard regression and classification task, so can we with neural networks.  \n",
    "\n",
    "Not only can we apply the usual $l1$ or $l2$ regularization techniques, but we can also use *dropout* which, as the name indicates, causes some neurons to be temporarily \"dropped out\" during a training set (usually by setting some probability for that to happen, typically 10-50%).  After training, all of the neurons are used.\n",
    "\n",
    "Geron explains this as a company needing to try to figure out how to adapt to a crucial employee being out sick for a period of time.  In the end, it can make the company stronger as more people (neurons) are able to handle certain parts of the process.\n",
    "\n",
    "We'll do an example of this next time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Batch normalization\n",
    "\n",
    "Just as it is often necessary to normalize or standardize our features, sometimes it is helpful to do the same to the output of the hidden layers.  This is called [batch normalization](https://en.wikipedia.org/wiki/Batch_normalization) and is done before passing the data to the activation function.  It make the process more stable and can also make it faster.  We'll do an example next time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster Optimizers\n",
    "\n",
    "We aren't going to talk about optimizers, but it might be useful for you to have some options to feed into a search for the best parameter using cross validation.  For example `['mse', 'adam', 'sgd', 'adagrad']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advanced Topics in Neural Networks with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recent interest in neural networks surged in 2012 when a team using a deep [convolutional neural network (CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network) acheived record results classifying objects in the [ImageNet](http://image-net.org/) data set.\n",
    "\n",
    "The idea behind CNNs is inspired by human visual perception.  Each neuron in your visual cortex doesn't \"see\" all of what your eye can see at once and some neuron are more sensitive to one pattern over another (e.g., horizontal lines vs. vertical lines).  \n",
    "\n",
    "Moreover, the simplest deeply connected neural networks would choke on \"real\" data which has far more than 28x28 pixels and would require following tens of millions of connections.  So we use a combination of \"convolution\" and \"pooling\" to reduce the dimensionality of the data first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolutional Layers\n",
    "\n",
    "In a convolutional layer, each neuron is not connected to each neuron in the previous layer, but only those that are within its \"field of view\" as defined by a kernel (filter).  We slide the kernel over the input layer and the value in the next layer depends only on those pixels.\n",
    "\n",
    "![Convolutional Layer](https://miro.medium.com/max/1400/1*wLlXFtWI--7knyQT2wlhMg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's another perspective that helps to visualize going from one layer to the next.\n",
    "\n",
    "![Convolution example](https://developer.apple.com/library/content/documentation/Performance/Conceptual/vImage/Art/kernel_convolution.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Choosing a filter (kernel) with a certain pattern can help recognize certain types of features (like horizontal aor vertical lines).\n",
    "\n",
    "![vertical filter](https://miro.medium.com/max/1338/1*7IEbBib7Yc7qST5IGQoYXA.jpeg)\n",
    "\n",
    "![horizontal filter](https://miro.medium.com/max/1238/1*PSSAaH2pZbl5bK3Ef_zk4A.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is common to follow the convolutional layer by a so-called \"pooling layer\", essentially to reduce the amount of data that needs to be processed.  The full diagram of a CNN might look something like this:\n",
    "![CNN Example](https://www.researchgate.net/profile/Xian_Wei2/publication/331986652/figure/fig1/AS:740547106988032@1553571597647/The-classic-structure-of-CNN-It-consists-of-two-modules-Feature-extraction-module-and.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Where the pooling layers are reducing the number of pixels by averaging, summing, taking the max, etc.:\n",
    "\n",
    "![Pooling example](https://miro.medium.com/max/1000/1*ydNsGDxMldAiq7b96GDQwg.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When you are done, the output gets feed into a regular, fully connected neural network which outputs the predictions.\n",
    "\n",
    "This is clearly much more sophisticated than our basic perceptron. \"Deep\" networks consist of tens of layers with thousands of neurons. These large networks have become usabel thanks to two breakthroughs: the use of sparse layers and the power of graphics processing units (GPUs).\n",
    "\n",
    "The sparse layers or convolutional layers in a deep network contain a large number of hidden nodes but very few synapses. The sparseness arises from the relatively small size of a typical convolution kernel (15x15 is a large kernel), so a hidden node representing one output of the convolution is connected to only a few input nodes. Compare this the our previous perceptron, in which every hidden node was connected to every input node.\n",
    "\n",
    "Even though the total number of connections is greatly reduced in the sparse layers, the total number of nodes and connections in a modern deep network is still enormous. Luckily, training these networks turns out to be a great task for GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll see a worked example of a CNN next time.\n",
    "\n",
    "For further study, there are lots of resources for CNNs online.  For example, see\n",
    "https://medium.com/analytics-vidhya/convolutional-neural-networks-cnn-explained-step-by-step-69137a54e5e7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Autoencoders\n",
    "\n",
    "Autoencoders are neural networks that copy their input to their output, but after passing the data through a bottleneck.  For example if there are 28x28 = 784 inputs, there will also be 784 outputs, but there will be one or more (odd, but symmetric) hidden layers with fewer neuron than that.\n",
    "\n",
    "For example see \n",
    "\n",
    "![autoencoder structure](https://miro.medium.com/max/1400/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png)\n",
    "\n",
    "from\n",
    "https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can think of this as doing PCA with a neural network -- breaking our data down into the only the most important features that we actually *need* (finding the intrinsic dimensionality).   In fact, if network uses only linear (or no) activation functions and $l2$ cost function, then we have exactly PCA.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How might this be useful?  Well, for example we can use it to reconstruct MNIST digits that have had noise added to them:\n",
    "\n",
    "![autoencoder example](https://miro.medium.com/max/1400/1*SxwRp9i23OM0Up4sEze1QQ@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More exciting to me is the possibility to use autoencoders for \"unsupervised pretraining\".  For example you have data that is only partially labeled (at least not enough to do traditional supervised classification).  We can train an autoencoder on the full data set, then used the encoder part as the base of a regular neural network that is trained on the labeled data that we do have.  See Geron Figure 17.6\n",
    "\n",
    "Also for [anomaly detection](https://scikit-learn.org/stable/modules/outlier_detection.html.\n",
    "See also\n",
    "https://www.pyimagesearch.com/2020/03/02/anomaly-detection-with-keras-tensorflow-and-deep-learning/ and \n",
    "https://towardsdatascience.com/autoencoder-neural-network-for-anomaly-detection-with-unlabeled-dataset-af9051a048."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generative Adversarial Networks (GANs)\n",
    "\n",
    "GANs are pure evil, see\n",
    "https://thispersondoesnotexist.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But they are also brilliant, incredibly useful, and relatively new (2014).  The idea builds logically on autoencoders in that we have a generator (like the decoder part of an autoencoder) that can produce fake data (e.g., an image).  Then we have a discriminator (a standard binary classifier) that tries to distinguish fake data from real.  Then the generator learns to produce more and more accurate images to trick the discriminator -- without ever seeing any real images -- it just has the feedback from the discriminator.\n",
    "\n",
    "See Geron Figure 17.15 for a cartoon of the structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For more, see\n",
    "https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Astronomy has seen some interesting uses of autoencoders and GANs in recent years.  For example:\n",
    "    \n",
    "https://arxiv.org/abs/1702.00403    \n",
    "https://www.aanda.org/articles/aa/full_html/2017/07/aa30240-16/aa30240-16.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nbpresent": {
   "slides": {
    "a6340146-092b-47d0-8584-e84bb64c0952": {
     "id": "a6340146-092b-47d0-8584-e84bb64c0952",
     "prev": null,
     "regions": {
      "9f3c3dc8-1276-4b96-9b97-e4b352cc7fb5": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0.008349570712902259,
        "y": -0.008482103581361025
       },
       "id": "9f3c3dc8-1276-4b96-9b97-e4b352cc7fb5"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
