{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization and Nonlinear Regression\n",
    "G. Richards\n",
    "(2016,2018,2020,2022)\n",
    "based on materials from Connolly (especially) and Ivezic Chapter 8.3, 8.5-8.7, 8.9, 8.10.  With updates to my own class from [Stephen Taylor's class at Vanderbilt](https://github.com/VanderbiltAstronomy/astr_8070_s22)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents\n",
    "* [Regularization](#one)\n",
    "* [Non-linear regression](#two)\n",
    "* [Outliers](#three)\n",
    "* [Gaussian process regression (GPR)](#four)\n",
    "* [Uncertainties all round](#five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization  <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "We have to be a little careful when doing regression because if we progressively increase the complexity of the model, then we reach a regime where we are overfitting the data (i.e. there are too many degrees of freedom in the model).  This causes high variance in the model, resulting in poor performance in cross-validation and testing datasets.\n",
    "\n",
    "Let's look at an example using `Polynomial Regression`. We'll fit some data with successively higher-degree polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from astroML.linear_model import PolynomialRegression\n",
    "\n",
    "orders=[3,4,5]\n",
    "\n",
    "def f(x):\n",
    "    \"\"\" function to approximate by polynomial interpolation\"\"\"\n",
    "    return np.sin(x)\n",
    "\n",
    "# generate points used to plot\n",
    "x_plot = np.linspace(0, 8, 100)\n",
    "\n",
    "# generate points and keep a subset of them\n",
    "x = np.linspace(0, 8, 10)\n",
    "#rng = np.random.RandomState(0)\n",
    "#rng.shuffle(x)\n",
    "#x = np.sort(x[:10])\n",
    "y = f(x)+0.25*(np.random.random(len(x))-0.5)\n",
    "\n",
    "# create matrix versions of these arrays\n",
    "X = x[:, None]\n",
    "X_plot = x_plot[:, None]\n",
    "\n",
    "colors = ['teal', 'yellowgreen', 'gold']\n",
    "lw = 2\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw, label=\"ground truth\")\n",
    "plt.scatter(x, y, color='navy', s=30, marker='o', label=\"training points\")\n",
    "\n",
    "for count, degree in enumerate(orders):\n",
    "    poly = PolynomialRegression(degree)\n",
    "    poly.fit(X,y)\n",
    "    y_plot = poly.predict(X_plot)\n",
    "\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw, label=\"degree %d\" % degree)\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is fit with order = 3, 4, and 5.  What happens if you make the order $\\sim N_{\\rm points}$?  Try it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For cases where we are concerned with overfitting, instead of making our log-likelihood be related to just the sum of the squares: \n",
    "\n",
    "$$(Y - M\\theta)^T C^{-1} (Y - M\\theta),$$\n",
    "\n",
    "we can apply additional constraints (usually of **smoothness**, **number of coefficients**, **size of coefficients**):\n",
    "\n",
    "$$(Y - M \\theta)^T(Y- M \\theta) + \\lambda \\theta^T \\theta,$$\n",
    "\n",
    "with $\\lambda$ as the **regularization parameter**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Upon maximizing the modified log-likelihood (or minimizing the modified relationship in the previous equation), this leads to a solution for the parameters of the model\n",
    "\n",
    "$$\\theta = (M^T C^{-1} M + \\lambda I)^{-1} (M^T C^{-1} Y)$$\n",
    "\n",
    "with $I$ the identity matrix.\n",
    "\n",
    "From the Bayesian perspective this is the same as applying a prior like:\n",
    "\n",
    "$$p(\\theta | I ) \\propto \\exp{\\left(\\frac{-(\\lambda \\theta^T \\theta)}{2}\\right)}$$\n",
    "\n",
    "which, when multiplied by the likelihood for regression, gives the same answer for the maximum likelihood of the posterior probability as described above."
   ]
  },
  {
   "attachments": {
    "towardsdatascience_ridge.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Reguarlization tries to get us into the Goldilocks regime where we have a [balanced solution](https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db) to the regression between underfitting and overfitting. \n",
    "![towardsdatascience_ridge.png](attachment:towardsdatascience_ridge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge regression \n",
    "\n",
    "The case for such a Gaussian prior corresponds to [Ridge Regression](https://en.wikipedia.org/wiki/Tikhonov_regularization), which\n",
    "penalizes the regression coefficients according to\n",
    "\n",
    "$$ \\theta^T \\theta \\equiv \\sum_i \\theta_i^2 < s.$$\n",
    "\n",
    "That is **the sum of the squares of the regression coefficients are restricted to be less than some tolerance value**, $s$. We'll come back to exactly what $s$ is in a minute. Doing this supresses large regression coefficients and limits the variance of the model---at the cost of increased bias (i.e. the solution may not pass as closely through all data points).\n",
    "\n",
    "The following figure illustrates the interaction of the prior and the posterior without the prior. In the left panel, the ridge prior drags the solution to smaller values of the regression coefficients than they would otherwise have been with an uninformative prior:\n",
    "![Ivezic, Figure 8.3](http://www.astroml.org/_images/fig_lasso_ridge_1.png)\n",
    "\n",
    "Scikit-Learn's [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) is their implementation of ridge regression, while AstroML implements Ridge Regression as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "X = np.random.random((100,10))\n",
    "y = np.dot(X, np.random.random(10))\n",
    "model = Ridge(alpha=0.05) #alpha here is lambda in the book\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following example compares Gaussian Basis Regression with and without the constraints from Ridge Regression.  It uses 100 evenly spaced Gaussians, which we can see strongly overfits the problem and has very large coefficient values, until a constraint is imposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "#from astroML.cosmology import Cosmology\n",
    "from astroML.datasets import generate_mu_z\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# generate data\n",
    "np.random.seed(0)\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0)\n",
    "\n",
    "from astropy.cosmology import LambdaCDM\n",
    "cosmo = LambdaCDM(H0=70, Om0=0.30, Ode0=0.70, Tcmb0=0)\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu = cosmo.distmod(z)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Manually convert data to a gaussian basis\n",
    "#  note that we're ignoring errors here, for the sake of example.\n",
    "def gaussian_basis(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "centers = np.linspace(0, 1.8, 100)\n",
    "widths = 0.2\n",
    "X = gaussian_basis(z_sample[:, None], centers, widths)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Set up the figure to plot the results\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "classifier = [LinearRegression, Ridge]\n",
    "kwargs = [dict(), dict(alpha=0.005)]\n",
    "labels = ['Gaussian Basis Regression', 'Ridge Regression']\n",
    "\n",
    "for i in range(2):\n",
    "    clf = classifier[i](fit_intercept=True, **kwargs[i])\n",
    "    clf.fit(X, mu_sample)\n",
    "    w = clf.coef_\n",
    "    fit = clf.predict(gaussian_basis(z[:, None], centers, widths))\n",
    "\n",
    "    # plot fit\n",
    "    ax = fig.add_subplot(221 + i)\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    # plot curves for regularized fits\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('$\\mu$')\n",
    "    else:\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "        curves = 37 + w * gaussian_basis(z[:, np.newaxis], centers, widths)\n",
    "        curves = curves[:, abs(w) > 0.01]\n",
    "        ax.plot(z, curves,\n",
    "                c='gray', lw=1, alpha=0.5)\n",
    "\n",
    "    ax.plot(z, fit, '-k')\n",
    "    ax.plot(z, mu, '--', c='gray')\n",
    "    ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1, ms=4)\n",
    "    ax.set_xlim(0.001, 1.8)\n",
    "    ax.set_ylim(36, 52)\n",
    "    ax.text(0.05, 0.93, labels[i],\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w'),\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "    # plot weights\n",
    "    ax = plt.subplot(223 + i)\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "    ax.set_xlabel('$z$')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r'$\\theta$')\n",
    "        w *= 1E-12\n",
    "        ax.text(0, 1.01, r'$\\rm \\times 10^{12}$',\n",
    "                transform=ax.transAxes)\n",
    "    ax.scatter(centers, w, s=9, lw=0, c='k')\n",
    "\n",
    "    ax.set_xlim(-0.05, 1.8)\n",
    "\n",
    "    if i == 1:\n",
    "        ax.set_ylim(-2, 4)\n",
    "    elif i == 2:\n",
    "        ax.set_ylim(-0.5, 2)\n",
    "\n",
    "    ax.text(0.05, 0.93, labels[i],\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w'),\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least absolute shrinkage and selection (LASSO) regularization\n",
    "\n",
    "An alternative to Ridge Regression is LASSO, which implies the following contraint\n",
    "\n",
    "$$(Y - M \\theta)^T(Y- M \\theta) + \\lambda \\sum_i|\\theta_i|.$$\n",
    "\n",
    "This is equivalent to least-squares minimization with the restriction that\n",
    "\n",
    "$$ \\sum_i|\\theta_i| < s,$$\n",
    "that is, the penalty is on the absolute values of the regression coefficients, which is also illustrated in Ivezic, Figure 8.3 as shown in the right panel above.\n",
    "\n",
    "It not only weights the regression coefficients, it also imposes sparsity on the regression\n",
    "model (i.e. the penalty preferentially selects regions of likelihood space that coincide with one of the vertices within the region defined by the regularization).\n",
    "\n",
    "This has the effect of setting one (or more) of the model attributes to zero.  \n",
    "\n",
    "[Scikit-Learn's `LASSO`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "XX = np.random.random((100,10))\n",
    "yy = np.dot(XX, np.random.random(10))\n",
    "model = Lasso(alpha = 0.05)\n",
    "model.fit(XX,yy)\n",
    "y_pred = model.predict(XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below I have copied the Ridge regression cell from above.  Modify the three lines of code needed to replace Ridge regression with Lasso regression.  Then experiment with different values of the regularization parameter.\n",
    "\n",
    "N.B.  $\\lambda$ in the book is related to $\\alpha$ in these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge ####\n",
    "\n",
    "#from astroML.cosmology import Cosmology\n",
    "from astroML.datasets import generate_mu_z\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# generate data\n",
    "np.random.seed(0)\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0)\n",
    "\n",
    "from astropy.cosmology import LambdaCDM\n",
    "cosmo = LambdaCDM(H0=70, Om0=0.30, Ode0=0.70, Tcmb0=0)\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu = cosmo.distmod(z)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Manually convert data to a gaussian basis\n",
    "#  note that we're ignoring errors here, for the sake of example.\n",
    "def gaussian_basis(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "centers = np.linspace(0, 1.8, 100)\n",
    "widths = 0.2\n",
    "X = gaussian_basis(z_sample[:, None], centers, widths)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Set up the figure to plot the results\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "classifier = [LinearRegression, Ridge] ####\n",
    "kwargs = [dict(), dict(alpha=0.005)] ####\n",
    "labels = ['Gaussian Basis Regression', 'Ridge Regression'] ####\n",
    "\n",
    "for i in range(2):\n",
    "    clf = classifier[i](fit_intercept=True, **kwargs[i])\n",
    "    clf.fit(X, mu_sample)\n",
    "    w = clf.coef_\n",
    "    fit = clf.predict(gaussian_basis(z[:, None], centers, widths))\n",
    "\n",
    "    # plot fit\n",
    "    ax = fig.add_subplot(221 + i)\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    # plot curves for regularized fits\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('$\\mu$')\n",
    "    else:\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "        curves = 37 + w * gaussian_basis(z[:, np.newaxis], centers, widths)\n",
    "        curves = curves[:, abs(w) > 0.01]\n",
    "        ax.plot(z, curves,\n",
    "                c='gray', lw=1, alpha=0.5)\n",
    "\n",
    "    ax.plot(z, fit, '-k')\n",
    "    ax.plot(z, mu, '--', c='gray')\n",
    "    ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1, ms=4)\n",
    "    ax.set_xlim(0.001, 1.8)\n",
    "    ax.set_ylim(36, 52)\n",
    "    ax.text(0.05, 0.93, labels[i],\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w'),\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "    # plot weights\n",
    "    ax = plt.subplot(223 + i)\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(0.5))\n",
    "    ax.set_xlabel('$z$')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r'$\\theta$')\n",
    "        w *= 1E-12\n",
    "        ax.text(0, 1.01, r'$\\rm \\times 10^{12}$',\n",
    "                transform=ax.transAxes)\n",
    "    ax.scatter(centers, w, s=9, lw=0, c='k')\n",
    "\n",
    "    ax.set_xlim(-0.05, 1.8)\n",
    "\n",
    "    if i == 1:\n",
    "        ax.set_ylim(-2, 4)\n",
    "    elif i == 2:\n",
    "        ax.set_ylim(-0.5, 2)\n",
    "\n",
    "    ax.text(0.05, 0.93, labels[i],\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', ec='k', fc='w'),\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Did you try `alpha=0`?  If not, go ahead and do that.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Are you totally confused?  Don't worry, it is much simpler than it seems.  I found [Hastie](http://statweb.stanford.edu/~tibs/ElemStatLearn/index.html) to be helpful in sorting this out. \n",
    "\n",
    "They write the constraint term as \n",
    "$$\\lambda \\sum_{j=1}^p |\\theta_j|^q,$$\n",
    "which allows us to see that Ridge regression corresponds to $q=2$, while LASSO regression corresponds to $q=1$.  So, they are really the same thing: Bayes estimates with different priors.   The wildly different names are just a nuisance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With that in mind, now let's see if we can understand what is going on in Ivezic, Figure 8.3.  We now have\n",
    "\n",
    "$$ \\lambda \\sum_{i=1} |\\theta_i|^q < s.$$\n",
    "\n",
    "Think of $s$ as a normalized distance where $s=1$ corresponds to there being no constraint on $\\theta_i$.  Requiring $s<1$ limits the magnitude of $\\theta_i$.  So, in this figure\n",
    "![Ivezic, Figure 8.3](http://www.astroml.org/_images/fig_lasso_ridge_1.png)\n",
    "$s=1$ would make the circle/diamond big enough to include what they call $\\theta_{\\rm normal}$.  \n",
    "\n",
    "It isn't obvious to me, but I guess that the $\\theta_i$ are normalized such that the contraint region is symmetric.  \n",
    "\n",
    "Shrinking $s$ has the effect of adding a prior that moves the best-fit parameters to the intersection of the two sets of contours.  The difference between Ridge and LASSO is just the shape of the constraint region.  For LASSO, the shape is such that some of the parameters may end up being 0.  \n",
    "\n",
    "One drawback of LASSO is that it does not have a closed-form analytic regression like Ridge does-- the problem must be solved numerically.\n",
    "\n",
    "Figures 3.8 (page 84 of the [Hastie PDF](http://statweb.stanford.edu/~tibs/ElemStatLearn/index.html)), 3.10 (page 89), and 3.12 (page 91) may be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we choose $\\lambda$?\n",
    "\n",
    "Use cross-validation as discussed last time.  \n",
    "\n",
    "In fact...`Scikit-Learn` has versions of Ridge and LASSO regression that do this automatically for you-- see [`RidgeCV`](https://scikit-learn.org/stable/modules/linear_model.html#setting-the-regularization-parameter-leave-one-out-cross-validation) and [`LassoCV`](https://scikit-learn.org/stable/modules/linear_model.html#using-cross-validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's practice using some Housing data.\n",
    "\n",
    "Because the attributes (columns of $X$) are inhomogenous, some may be more relevant than others.  So LASSO might be a good thing to try in such situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Execute this cell to read in the data\n",
    "#Also identify the index of the \"Number of Rooms\" attribute\n",
    "import numpy as np\n",
    "#from sklearn.datasets import load_boston\n",
    "#boston = load_boston()\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "print(housing.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Start by looking at just how the number of rooms predicts the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "RMindex = np.argwhere(np.array(housing.feature_names)==\"AveRooms\")[0,0]\n",
    "print(RMindex)\n",
    "\n",
    "X_RM = housing.data[:,RMindex][:,None]\n",
    "y = housing.target\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X_RM, y)\n",
    "print(reg.coef_, reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_RM,y)\n",
    "plt.xlabel(\"Number of Rooms\")\n",
    "plt.ylabel(\"House Value (/100,000)\")\n",
    "\n",
    "Xgrid = np.linspace(1,9,9)\n",
    "ypred = reg.intercept_ + Xgrid*reg.coef_[0]\n",
    "plt.plot(Xgrid,ypred,c='r')\n",
    "plt.xlim(0,50)\n",
    "#plt.ylim(0,52)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Now use **all attributes**, first with `LinearRegression`.  \n",
    "- Then with `LinearRegression again`, but with the data scaled using `StandardScaler` from `sklearn.preprocessing` (since the features are heterogeneous, i.e. they vary over different ranges and scales). \n",
    "- Then fit the scaled data agin with LASSO to see which features aren't that important. Experiment with the $\\alpha$ value. \n",
    "- We'll plot the coefficients of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import ___\n",
    "\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "___ = StandardScaler()\n",
    "Xscaled = scaler.fit_transform(___)\n",
    "\n",
    "linreg = LinearRegression().fit(___,___)\n",
    "linreg_scaled = LinearRegression().fit(___,___)\n",
    "lasso_scaled = Lasso(alpha=___).fit(___,___)\n",
    "\n",
    "print(linreg.coef_, linreg.intercept_)\n",
    "print(linreg_scaled.coef_, linreg_scaled.intercept_)\n",
    "print(lasso_scaled.coef_, lasso_scaled.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5)) \n",
    "plt.subplots_adjust(hspace=0.001)\n",
    "\n",
    "x_pos = np.arange(len(housing.feature_names))\n",
    "\n",
    "for i in range(3):\n",
    "    # plot theta\n",
    "    ax = plt.subplot(311 + i)\n",
    "    ax.set_ylabel(r'$\\theta$')\n",
    "\n",
    "    ax.set_xticks(x_pos)\n",
    "    if i == 2:\n",
    "        ax.set_xticklabels(housing.feature_names, rotation=60)     \n",
    "    else: \n",
    "        ax.set_xticklabels([])\n",
    "    \n",
    "    ax.set_xlim(-0.5, 12.5)\n",
    " \n",
    "    if i == 0:\n",
    "        ax.bar(x_pos, linreg.coef_, alpha=0.5, label=\"LinReg\")\n",
    "        #ax.set_ylim(-2, 4)\n",
    "        plt.legend(loc=4)\n",
    "    elif i == 1:\n",
    "        ax.bar(x_pos, linreg_scaled.coef_, alpha=0.5, label=\"LinRegScaled\")\n",
    "        #ax.set_ylim(-2, 4)\n",
    "        plt.legend(loc=1)\n",
    "    elif i == 2:\n",
    "        ax.bar(x_pos, lasso_scaled.coef_, alpha=0.5, label=\"LASSO\")\n",
    "        #ax.set_ylim(-0.5, 2)\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which attribute coefficients are driven toward zero the most, and thus which attributes don't matter in determining the housing prices?\n",
    "\n",
    "Let's try `LassoCV` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso_scaled_cv = LassoCV(cv=5).fit(Xscaled,y)\n",
    "\n",
    "print(lasso_scaled_cv.coef_, lasso_scaled_cv.intercept_)\n",
    "print(lasso_scaled_cv.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Up to now we have been fitting using *linear* models.  Before moving on to *non-linear* models, we'll look at local linear fitting.  Kernel Regression (or Nadaraya-Watson) was one such method.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Locally Linear Regression (LOWESS or LOESS)\n",
    "\n",
    "In [Local Linear Regression](https://en.wikipedia.org/wiki/Local_regression) we assume that the regression function at any point can\n",
    "be approximated by a [Taylor series expansion](https://www.mathsisfun.com/algebra/taylor-series.html).  If we truncated the Taylor series at the first term, then this would be the same as Nadaraya-Watson.\n",
    "\n",
    "This is similar to Kernel regression, except that we fit the local regression to the weighted points by finding a $w(x)$ that minimizes\n",
    "\n",
    "$$\\sum_{i=1}^N  K\\left(\\frac{||x-x_i||}{h}\\right) \\left( y_i - w(x) \\, x_i \\right)^2.$$\n",
    "\n",
    "One version of this called LOWESS (locally weighted scatter plot smoothing) uses the \"tricubic\" Kernel:\n",
    "\n",
    "$$K(x_i,x) = \\left ( 1 - \\left ( \\frac{|x - x_i |}h{}\\right )^3 \\right )^3.$$\n",
    "\n",
    "However, the book isn't really very clear on this (or LOESS, which might stand for LOcal regrESSion) and it doesn't appear that this is implemented in either astroML or Scikit-Learn.  So, we are going to move on--just realize that there algorithms that are intermediate between linear and nonlinear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-linear Regression  <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "Often we can make our non-linear data linear (e.g., if your $y$ values increase exponentially with $x$, by taking the log), but that has its own set of complications (e.g., asymmetric error bars).  So we should also consider non-linear regression, i.e., where the model depends non-linearly on the regression parameters.\n",
    "\n",
    "If we know the theoretical form of the model, then one option is to use MCMC techniques to sample the parameter space and find the optimal model parameters.\n",
    "\n",
    "An alternate approach is to use the **Levenberg-Marquardt (LM) algorithm** to optimize the maximum likelihood estimation. [Numerical Recipes](http://numerical.recipes/) is an excellent resource for more information about LM.  I can't really emphasize enough how ubiquitous LM is, you really should learn how it works in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For now let's leave it as these few words of explanation (with links for further study).\n",
    "LM searches through a combination of [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) and [Gauss-Newton](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm) optimization. If we can express our\n",
    "regression function as a Taylor series expansion then, to first order,\n",
    "then we can write\n",
    "\n",
    "$$f(x_i|\\theta) = f(x_i|\\theta_0) + J d\\theta.$$\n",
    "\n",
    "Here $\\theta_0$ is an initial guess for the regression parameters,\n",
    "$J$ is the Jacobian about this point ($J=\\partial f(x_i|\\theta)/ \\partial\n",
    " \\theta$), and $d\\theta$ is a perturbation in the regression\n",
    "parameters. \n",
    "\n",
    "LM minimizes the sum of square errors,\n",
    "\n",
    "$$\\sum_i [y_i- f(x_i|\\theta_0) - J_i d\\theta]^2,$$\n",
    "\n",
    "for a perturbation $d\\theta$. This minimization results in an update relation for\n",
    "$d\\theta$ given by\n",
    "\n",
    "$$(J^TC^{-1}J + \\lambda\\ {\\rm diag}(J^TC^{-1}J) )\\,d\\theta = J^TC^{-1}(Y-f(X|\\theta)),$$\n",
    "\n",
    "where **$\\lambda$ term acts as a damping parameter**. \n",
    "- If $\\lambda$ is small, then the relation approximates a Gauss-Newton method (i.e., it minimizes the parameters assuming the function is quadratic). \n",
    "- If $\\lambda$ is large the perturbation $d\\theta$ follows the direction of steepest descent. \n",
    "- The diag$(J^TC^{-1}J)$ term is what makes it different from Ridge Regression and it ensures that the update of $d\\theta$ is largest along directions where the gradient is smallest (which improves convergence).\n",
    "\n",
    "This is an iterative process which ceases when the change in likelihood values reaches a predetermined limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Don't worry if that doesn't make any sense.  We are going to talk about gradient descent again when we get to artificial neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In SciPy [`scipy.optimize.leastsq`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.leastsq.html) implements the LM algorithm.\n",
    "Here is an example call to estimate the first 6 terms of the Taylor series for $y=\\sin x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "x = np.linspace(-3,3,100) # 100 values between -3 and 3\n",
    "\n",
    "def taylor_err(a, x, f):\n",
    "    p = np.arange(len(a))[:, None]\n",
    "    return f(x) - np.dot(a,x**p)\n",
    "\n",
    "a_start = np.zeros(6) # starting guess\n",
    "a_best, flat = optimize.leastsq(taylor_err, a_start, args=(x,np.sin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(a_best) #Print coefficients of the Taylor series exapansion.  Do they make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outliers  <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "To be honest, I'm not at all certain why the book brings up outliers at this particular point.  However, we need to talk about outliers sometime.  As with other things today, we'll skip over a lot and do just enough to give you a feel for what can be done.\n",
    "\n",
    "We'll use what we learned from Chapter 5 to adopt a Bayesian approach to identifying outliers and account for them in our fit.\n",
    "\n",
    "Let's assume the data are drawn from two Gaussians distributions (one for the function and the other for the outliers)\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "&  p(\\{y_i\\}|\\{x_i\\}, \\{\\sigma_i\\}, \\theta_0, \\theta_1, \\mu_b, V_b, p_b)\n",
    "  \\propto \\nonumber\\\\\n",
    "&  \\prod_{i=1}^{N} \\bigg[\n",
    "    \\frac{1-p_b}{\\sqrt{2\\pi\\sigma_i^2}}\n",
    "      \\exp\\left(-\\frac{(y_i - \\theta_1 x_i - \\theta_0)^2}\n",
    "               {2 \\sigma_i^2}\\right)\n",
    "    + \\frac{p_b}{\\sqrt{2\\pi(V_b + \\sigma_i^2)}}\n",
    "    \\exp\\left(-\\frac{(y_i - \\mu_b)^2}{2(V_b + \\sigma_i^2)}\\right)\n",
    "    \\bigg].\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "$V_b$ is the variance of the outlier distribution. If we use MCMC we can marginalize over the nuisance  parameters $p_b$, $V_b$, $\\mu_b$. We could also calculate the probability that a point is drawn from the outlier or \"model\" Gaussian.\n",
    "\n",
    "![Ivezic, Figure 8.9](https://www.astroml.org/_images/fig_outlier_rejection_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The top-left panel shows the data, including 3 obvious outlier points.  Not accounting for the outliers gives the dotted line and the model parameters shown in the top right.  Accounting for the outliers with two different methods gives the dashed and solid lines in the top left and the parameter fits given in the bottom 2 plots.\n",
    "\n",
    "What is going on here is beyond the scope of what we have time to get into for this class, but I wanted you to be aware that there are tools/methods to handle such cases.  If you want to try this, take a look at the [code for Figure 8.9](https://www.astroml.org/book_figures/chapter8/fig_outlier_rejection.html) [GTR 2022: Which still isn't working for me.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 8.9\n",
    "# Author: Jake VanderPlas (adapted to PyMC3 by Brigitta Sipocz)\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "\n",
    "import pymc3 as pm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from theano import shared as tshared\n",
    "import theano.tensor as tt\n",
    "\n",
    "from astroML.datasets import fetch_hogg2010test\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Get data: this includes outliers. We need to convert them to Theano variables\n",
    "data = fetch_hogg2010test()\n",
    "xi = tshared(data['x'])\n",
    "yi = tshared(data['y'])\n",
    "dyi = tshared(data['sigma_y'])\n",
    "size = len(data)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Define basic linear model\n",
    "\n",
    "def model(xi, theta, intercept):\n",
    "    slope = np.tan(theta)\n",
    "    return slope * xi + intercept\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# First model: no outlier correction\n",
    "with pm.Model():\n",
    "    # set priors on model gradient and y-intercept\n",
    "    inter = pm.Uniform('inter', -1000, 1000)\n",
    "    theta = pm.Uniform('theta', -np.pi / 2, np.pi / 2)\n",
    "\n",
    "    y = pm.Normal('y', mu=model(xi, theta, inter), sd=dyi, observed=yi)\n",
    "\n",
    "    trace0 = pm.sample(draws=5000, tune=1000)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Second model: nuisance variables correcting for outliers\n",
    "# This is the mixture model given in equation 17 in Hogg et al\n",
    "def mixture_likelihood(yi, xi):\n",
    "    \"\"\"Equation 17 of Hogg 2010\"\"\"\n",
    "\n",
    "    sigmab = tt.exp(log_sigmab)\n",
    "    mu = model(xi, theta, inter)\n",
    "\n",
    "    Vi = dyi ** 2\n",
    "    Vb = sigmab ** 2\n",
    "\n",
    "    root2pi = np.sqrt(2 * np.pi)\n",
    "\n",
    "    L_in = (1. / root2pi / dyi * np.exp(-0.5 * (yi - mu) ** 2 / Vi))\n",
    "\n",
    "    L_out = (1. / root2pi / np.sqrt(Vi + Vb)\n",
    "             * np.exp(-0.5 * (yi - Yb) ** 2 / (Vi + Vb)))\n",
    "\n",
    "    return tt.sum(tt.log((1 - Pb) * L_in + Pb * L_out))\n",
    "\n",
    "\n",
    "with pm.Model():\n",
    "    # uniform prior on Pb, the fraction of bad points\n",
    "    Pb = pm.Uniform('Pb', 0, 1.0, testval=0.1)\n",
    "\n",
    "    # uniform prior on Yb, the centroid of the outlier distribution\n",
    "    Yb = pm.Uniform('Yb', -10000, 10000, testval=0)\n",
    "\n",
    "    # uniform prior on log(sigmab), the spread of the outlier distribution\n",
    "    log_sigmab = pm.Uniform('log_sigmab', -10, 10, testval=5)\n",
    "\n",
    "    inter = pm.Uniform('inter', -200, 400)\n",
    "    theta = pm.Uniform('theta', -np.pi / 2, np.pi / 2, testval=np.pi / 4)\n",
    "\n",
    "    y_mixture = pm.DensityDist('mixturenormal', logp=mixture_likelihood,\n",
    "                               observed={'yi': yi, 'xi': xi})\n",
    "\n",
    "    trace1 = pm.sample(draws=5000, tune=1000)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Third model: marginalizes over the probability that each point is an outlier.\n",
    "# define priors on beta = (slope, intercept)\n",
    "\n",
    "def outlier_likelihood(yi, xi):\n",
    "    \"\"\"likelihood for full outlier posterior\"\"\"\n",
    "\n",
    "    sigmab = tt.exp(log_sigmab)\n",
    "    mu = model(xi, theta, inter)\n",
    "\n",
    "    Vi = dyi ** 2\n",
    "    Vb = sigmab ** 2\n",
    "\n",
    "    logL_in = -0.5 * tt.sum(qi * (np.log(2 * np.pi * Vi)\n",
    "                                  + (yi - mu) ** 2 / Vi))\n",
    "\n",
    "    logL_out = -0.5 * tt.sum((1 - qi) * (np.log(2 * np.pi * (Vi + Vb))\n",
    "                                         + (yi - Yb) ** 2 / (Vi + Vb)))\n",
    "\n",
    "    return logL_out + logL_in\n",
    "\n",
    "\n",
    "with pm.Model():\n",
    "    # uniform prior on Pb, the fraction of bad points\n",
    "    Pb = pm.Uniform('Pb', 0, 1.0, testval=0.1)\n",
    "\n",
    "    # uniform prior on Yb, the centroid of the outlier distribution\n",
    "    Yb = pm.Uniform('Yb', -10000, 10000, testval=0)\n",
    "\n",
    "    # uniform prior on log(sigmab), the spread of the outlier distribution\n",
    "    log_sigmab = pm.Uniform('log_sigmab', -10, 10, testval=5)\n",
    "\n",
    "    inter = pm.Uniform('inter', -1000, 1000)\n",
    "    theta = pm.Uniform('theta', -np.pi / 2, np.pi / 2)\n",
    "\n",
    "    # qi is bernoulli distributed\n",
    "    qi = pm.Bernoulli('qi', p=1 - Pb, shape=size)\n",
    "\n",
    "    y_outlier = pm.DensityDist('outliernormal', logp=outlier_likelihood,\n",
    "                               observed={'yi': yi, 'xi': xi})\n",
    "\n",
    "    trace2 = pm.sample(draws=5000, tune=1000)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# plot the data\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, wspace=0.25,\n",
    "                    bottom=0.1, top=0.95, hspace=0.2)\n",
    "\n",
    "# first axes: plot the data\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.errorbar(data['x'], data['y'], data['sigma_y'], fmt='.k', ecolor='gray', lw=1)\n",
    "ax1.set_xlabel('$x$')\n",
    "ax1.set_ylabel('$y$')\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Go through models; compute and plot likelihoods\n",
    "linestyles = [':', '--', '-']\n",
    "labels = ['no outlier correction\\n(dotted fit)',\n",
    "          'mixture model\\n(dashed fit)',\n",
    "          'outlier rejection\\n(solid fit)']\n",
    "\n",
    "x = np.linspace(0, 350, 10)\n",
    "\n",
    "bins = [(np.linspace(140, 300, 51), np.linspace(0.6, 1.6, 51)),\n",
    "        (np.linspace(-40, 120, 51), np.linspace(1.8, 2.8, 51)),\n",
    "        (np.linspace(-40, 120, 51), np.linspace(1.8, 2.8, 51))]\n",
    "\n",
    "for i, trace in enumerate([trace0, trace1, trace2]):\n",
    "    H2D, bins1, bins2 = np.histogram2d(np.tan(trace['theta']),\n",
    "                                       trace['inter'], bins=50)\n",
    "    w = np.where(H2D == H2D.max())\n",
    "\n",
    "    # choose the maximum posterior slope and intercept\n",
    "    slope_best = bins1[w[0][0]]\n",
    "    intercept_best = bins2[w[1][0]]\n",
    "\n",
    "    # plot the best-fit line\n",
    "    ax1.plot(x, intercept_best + slope_best * x, linestyles[i], c='k')\n",
    "\n",
    "    # For the model which identifies bad points,\n",
    "    # plot circles around points identified as outliers.\n",
    "    if i == 2:\n",
    "        Pi = trace['qi'].mean(0)\n",
    "        outlier_x = data['x'][Pi < 0.32]\n",
    "        outlier_y = data['y'][Pi < 0.32]\n",
    "        ax1.scatter(outlier_x, outlier_y, lw=1, s=400, alpha=0.5,\n",
    "                    facecolors='none', edgecolors='red')\n",
    "\n",
    "    # plot the likelihood contours\n",
    "    ax = plt.subplot(222 + i)\n",
    "\n",
    "    H, xbins, ybins = np.histogram2d(trace['inter'],\n",
    "                                     np.tan(trace['theta']), bins=bins[i])\n",
    "    H[H == 0] = 1E-16\n",
    "    Nsigma = convert_to_stdev(np.log(H))\n",
    "\n",
    "    ax.contour(0.5 * (xbins[1:] + xbins[:-1]),\n",
    "               0.5 * (ybins[1:] + ybins[:-1]),\n",
    "               Nsigma.T, levels=[0.683, 0.955], colors='black')\n",
    "\n",
    "    ax.set_xlabel('intercept')\n",
    "    ax.set_ylabel('slope')\n",
    "    ax.grid(color='gray')\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(40))\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "\n",
    "    ax.text(0.96, 0.96, labels[i], ha='right', va='top',\n",
    "            bbox=dict(fc='w', ec='none', alpha=0.5),\n",
    "            transform=ax.transAxes)\n",
    "    ax.set_xlim(bins[i][0][0], bins[i][0][-1])\n",
    "    ax.set_ylim(bins[i][1][0], bins[i][1][-1])\n",
    "\n",
    "ax1.set_xlim(0, 350)\n",
    "ax1.set_ylim(100, 700)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Proccess Regression <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "A powerful class of regression algorithms is **Gaussian Process Regression**; despite its name, GPR is widely applicable to data that are not generated by a Gaussian process, and can give very general and flexible models that are more data-driven than other techniques. It's become quite a hot topic in astrostatistics. Not only is it **an excellent data-driven interpolation technique, but it also gives you a measure of the interpolation uncertainty.** \n",
    "\n",
    "The big text for GPR is **[Rasmussen and Williams \"Gaussian Processes for Machine Learning\" (2005)](http://www.gaussianprocess.org/gpml/)**. But what are Gaussian processes? \n",
    "\n",
    "> *A **[Gaussian Process](https://en.wikipedia.org/wiki/Gaussian_process) (GP)** is a collection of random variables in a parameter space for which any subset can be defined by a joint Gaussian distribution.*\n",
    "\n",
    "In the **top-left panel below**, we have drawn some random distributions from a Gaussian Basis. Specifically, we have put down evenly spaced Gaussians across the parameter space, which have width of $h$ and covariance given by \n",
    "\n",
    "$${\\rm Cov}(x_1, x_2; h) = \\exp\\left(\\frac{-(x_1 - x_2)^2}{2 h^2}\\right).$$\n",
    "\n",
    "For a given bandwidth we can define an infinite set of such functions.\n",
    "\n",
    "Then in the **top-right panel**, we constrain these functions by selecting those that pass though a given set of points using the posterior:\n",
    "\n",
    "$$p(f_j | \\{x_i, y_i, \\sigma_i\\}, x_j^\\ast).$$\n",
    "\n",
    "The **bottom panels** show the result for the same points with error bars (*bottom left*) and 20 noisy points drawn from $y=\\cos(x)$ (*bottom right*). You can perhaps see how this might be useful-- flexible, data-driven interpolation with a measure of interpolation uncertainty!\n",
    "\n",
    "![Ivezic, Figure 8.10](http://www.astroml.org/_images/fig_gp_example_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is the code that produced that plot (Ivezic, Figure 8.10).  See what happens if you make the number of Gaussians much smaller or much bigger, or if you change the bandwidth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 8.10\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor, kernels\n",
    "from scipy.optimize import fmin_cobyla\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "#if \"setup_text_plots\" not in globals():\n",
    "#    from astroML.plotting import setup_text_plots\n",
    "#setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# define a squared exponential covariance function\n",
    "def squared_exponential(x1, x2, h):\n",
    "    return np.exp(-0.5 * (x1 - x2) ** 2 / h ** 2)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# draw samples from the unconstrained covariance\n",
    "np.random.seed(1)\n",
    "x = np.linspace(0, 10, 100) #This sets the number of Gaussians\n",
    "h = 1.0  #This is the Bandwidth\n",
    "\n",
    "mu = np.zeros(len(x))\n",
    "C = squared_exponential(x, x[:, None], h)\n",
    "draws = np.random.multivariate_normal(mu, C, 3)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Constrain the mean and covariance with two points\n",
    "x1 = np.array([2.5, 7])\n",
    "y1 = np.cos(x1)\n",
    "kernel1 = kernels.RBF(1/0.5, (1/0.5, 1/0.5))\n",
    "gp1 = GaussianProcessRegressor(kernel=kernel1, random_state=0, normalize_y=True)\n",
    "gp1.fit(x1[:, None], y1)\n",
    "f1, f1_err = gp1.predict(x[:, None], return_std=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Constrain the mean and covariance with two noisy points\n",
    "#  scikit-learn gaussian process uses nomenclature from the geophysics\n",
    "#  community, where a \"nugget (alpha parameter)\" can be specified.\n",
    "#  The diagonal of the assumed covariance matrix is multiplied by the nugget.\n",
    "#  This is how the error on inputs is incorporated into the calculation.\n",
    "dy2 = 0.2\n",
    "kernel2 = kernels.RBF(1/0.5, (1/0.5, 1/0.5))\n",
    "gp2 = GaussianProcessRegressor(kernel=kernel2,\n",
    "                               alpha=(dy2 / y1) ** 2, random_state=0)\n",
    "gp2.fit(x1[:, None], y1)\n",
    "f2, f2_err = gp2.predict(x[:, None], return_std=True)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Constrain the mean and covariance with many noisy points\n",
    "x3 = np.linspace(0, 10, 20)\n",
    "y3 = np.cos(x3)\n",
    "dy3 = 0.2\n",
    "y3 = np.random.normal(y3, dy3)\n",
    "\n",
    "kernel3 = kernels.RBF(0.5, (0.01, 10.0))\n",
    "gp3 = GaussianProcessRegressor(kernel=kernel3,\n",
    "                               alpha=(dy3 / y3) ** 2, random_state=0)\n",
    "gp3.fit(x3[:, None], y3)\n",
    "f3, f3_err = gp3.predict(x[:, None], return_std=True)\n",
    "\n",
    "# we have fit for the `h` parameter: print the result here:\n",
    "print(\"best-fit theta =\", gp3.kernel_.theta[0])\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the diagrams\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "\n",
    "# first: plot a selection of unconstrained functions\n",
    "ax = fig.add_subplot(221)\n",
    "ax.plot(x, draws.T, '-k')\n",
    "ax.set_ylabel('$f(x)$')\n",
    "\n",
    "# second: plot a constrained function\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(x, f1, '-', color='gray')\n",
    "ax.fill_between(x, f1 - 2 * f1_err, f1 + 2 * f1_err, color='gray', alpha=0.3)\n",
    "ax.plot(x1, y1, '.k', ms=6)\n",
    "\n",
    "\n",
    "# third: plot a constrained function with errors\n",
    "ax = fig.add_subplot(223)\n",
    "ax.plot(x, f2, '-', color='gray')\n",
    "ax.fill_between(x, f2 - 2 * f2_err, f2 + 2 * f2_err, color='gray', alpha=0.3)\n",
    "ax.errorbar(x1, y1, dy2, fmt='.k', ms=6)\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$f(x)$')\n",
    "\n",
    "# third: plot a more constrained function with errors\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(x, f3, '-', color='gray')\n",
    "ax.fill_between(x, f3 - 2 * f3_err, f3 + 2 * f3_err, color='gray', alpha=0.3)\n",
    "ax.errorbar(x3, y3, dy3, fmt='.k', ms=6)\n",
    "\n",
    "ax.plot(x, np.cos(x), ':k')\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlim(0, 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For GP regression we want to estimate the value and variance of a new set of points given an input data set. This is equivalent to averaging over all functions that pass through our input data\n",
    "\n",
    "We compute the covariance matrix of all observed data and the new set of points that we want to predict:\n",
    "\n",
    "> $  K = \\begin{pmatrix}\n",
    "    K_{11} & K_{12} \\\\\n",
    "    K_{12}^T & K_{22}\n",
    "  \\end{pmatrix},\n",
    "$\n",
    "\n",
    "where $K_{11}$ is the covariance between the input points $x_i$ with\n",
    "observational errors $\\sigma_i^2$ added in quadrature to the diagonal,\n",
    "$K_{12}$ is\n",
    "the cross-covariance between the input points $x_i$ and the unknown points\n",
    "$x^\\ast_j$, and $K_{22}$ is the covariance between the unknown points\n",
    "$x_j^\\ast$.  \n",
    "\n",
    "where \n",
    "- $K_{11}$ is the covariance between the input points $x_i$ with observational errors $\\sigma_i^2$ added in quadrature to the diagonal, \n",
    "- $K_{12}$ is the cross-covariance between the input points $x_i$ and the unknown points $x^\\ast_j$,\n",
    "- $K_{22}$ is the covariance between the unknown points $x_j^\\ast$. \n",
    "\n",
    "For observed vectors $\\vec{x}$ and $\\vec{y}$, and a vector of unknown points $\\vec{x}^\\ast$, it can be shown that the posterior is given by (see the Rasmussen & Williams text above)\n",
    "\n",
    ">$  p(f_j | \\{x_i, y_i, \\sigma_i\\}, x_j^\\ast) = \\mathcal{N}(\\vec{\\mu}, \\Sigma)$\n",
    "\n",
    "where\n",
    "\n",
    ">$\n",
    "\\begin{eqnarray}\n",
    "  \\vec{\\mu} &=& K_{12} K_{11}^{-1} \\vec{y}, \\\\\n",
    "  \\Sigma &=& K_{22} - K_{12}^TK_{11}^{-1}K_{12}\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "$\\mu_j$ gives the expected value $\\bar{f}^\\ast_j$ of the result, and\n",
    "$\\Sigma_{jk}$ gives the error covariance between any two unknown points (which measures the prediction uncertainty).\n",
    "\n",
    "**This is key: it gives the mean AND uncertainty of a predicted point.**\n",
    "\n",
    "Note that the physics of the underlying process enters through the assumed\n",
    "form of the covariance function, which has a variety of choices-- again see the [Kernel Cookbook](https://www.cs.toronto.edu/~duvenaud/cookbook/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Scikit-Learn [`GaussianProcess`](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor) implementation looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "X = np.random.random((100,2))\n",
    "y = np.sin(10*X[:,0] + X[:,1])\n",
    "gp = GaussianProcessRegressor()\n",
    "gp.fit(X,y)\n",
    "y_pred, dy_pred = gp.predict(X, return_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below we see what GP does for the supernova example that we used last time.   What is great is that not only do you get a fit, you get errors and can tell where the fit is good and where it is poor.  As expected, the prediction gets successively poor as we move beyond the range of the training data.\n",
    "\n",
    "  \n",
    "Gaussian Processes are also pretty useful for time-domain data too, so maybe we'll come back to it after next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 8.11\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from astropy.cosmology import LambdaCDM\n",
    "\n",
    "from astroML.datasets import generate_mu_z\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=14, usetex=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Generate data\n",
    "cosmo = LambdaCDM(H0=71, Om0=0.27, Ode0=0.73, Tcmb0=0)\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0, cosmo=cosmo)\n",
    "\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu_true = cosmo.distmod(z)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# fit the data\n",
    "# Mesh the input space for evaluations of the real function,\n",
    "# the prediction and its MSE\n",
    "z_fit = np.linspace(0, 2, 1000)\n",
    "\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=dmu ** 2)\n",
    "\n",
    "gp.fit(z_sample[:, None], mu_sample)\n",
    "y_pred, sigma = gp.predict(z_fit[:, None], return_std=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot the gaussian process\n",
    "#  gaussian process allows computation of the error at each point\n",
    "#  so we will show this as a shaded region\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, bottom=0.1, top=0.95)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_true, '--k')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', markersize=6)\n",
    "ax.plot(z_fit, y_pred, '-k')\n",
    "ax.fill_between(z_fit, y_pred - 1.96 * sigma, y_pred + 1.96 * sigma,\n",
    "                alpha=0.2, color='b', label='95% confidence interval')\n",
    "\n",
    "ax.set_xlabel('$z$')\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "\n",
    "ax.set_xlim(0, 2)\n",
    "ax.set_ylim(36, 48)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "N.B.  In this process we are assuming that the $x$ values are error free.  But really they will have error too.  Ivezic $\\S$ 8.8 deals with this, but we are skipping it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uncertainties All Round <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "What about real-world applications, where the **independent variable, $x$, (usually) is not free from any uncertainty**? Imagine the \"true\" relation,\n",
    "\n",
    "$$ y^*_i=\\theta_0 + \\theta_1x^*_{i}. $$\n",
    "\n",
    "Now we make measurements in $x$, and $y$ which are noisy (Gaussian noise)\n",
    "\n",
    "$$\n",
    "x_i = x^*_i + \\delta_i,\\\\\n",
    "y_i = y^* + \\epsilon_i,\n",
    "$$\n",
    "\n",
    "Solving for $y$ we get\n",
    "$$\n",
    "      \\hat{y}_i=  \\theta_0 + \\theta_1 (x_i - \\delta_i) +\\epsilon_i.\n",
    "$$\n",
    "\n",
    "Thus $\\hat{y}$ depends on the noise in $x$, such that the uncertainty in $x$ is now part of the regression equation and scales with the regression coefficients. \n",
    "\n",
    "This problem is known as **Total Least Squares (TLS)**. In TLS, we build a likelihood that measures the **perpendicular distance of points from a modeled line**, rather than what we have done thus far in ordinary least squares (OLS) which measures only the **vertical distance of $y$ data values from the modeled curve**. Image credit: [here](https://towardsdatascience.com/total-least-squares-in-comparison-with-ols-and-odr-f050ffc1a86a).\n",
    "\n",
    "![](https://miro.medium.com/max/854/1*illoIj5LRD3NrQ69iV30kw.png)\n",
    "\n",
    "How can we account for the measurement uncertainties in both the independent and dependent variables? Assuming they are Gaussian, the total data covariance matrix is\n",
    "\n",
    "$$ \\Sigma_i = \\left[\n",
    "\\begin{array}{cc}\n",
    "\\sigma_{x_i}^2 & \\sigma_{xy_i} \\\\\n",
    "\\sigma_{xy_i} & \\sigma_{y_i}^2\n",
    "\\end{array}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "We'll consider straight-line regression. **It will help us to think of our line geometrically, where we define it in terms of its normal vector**:\n",
    "\n",
    "$$ {\\bf n} = \\left [\n",
    "\\begin{array}{c}\n",
    "-\\sin \\alpha\\\\\n",
    "\\cos \\alpha\\\\\n",
    "\\end{array}\n",
    "\\right ] \n",
    "$$\n",
    "\n",
    "with $\\theta_1 = \\arctan(\\alpha)$ and $\\alpha$ is the angle between the line and the $x$-axis. \n",
    "\n",
    "The covariance matrix projects onto this space as\n",
    "\n",
    "$$ S_i^2 = {\\bf n}^T \\Sigma_i {\\bf n} $$\n",
    "\n",
    "and the  distance between a point and the line is\n",
    "\n",
    "$$\\Delta_i = {\\bf n}^T z_i - \\theta_0\\ \\cos \\alpha, $$\n",
    "  \n",
    "where $z_i$ represents the data point $(x_i,y_i)$. You may need to think and sketch out the latter equation. The first part makes sense since it is just projecting the coordinate vector onto the line's normal vector. The second part subtracts off the excess of this projection in case the line is vertically offset from zero-intercept. \n",
    "\n",
    "The log-likelihood is then\n",
    "\n",
    "$$ \\ln\\mathcal{L} \\propto - \\sum_i \\frac{\\Delta_i^2}{2 S_i^2}$$\n",
    "\n",
    "and we can maximize the likelihood as a brute-force search or through MCMC. \n",
    "\n",
    "In the following code cell, data has uncertainties in $x$ and $y$, and a straight line is numerically fit through **Total Least Squares**. The resulting measured parameter uncertainty regions for the intercept and slope are shown in the right panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "from scipy import optimize\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "from astroML.linear_model import TLS_logL\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "from astroML.datasets import fetch_hogg2010test\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define some convenience functions\n",
    "\n",
    "# translate between typical slope-intercept representation,\n",
    "# and the normal vector representation\n",
    "def get_m_b(beta):\n",
    "    b = np.dot(beta, beta) / beta[1]\n",
    "    m = -beta[0] / beta[1]\n",
    "    return m, b\n",
    "\n",
    "\n",
    "def get_beta(m, b):\n",
    "    denom = (1 + m * m)\n",
    "    return np.array([-b * m / denom, b / denom])\n",
    "\n",
    "\n",
    "# compute the ellipse pricipal axes and rotation from covariance\n",
    "def get_principal(sigma_x, sigma_y, rho_xy):\n",
    "    sigma_xy2 = rho_xy * sigma_x * sigma_y\n",
    "\n",
    "    alpha = 0.5 * np.arctan2(2 * sigma_xy2,\n",
    "                             (sigma_x ** 2 - sigma_y ** 2))\n",
    "    tmp1 = 0.5 * (sigma_x ** 2 + sigma_y ** 2)\n",
    "    tmp2 = np.sqrt(0.25 * (sigma_x ** 2 - sigma_y ** 2) ** 2 + sigma_xy2 ** 2)\n",
    "\n",
    "    return np.sqrt(tmp1 + tmp2), np.sqrt(tmp1 - tmp2), alpha\n",
    "\n",
    "\n",
    "# plot ellipses\n",
    "def plot_ellipses(x, y, sigma_x, sigma_y, \n",
    "                  rho_xy, factor=2, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    sigma1, sigma2, alpha = get_principal(sigma_x, \n",
    "                                          sigma_y, \n",
    "                                          rho_xy)\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        ax.add_patch(Ellipse((x[i], y[i]),\n",
    "                             factor * sigma1[i], factor * sigma2[i],\n",
    "                             alpha[i] * 180. / np.pi,\n",
    "                             fc='none', ec='k'))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# We'll use the data from table 1 of Hogg et al. 2010\n",
    "data = fetch_hogg2010test()\n",
    "data = data[5:]  # no outliers\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "sigma_x = data['sigma_x']\n",
    "sigma_y = data['sigma_y']\n",
    "rho_xy = data['rho_xy']\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Find best-fit parameters\n",
    "X = np.vstack((x, y)).T\n",
    "dX = np.zeros((len(x), 2, 2))\n",
    "dX[:, 0, 0] = sigma_x ** 2\n",
    "dX[:, 1, 1] = sigma_y ** 2\n",
    "dX[:, 0, 1] = dX[:, 1, 0] = rho_xy * sigma_x * sigma_y\n",
    "\n",
    "min_func = lambda beta: -TLS_logL(beta, X, dX)\n",
    "beta_fit = optimize.fmin(min_func,\n",
    "                         x0=[-1, 1])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the data and fits\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, wspace=0.25,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# first let's visualize the data\n",
    "ax = fig.add_subplot(121)\n",
    "ax.scatter(x, y, c='k', s=9)\n",
    "plot_ellipses(x, y, sigma_x, sigma_y, rho_xy, ax=ax)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the best-fit line\n",
    "m_fit, b_fit = get_m_b(beta_fit)\n",
    "x_fit = np.linspace(0, 300, 10)\n",
    "ax.plot(x_fit, m_fit * x_fit + b_fit, '-k')\n",
    "\n",
    "ax.set_xlim(40, 250)\n",
    "ax.set_ylim(100, 600)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the likelihood contour in m, b\n",
    "ax = fig.add_subplot(122)\n",
    "m = np.linspace(1.7, 2.8, 100)\n",
    "b = np.linspace(-60, 110, 100)\n",
    "logL = np.zeros((len(m), len(b)))\n",
    "\n",
    "for i in range(len(m)):\n",
    "    for j in range(len(b)):\n",
    "        logL[i, j] = TLS_logL(get_beta(m[i], b[j]), X, dX)\n",
    "\n",
    "ax.contour(m, b, convert_to_stdev(logL.T),\n",
    "           levels=(0.683, 0.955, 0.997),\n",
    "           colors='k')\n",
    "ax.set_xlabel('slope')\n",
    "ax.set_ylabel('intercept')\n",
    "ax.set_xlim(1.7, 2.8)\n",
    "ax.set_ylim(-60, 110)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
