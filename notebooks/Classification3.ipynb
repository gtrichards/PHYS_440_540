{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other Topics in Classification\n",
    "\n",
    "G. Richards\n",
    "(2016, 2018, 2020, 2022)\n",
    "based on materials from Connolly, VanderPlas, Geron, and Ivezic.  With updates to my own class from [Stephen Taylor's class at Vanderbilt](https://github.com/VanderbiltAstronomy/astr_8070_s21).\n",
    "\n",
    "\n",
    "#### Reading:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 9.\n",
    "- Many blogs and videos (e.g., https://www.youtube.com/watch?v=bxe2T-V8XRs)\n",
    "- Free online book! http://neuralnetworksanddeeplearning.com/index.html\n",
    "\n",
    "A lot of the next 3 lectures will come from the book by [Geron](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_5?dchild=1&keywords=machine+learning&qid=1596499152&sr=8-5).  You can access it with your Drexel login through [O'Reilly](https://learning-oreilly-com.ezproxy2.library.drexel.edu/library/view/hands-on-machine-learning/9781492032632/titlepage01.html).  We aren't on the list for some reason, but just select that option and give your Drexel e-mail address.  However, if you think that you might spend any time working with neural networks beyond this course, it would be a worthwhile purchase.\n",
    "\n",
    "\n",
    "In these lectures we'll talk about more advanced topics in classification, before moving on to neural networks and deep learning.   Let's start by introducing a few useful concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents\n",
    "* [Loss functions](#one)\n",
    "* [Gradient descent](#two)\n",
    "* [AdaBoost](#three)\n",
    "* [Neural Networks](#four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss Functions  <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "A **[loss function](https://en.wikipedia.org/wiki/Loss_functions_for_classification)** is like a cost function, or a likelihood, or optimization function, or an objective function. We're trying to minimize the offset between a model and some data. The difference with a loss function is that we evaluate it on a single training example rather than the full data set.\n",
    "\n",
    "- Whether you realize it or not, you are typically working with **$L2$ loss functions**:\n",
    "$$(y-f(x))^2,$$\n",
    "where the corresponding ***cost function*** is the mean of those for all $x_i$ or the **Mean Squared Error (MSE)** (just another name for $\\chi^2$, which of course is related to the probability of Gaussian uncertainties).\n",
    "\n",
    "- We also talked about **$L1$ loss functions** way back when we looked at the ***Huber loss*** which was more robust to outliers:\n",
    "$$|y-f(x)|,$$\n",
    "and the L1 penalty was important in LASSO regression.\n",
    "\n",
    "For classification we plot the **loss vs. $(y\\times f(x))$**, which corresponds to the known class (either $+1$ or $-1$) times the predicted value. If that product is positive, we predict $+1$. If it is negative, we predict $-1$. We define the loss to be zero for $y*f(x)=1$, that is, when we have gotten the right answer.\n",
    "\n",
    "So what do the mean squared error (MSE or $l2$) and mean absolute error (MAE or $l1$) look like for classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Mathematical formulas for various loss functions\n",
    "def log_loss(raw_model_output):\n",
    "   return np.log(1+np.exp(-raw_model_output))\n",
    "\n",
    "def hinge_loss(raw_model_output):\n",
    "   return np.maximum(0,1-raw_model_output)\n",
    " \n",
    "def l2(raw_model_output):\n",
    "   return (raw_model_output-1)**2  \n",
    "\n",
    "def l1(raw_model_output):\n",
    "   return np.abs(raw_model_output-1)  \n",
    " \n",
    "def zero_one(raw_model_output):\n",
    "   return np.where(raw_model_output < 0, 1, 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a grid of values and plot\n",
    "grid = np.linspace(-3,3,1000)\n",
    "plt.plot(grid, l2(grid), \"g\", label='l2')\n",
    "plt.plot(grid, l1(grid), \"brown\", label='l1')\n",
    "\n",
    "plt.fill_between([0,3],-0.02,5,color=\"b\",alpha=0.4)\n",
    "plt.fill_between([-3,0],-0.02,5,color=\"r\",alpha=0.5)\n",
    "plt.xlim([-3,3])\n",
    "plt.ylim([-0.02,5])\n",
    "plt.xlabel(\"y*f(x)\",fontsize=12)\n",
    "plt.ylabel(\"loss\",fontsize=12)\n",
    "plt.title(\"predict -1 (incorrect)         predict +1 (correct)\",fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This does something reasonable for $y*f(x)\\le1$.  However, look what happens at larger values (where we are even more confident that $y*f(x)$ is positive and that our class should be $+1$.  The loss goes **up**.  That's bad.\n",
    "\n",
    "Now, you may be wondering how $y*f(x)$ can be larger than 1. Here's how this works.\n",
    "\n",
    "$f(x)$ isn't just a value between $-1$ and $1$-- it is a function. For example, let's say that our training data looks like this (where `xx` are features and `yy` are class labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "xx = np.array([0.1,0.2,0.4,0.6,0.8,1.1,1.4,1.5])\n",
    "yy = np.array([-1,-1,-1,-1,1,1,1,1])\n",
    "\n",
    "#with plt.xkcd():\n",
    "if 1:\n",
    "    \n",
    "    fig = plt.figure(1)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    # Move left y-axis and bottim x-axis to centre, passing through (0,0)\n",
    "    ax.spines['left'].set_position(('axes',0.045))\n",
    "    ax.spines['bottom'].set_position('center')\n",
    "\n",
    "    # Eliminate upper and right axes\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "\n",
    "    # Show ticks in the left and lower axes only\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    \n",
    "    ax.plot(1, 0, \">k\", transform=ax.get_yaxis_transform(), clip_on=False)\n",
    "    ax.plot(0, 1, \"^k\", transform=ax.get_xaxis_transform(), clip_on=False)\n",
    "        \n",
    "    ax.scatter(xx,yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's fit a linear model to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(xx[:,None],yy)\n",
    "ypred = linreg.predict(grid[:,None])\n",
    "\n",
    "if 1:\n",
    "    \n",
    "    fig = plt.figure(1)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    # Move left y-axis and bottim x-axis to centre, passing through (0,0)\n",
    "    ax.spines['left'].set_position(('axes',0.25))\n",
    "    ax.spines['bottom'].set_position('center')\n",
    "\n",
    "    # Eliminate upper and right axes\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "\n",
    "    # Show ticks in the left and lower axes only\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    \n",
    "    ax.plot(1, 0, \">k\", transform=ax.get_yaxis_transform(), clip_on=False)\n",
    "    ax.plot(0, 1, \"^k\", transform=ax.get_xaxis_transform(), clip_on=False)\n",
    "    \n",
    "    ax.plot(grid,ypred)\n",
    "    ax.set_xlim([-1,3])\n",
    "    ax.set_ylim([-2,2])\n",
    "        \n",
    "    ax.scatter(xx,yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We see that for $x$ greater than about 1.3, $f(x)$ can indeed be larger than 1 and so can $y*f(x)$, which is indicating an increased certainty of the $+1$ class. \n",
    "\n",
    "OK, so now we can understand the plot, but we still need a loss function that makes sense for classification.\n",
    "\n",
    "- The first we'll try is the so-called **[\"Zero-One Loss\"](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html)** shown in **black** below. It is 1 for $yf(x)<0$ and 0 for $yf(x)>0$; thus the name. You increment the loss function by 1 every time you make a wrong prediction. It is just a count of the total number of mistakes.\n",
    "\n",
    "However, the Zero-One loss is hard to minimize, so instead we can try something that allows the loss to be continuous function in $y*f(x)$.  \n",
    "\n",
    "- The **[Hinge Loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html)**, which looks like\n",
    "$${\\rm max}(0,1-y*f(x)),$$\n",
    "is plotted in **orange**. Here there is no contribution to the loss for values $\\ge 1$, but there is a linearly increasing loss for smaller values. So, it penalizes both wrong predictions and also correct predictions that have low confidence.\n",
    "\n",
    "- A **[Logistic Loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss)** (also called the *log loss* and *cross entropy loss*) function has similar properties as shown in **blue**, but is smoother and has slightly less and less penalty for more and more confident $+1$ predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a grid of values and plot\n",
    "grid = np.linspace(-3,3,1000)\n",
    "#plt.plot(grid, log_loss(grid), label='logistic')\n",
    "plt.plot(grid, zero_one(grid), \"k\", label='0-1')\n",
    "plt.plot(grid, hinge_loss(grid), \"orange\", label='hinge')\n",
    "plt.plot(grid, log_loss(grid), \"b\", label='logistic')\n",
    "#plt.plot(grid, l2(grid), label='l2')\n",
    "#plt.plot(grid, l1(grid), label='l1')\n",
    "\n",
    "plt.fill_between([0,3],-0.02,5,color=\"b\",alpha=0.4)\n",
    "plt.fill_between([-3,0],-0.02,5,color=\"r\",alpha=0.5)\n",
    "plt.xlim([-3,3])\n",
    "plt.ylim([-0.02,5])\n",
    "plt.xlabel(\"y*f(x)\",fontsize=12)\n",
    "plt.ylabel(\"loss\",fontsize=12)\n",
    "plt.title(\"predict -1 (incorrect)         predict +1 (correct)\",fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For more see [Linear Classsifiers in Python course](https://learn.datacamp.com/courses/linear-classifiers-in-python).  Also\n",
    "\n",
    "https://datascience103579984.wordpress.com/2019/09/18/linear-classifiers-in-python-from-datacamp/\n",
    "\n",
    "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2019/08/detailed-guide-7-loss-functions-machine-learning-python-code\n",
    "\n",
    "http://www.datasciencecourse.org/notes/linear_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent   <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "That brings us to the topic of [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) for finding the optimal extremal position of a cost function.\n",
    "\n",
    "Throughout this couse we have been trying to determine model parameters, $\\theta$, that minimize either the regression error or the classification error when fitting our training data (and not overfitting!). \n",
    "\n",
    "- Sometimes we have been able to write an analytic solution for $\\theta$.\n",
    "- In MCMC we semi-randomly sampled the multi-dimensional $\\theta$ space to find the best answer (and map the full parameter posterior distribution along the way). \n",
    "- But what happens if you are stuck on top of a freezing cold mountain and you have no map and can't magically jump from place to place? You start walking **down**. That's the basic idea of ***gradient descent***--take a look around you, figure out which way is sloping downward the most and go *that* way.\n",
    "\n",
    "We are going to determine the local gradient of the loss function with respect to $\\theta$ and go in the steepest direction, until the gradient is zero (and we have arrived at our destination)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mathematically, for a simple linear model the MSE is defined as\n",
    "\n",
    "$$ {\\rm MSE}({\\mathbf \\theta}) = \\frac{1}{N}(Y - X\\theta)^T (Y - X\\theta).$$\n",
    "\n",
    "Therefore the partial derivative with respect to the vector of parameters $\\theta$ is\n",
    "\n",
    "$$\\nabla_{\\theta}{\\rm MSE}({\\mathbf \\theta}) = \\frac{2}{N}X^T(X\\theta - Y).$$\n",
    "\n",
    "That gives the uphill direction, so we compute the next step as\n",
    "\n",
    "$$\\theta^{\\rm next step} = \\theta - \\eta\\nabla_{\\theta}{\\rm MSE}({\\mathbf \\theta}),$$\n",
    "\n",
    "where $\\eta$ is the \"learning rate\" and the rest are all matrices or vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The initial values for $\\theta$ are chosen randomly.\n",
    "- The **[learning rate](https://en.wikipedia.org/wiki/Learning_rate)** controls how big your steps \"down\" are. \n",
    "- If your step size is too small, it will take too long to converge. \n",
    "- If it is too big, you might miss the bottom completely (and possibly end up diverging from the solution).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![https://miro.medium.com/max/1400/0*GaO7X6j3coh3oNwf.png](https://miro.medium.com/max/1400/0*GaO7X6j3coh3oNwf.png)\n",
    "\n",
    "We also have to be careful that we don't end up in a local minimum instead of the global minimum.  One of the nice things about the $L2$ cost function is that it is guaranteed to have just a single global minimum.  Gradient descent is also useful for regression where there are too many training points (or too many features) to fit into memory at a given time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's an example from Geron where we apply gradient descent to a simple linear regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Run the next 4 cells\n",
    "#N points randomly drawn from a linear distribution\n",
    "N=100\n",
    "X = 2 * np.random.rand(N, 1)\n",
    "y = 4 + 3 * X + np.random.randn(N, 1)\n",
    "\n",
    "#Turn X into a matrix\n",
    "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each \n",
    "\n",
    "#Grid for plotting\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "eta = 0.1  # learning rate\n",
    "n_iterations = 100\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/N * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_lines=10\n",
    "color_idx = np.linspace(0, 1, n_lines)\n",
    "#Helper function\n",
    "theta_path_bgd = []\n",
    "def plot_gradient_descent(theta, eta, theta_path=None):\n",
    "    m = len(X_b)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    n_iterations = 1000\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration < 10:\n",
    "            y_predict = X_new_b.dot(theta)\n",
    "            #style = \"b-\" if iteration > 0 else \"r--\"\n",
    "            #plt.plot(X_new, y_predict, style)\n",
    "            plt.plot(X_new, y_predict, color=plt.cm.cool(color_idx[iteration]))\n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta = theta - eta * gradients\n",
    "        if theta_path is not None:\n",
    "            theta_path.append(theta)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.axis([0, 2, 0, 15])\n",
    "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(131); plot_gradient_descent(theta, eta=0.02)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\n",
    "plt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We've got a \"three bears\" scenario.\n",
    "\n",
    "- On the left our learning rate is too low. We'll eventually get to the solution, but it will take a long time.  \n",
    "- On the right it is too high and we have completely missed the solution.  \n",
    "- In the middle the learning rate is just right.\n",
    "\n",
    "Try $\\eta = 0.3$ and $0.4$ to see if you can understand what is going on in the right panel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost  <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "When I introduced Boosting in *Classification II*, I didn't mention **[AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)** or **[Gradient Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=gradient%20boosting#sklearn.ensemble.GradientBoostingClassifier)**, because it makes more sense to know about the other items above first. But ***AdaBoost = \"Adaptive Boosting\"***.\n",
    "\n",
    "Below is an example of AdaBoost from Geron.  See what happens when you change the learning rate, where half the learning rate means that weights are boosted half as much for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Run the next 3 cells\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Helper function for plotting\n",
    "from matplotlib.colors import ListedColormap\n",
    "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate=1.0\n",
    "\n",
    "m = len(X_train)\n",
    "from sklearn.svm import SVC\n",
    "fix, axes = plt.subplots(ncols=1, figsize=(10,6))\n",
    "sample_weights = np.ones(m)\n",
    "for i in range(5):\n",
    "    svm_clf = SVC(kernel=\"rbf\", C=0.05, gamma=\"scale\", random_state=42)\n",
    "    svm_clf.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    y_pred = svm_clf.predict(X_train)\n",
    "    sample_weights[y_pred != y_train] *= (1 + learning_rate)\n",
    "    plot_decision_boundary(svm_clf, X, y, alpha=0.2)\n",
    "    plt.title(\"learning_rate = {}\".format(learning_rate), fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Try some learning rates between $0.1$ and $1.0$.  It is important to note the order of these decision boundaries; that is, are they diverging?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Before we start on Neural Networks, let's talk more about [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression), which we saw last time is actually used for classification.\n",
    "\n",
    "In logistic regression, we take our output and pass it through the logistic function to determine the output, where the logistic function is \n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1+e^{-t}},$$\n",
    "which is similar to the log loss function we saw above.\n",
    "\n",
    "Positive output values have $+1$ class probability greater than 50% and are classified as $+1$, while negative output values have $+1$ class probability less than 50% and are classified as $-1$.  The logistic function provides for a smooth transition in probability between the classes as a function of our output as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Geron\n",
    "t = np.linspace(-10, 10, 100)\n",
    "sig = 1 / (1 + np.exp(-t))\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.plot([-10, 10], [0, 0], \"k-\")\n",
    "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
    "plt.plot([-10, 10], [1, 1], \"k:\")\n",
    "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
    "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.legend(loc=\"upper left\", fontsize=20)\n",
    "plt.axis([-10, 10, -0.1, 1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll try it on the iris dataset, where we will do a [one-vs-rest](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) type analysis with just 2 classes: virginica and *not* virginica and just 1 feature (petal width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris[\"data\"][:, 3:]  # petal width\n",
    "y = (iris[\"target\"] == 2).astype(int)  # 1 if Iris virginica, else 0\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_grid = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_grid)\n",
    "\n",
    "decision_boundary = X_grid[y_proba[:, 1] >= 0.5][0]\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(X[y==0], y[y==0], \"bs\")\n",
    "plt.plot(X[y==1], y[y==1], \"g^\")\n",
    "plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\n",
    "plt.plot(X_grid, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\n",
    "plt.plot(X_grid, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")\n",
    "plt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
    "plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
    "plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\n",
    "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
    "plt.ylabel(\"Probability\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([0, 3, -0.02, 1.02])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we see that flowers having petal width larger than 1.6cm get classified as virginica, but that there are 5 examples that are misclassified.\n",
    "\n",
    "We can also look at the decision boundary in 2-D to see if adding another feature helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(int)\n",
    "\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", C=10**10, random_state=42)\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
    "        np.linspace(0.8, 2.7, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_grid = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "y_proba = log_reg.predict_proba(X_grid)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"g^\")\n",
    "\n",
    "zz = y_proba[:, 1].reshape(x0.shape)\n",
    "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
    "\n",
    "\n",
    "left_right = np.array([2.9, 7])\n",
    "boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
    "\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
    "plt.text(3.5, 1.5, \"Not Iris virginica\", fontsize=14, color=\"b\", ha=\"center\")\n",
    "plt.text(6.5, 2.3, \"Iris virginica\", fontsize=14, color=\"g\", ha=\"center\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.axis([2.9, 7, 0.8, 2.7])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Adding the second feature didn't really help, but we can see that the misclassifications are all in the region where there is intermediate probability.  Really that's half the battle.  It is OK to be wrong as long as you have some sense that that might be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The loss function for training our logistic regression algorithm is a \"log loss\", which looks like\n",
    "\n",
    "$$-\\log(p), {\\rm if} y=1$$\n",
    "$$-\\log(1-p), {\\rm if} y=-1$$ \n",
    "\n",
    "So, if the model estimates a $+1$ class with probability near 0, the misclassification cost will be very high.  High probability of class $+1$ has nearly 0 cost.  For a training object in class $-1$, if the model estimates a $+1$ class with low probability, then the cost is low, while a high $+1$ class probability gives a high cost.\n",
    "\n",
    "In one line, we can write it as\n",
    "\n",
    "$$-\\frac{1}{N}\\Sigma\\left[y_i\\log(p_i) + (1-y_i)\\log(1-p_i)\\right].$$\n",
    "\n",
    "One of the reasons that we introduced gradient descent above is that this function has no analytic solution.  However, it does have a single minimum so gradient descent is guaranteed to work (with an appropriate learning rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we wanted to classify all three iris at once and not just do binary classification, then we would instead use a [softmax function](https://en.wikipedia.org/wiki/Softmax_function) instead of the logistic function and cross entropy for training.  But that's a topic for either later or your own exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That gives us some background to start talking about Neural Networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks  <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "G. Richards\n",
    "(2016, 2018, 2020, 2022)\n",
    "Ivezic 9.8\n",
    "\n",
    "where I found this video series particularly helpful in trying to simplify the explanation https://www.youtube.com/watch?v=bxe2T-V8XRs. \n",
    "\n",
    "[Artificial Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network) are a simplified computation architecture based loosely on the real neural networks found in brains.  In reality, what we are going to explore is a [multi-layer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron).\n",
    "\n",
    "In the image below the circles on the left represent the **attributes** of our input data, $X$, which here is 3 dimensional.  The circles in the middle represent the neurons.  They take in the information from the input and, based on some criterion decide whether or not to \"fire\".  The collective results of the neurons in the hidden layer produce the output, $y$, which is represented by the circles on the right, which here is 2 dimensional result.  The lines connecting the circles represent the synapses.  This is a simple example with just one layer of neurons; however, there can be many layers of neurons.\n",
    "![Cartoon of Neural Network](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/500px-Artificial_neural_network.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In more detail:\n",
    "\n",
    "The job of a synapse is to take input values and multiply them by some **weight**, $w$, and add a **bias**, $b$, before passing them to the neuron (hidden layer):\n",
    "\n",
    "$$z = \\sum_i w x_i + b$$\n",
    "\n",
    "The bias determines the input level at which the neuron \"fires\". It is always present, and unique to each neuron, but we'll set that to zero for the sake of simplicity here.\n",
    "\n",
    "The neuron then sums up the inputs from all of the synapses connected to it and applies an \"**activation function**\", e.g., a **[sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation function**.\n",
    "\n",
    "$$a = \\frac{1}{1+e^{-z}}.$$\n",
    "\n",
    "![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/500px-Logistic-curve.svg.png)\n",
    "\n",
    "What the neural network does is to learn the weights and biases of the synapses that are needed to produce an accurate model of $y_{\\rm train}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Rather than think about the inputs individually, we can write this process in matrix form as\n",
    "$$X W^{(1)} = Z^{(2)}.$$\n",
    "\n",
    "If $D$ is the number of attributes (here 3) and $H$ is the number of neurons in the hidden layer (here 4), then $X$ is an $N\\times D$ matrix, while $W^{(1)}$ is a $D\\times H$ matrix.  The result, $Z^{(2)}$, is then an $N\\times H$ matrix.\n",
    "\n",
    "We then apply the activation function to each entry of $Z^{(2)}$ independently: \n",
    "$$A^{(2)} = f(Z^{(2)}),$$\n",
    "where $A^{(2)}$ is the output of the neurons in the hidden layer and is also $N\\times H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These values are then the inputs for the next set of synapses, where we multiply the inputs by another set of weights, $W^{(2)}:$\n",
    "$$A^{(2)} W^{(2)} = Z^{(3)},$$\n",
    "\n",
    "where $W^{(2)}$ is an $H\\times O$ matrix and $Z^{(3)}$ is an $N\\times O$ matrix with $O$-dimensional output.\n",
    "\n",
    "Another activation function is then applied to $Z^{(3)}$ to give\n",
    "$$\\hat{y} = f(Z^{(3)}),$$\n",
    "which is our estimator of $y$.\n",
    "\n",
    "This is a [feedforward](https://en.wikipedia.org/wiki/Feedforward_neural_network) neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example we might have $N=100$ people for which we have measured \n",
    "* shoe size\n",
    "* belt size\n",
    "* hat size\n",
    "\n",
    "for whom we know their height and mass.  \n",
    "\n",
    "Then we are going to use this to predict the height and mass for people where we only know shoe size, belt size, and hat size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The neural network then essentially boils down to determining the weights and biases of the synapses, which are usually initialized randomly.\n",
    "\n",
    "We do that by minimizing the cost function (which compares the true values of $y$ to our predicted values).  Typically an MSE cost:\n",
    "$$ {\\rm Cost} = J = \\sum\\frac{1}{2}(y - \\hat{y})^2.$$\n",
    "\n",
    "As we saw above, that would be a cost function for regression (where we have only one output node).  For classification, we'd use one of the examples above (but ideally one that is differentiable as we'll see next time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we just had 1 weight and we wanted to check 1000 possible values, that wouldn't be so bad.  But we have 20 weights, which means checking $20^{1000}$ possible combinations.    Remember the curse of dimensionality?  That might take a while.  Indeed, far, far longer than the age of the Universe.\n",
    "\n",
    "Thus the (first) death of neural networks (which are currently on life #3, which is looking more promising).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Life #2 begins with the realization that we can write an analytic formula for the *gradient* going backwards and use that to update our weights.\n",
    "\n",
    "For example, how about just checking 3 points for each weight and see if we can at least figure out which way is \"down hill\"?  That's a start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can rewrite $J$ as\n",
    "$$ J = \\sum\\frac{1}{2}\\left(y - f\\left( f(X W^{(1)}) W^{(2)} \\right) \\right)^2$$\n",
    "\n",
    "and then compute\n",
    "$$\\frac{\\partial J}{\\partial W}$$\n",
    "in order to determine the slope of the cost function for each weight.  This is the **gradient descent** method, which we encountered above.  Your choice of cost function is important here; specifically you want it to be differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll want $\\partial J/\\partial W^{(1)}$ and $\\partial J/\\partial W^{(2)}$ separately.  This allows us to [*backpropagate*](https://en.wikipedia.org/wiki/Backpropagation) the error contributions along each neuron and to change the weights where they most need to be changed.  It is like each observation gets a vote on which way is \"down hill\".  We compute the vector sum to decide the ultimate down hill direction.\n",
    "\n",
    "Once we know the down hill direction from the derivative, we update the weights by subtracting a scalar times that derivative from the original weights.  That's obviously much faster than randomly sampling all the possible combinations of weights.  Once the weights are set, then you have your Neural Network classifier/regressor.\n",
    "\n",
    "![Cartoon of Neural Network](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/500px-Artificial_neural_network.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scikit-Learn has both [unsupervised Neural Network](http://scikit-learn.org/stable/modules/neural_networks_unsupervised.html#neural-networks-unsupervised) and [supervised Neural Network](http://scikit-learn.org/stable/modules/neural_networks_supervised.html#neural-networks-supervised) examples. \n",
    "\n",
    "Let's try to use the multi-layer perceptron classifier on the Boston House Price dataset (using 75% of the data for training and 25% for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston = load_boston()\n",
    "#print boston.DESCR\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "Xtrain_scaled = preprocessing.scale(Xtrain)\n",
    "Xtest_scaled = preprocessing.scale(Xtest)\n",
    "Xscaled = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "clf = MLPRegressor(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=1, max_iter=5000)\n",
    "clf.fit(Xtrain_scaled, ytrain)\n",
    "\n",
    "# Look at the weights\n",
    "print([coef.shape for coef in clf.coefs_])\n",
    "\n",
    "ypred = clf.predict(Xtest_scaled)\n",
    "#print ypred, ytest\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.scatter(ytest,ypred)\n",
    "plt.xlabel(\"Actual Value [x$1000]\")\n",
    "plt.ylabel(\"Predicted Value [x$1000]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Of course, that only predicts the value for a fraction of the data set.  Again, we can use Scikit-Learn's [cross_val_predict](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html#sklearn.model_selection.cross_val_predict) to make predictions for the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "yCVpred = cross_val_predict(clf, Xscaled, y, cv=10) # Complete\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y,yCVpred)\n",
    "plt.xlabel(\"Actual Value [x$1000]\")\n",
    "plt.ylabel(\"Predicted Value [x$1000]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also use cross validation to figure out how many hidden layers and neurons to use.  We'll set the number of layers to 2 and the number of neurons in the 2nd layer to 2 as well, then figure out the best number of neurons for the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "hidden_size = np.arange(3,12)\n",
    "scores = np.array([])\n",
    "for sz in hidden_size:\n",
    "    clf = MLPRegressor(solver='lbfgs', alpha=1e-5, random_state=0, hidden_layer_sizes=(sz,2), max_iter=5000)\n",
    "    scores = np.append(scores, np.mean(cross_val_score(clf, Xscaled, y, cv=5)))\n",
    "    \n",
    "#plt.plot(hidden_size,scores)\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(hidden_size,scores,'x-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More on the number of layers, number of neurons, and other details next time."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
