{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discriminative Classification\n",
    "\n",
    "G. Richards\n",
    "(2016, 2018, 2020, 2022)\n",
    "based on materials from Connolly, VanderPlas, and Ivezic Ch 9.5-9.7, and 9.10.  With updates to my own class from [Stephen Taylor's class at Vanderbilt](https://github.com/VanderbiltAstronomy/astr_8070_s22)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents\n",
    "* [Discriminative classification](#one)\n",
    "* [Logistic regression](#two)\n",
    "* [Support vector machines (SVM)](#three)\n",
    "* [Decision trees](#four)\n",
    "* [Ensemble learning (bagging, random forests, and boosting)](#five)\n",
    "* [Alright, brilliant, but which one should I use?](#six)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discriminative Classification <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "Last time we talked about how to perform classification by mapping the full pdf of the data's feature space, and probabilistically ranking the population that a new sample would belong to. This was **generative classification**, because we we estimating a generative model of the data. \n",
    "\n",
    "This time we will concentrate on methods that seek only to determine the **decision boundary** in feature space, so called [**discriminative classification**](https://en.wikipedia.org/wiki/Discriminative_model) methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As before, let's say that you have 2 blobs of data as shown below.  In many cases, you might say \"just draw a line between those two blobs that are well separated\".  So let's do exactly that in the example below.  There are clearly lots of different lines that you could draw that would work.  So, how do you do this *optimally*?  And what if the blobs are not perfectly well separated?\n",
    "\n",
    "<!-- ![Ivezic, Figure 9.9](figures/svm_lines.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Source: https://github.com/jakevdp/ESAC-stats-2014/blob/master/notebooks/04.1-Classification-SVMs.ipynb\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring' ,edgecolor='k');\n",
    "\n",
    "Xgrid = np.linspace(-1, 3.5)\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(Xgrid, m * Xgrid + b, '-k')\n",
    "    \n",
    "plt.xlim(-1, 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "In [Logistic Regression](https://www.wikiwand.com/en/Logistic_regression) we use a model that is almost identical to how we did LDA. For two-class classification with $y=\\{0,1\\}$ we use the [logistic function](https://www.wikiwand.com/en/Logistic_function) to write\n",
    "\n",
    "$$ p(y=1|x) = \\frac{\\exp\\left[\\sum_j\\theta_jx^j \\right]}{1 + \\exp\\left[\\sum_j\\theta_jx^j \\right]}, $$\n",
    "\n",
    "where the sum is over data features, and for the $i$-th sample we define\n",
    "\n",
    "$$ \\ln\\left(\\frac{p_i}{1-p_i} \\right) = \\sum_j\\theta_jx^j_i. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In **LDA** we modeled the pdfs of each class as having identical multivariate Gaussian covariance functions in feature-space. We used the model: \n",
    "$$ \n",
    "\\begin{align}\n",
    "        \\ln\\left( \\frac{p(y=1|x)}{p(y=0|x)} \\right) &= -\\frac{1}{2}(\\mu_0+\\mu_1)^T\\Sigma^{-1}(\\mu_0-\\mu_1) + \\ln\\frac{p(y=0)}{p(y=1)} + x^T\\Sigma^{-1}(\\mu_1-\\mu_0) \\\\\n",
    "        &= \\alpha_0 + \\alpha^T x,\n",
    "\\end{align}\n",
    "$$\n",
    "which is clearly linear in the data.\n",
    "\n",
    "\n",
    "- In **Logistic Regression**, the model is by assumption\n",
    "\n",
    "$$ \\ln\\left( \\frac{p(y=1|x)}{p(y=0|x)} \\right) = \\beta_0 + \\beta^T x $$\n",
    "\n",
    "\n",
    "The only difference between LDA and Logstic Regression is how the regression coefficients are estimated. In LDA they are chosen to minimize density estimation error, whereas in Logistic Regression they are chosen to minimize classification error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how Logistic Regression performs on our classification of non-variable stars versus RR Lyrae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# # Ivezic v2, Figure 9.8, edits by SRT\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "from matplotlib import colors\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples\n",
    "from astroML.utils import completeness_contamination\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# get data and split into training & testing sets\n",
    "Xnew, ynew = fetch_rrlyrae_combined()\n",
    "Xnew = Xnew[:, [1, 0, 2, 3]]  # rearrange columns for better 1-color results\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(Xnew, ynew, [0.75, 0.25],\n",
    "                                                     random_state=0)\n",
    "\n",
    "N_tot = len(ynew)\n",
    "N_st = np.sum(ynew == 0)\n",
    "N_rr = N_tot - N_st\n",
    "N_train = len(y_train)\n",
    "N_test = len(y_test)\n",
    "N_plot = 5000 + N_rr\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# perform Classification\n",
    "classifiers = []\n",
    "predictions = []\n",
    "Ncolors = np.arange(1, Xnew.shape[1] + 1)\n",
    "\n",
    "for nc in Ncolors:\n",
    "    clf = LogisticRegression(class_weight='balanced')\n",
    "    clf.fit(X_train[:, :nc], y_train)\n",
    "    y_pred = clf.predict(X_test[:, :nc])\n",
    "\n",
    "    classifiers.append(clf)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "completeness, contamination = completeness_contamination(predictions, y_test)\n",
    "\n",
    "print(\"completeness\", completeness)\n",
    "print(\"contamination\", contamination)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the decision boundary\n",
    "clf = classifiers[1]\n",
    "xlim = (0.7, 1.35)\n",
    "ylim = (-0.15, 0.4)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 71),\n",
    "                     np.linspace(ylim[0], ylim[1], 81))\n",
    "\n",
    "Z = clf.predict_proba(np.c_[yy.ravel(), xx.ravel()])[:, 1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(Xnew[-N_plot:, 1], Xnew[-N_plot:, 0], c=ynew[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.Oranges, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "ax.contour(xx, yy, Z, [0.5], colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "# plot completeness vs Ncolors\n",
    "ax = fig.add_subplot(222)\n",
    "\n",
    "ax.plot(Ncolors, completeness, 'o-k', ms=6)\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.set_ylabel('completeness')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# plot contamination vs Ncolors\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(Ncolors, contamination, 'o-k', ms=6)\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "ax.set_xlabel('N colors')\n",
    "ax.set_ylabel('contamination')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines  <a class=\"anchor\" id=\"three\"></a>\n",
    "Visualizing different possible decision boundaries (and deciding which is best) is where [Support Vector Machines (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine) come in.  We are going to define a hyperplane (a plane in $N-1$ dimensions) that maximizes the distance of the closest point from each class.  This distance is the \"margin\".  It is the width of the \"cylinder\" or \"street\" that you can put between the closest points that just barely touches the points in each class.  The points that touch the margin (or that are on the wrong side) are called **support vectors**.  Obvious, right?\n",
    "\n",
    "Once again, we have an algortihm that seems purposely named to frighten people away.  Though I don't know that \"Data-Supported Hyperplane\" classification would be any better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Xgrid = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring', edgecolor='k')\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * Xgrid+ b\n",
    "    plt.plot(Xgrid, yfit, '-k')\n",
    "    plt.fill_between(Xgrid, yfit - d, yfit + d, edgecolor='None', color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are lots of potential decision boundaries, but we want the one that maximize the distance of the support vectors from the decision hyperplane.\n",
    "\n",
    "Here's the math for determining that decision boundary.  To make life easier, we'll assume that the classes are separable by a straight line and that the decision boundary is at 0, with the two edges at $-1$ and $1$, and define $y \\in \\{-1,1\\}$.\n",
    "\n",
    "The hyperplane which maximizes the margin is given by finding\n",
    "\n",
    "> \\begin{equation}\n",
    "\\max_{\\beta_0,\\beta}(m) \\;\\;\\;\n",
    "  \\mbox{subject to} \\;\\;\\; \\frac{1}{||\\beta||} y_i ( \\beta_0 + \\beta^T x_i )\n",
    "  \\geq m \\,\\,\\, \\forall \\, i,\n",
    "\\end{equation}\n",
    "\n",
    "where $||\\beta|| = \\sqrt{\\sum \\beta^2}$. \n",
    "\n",
    "The constraints can be written as $y_i ( \\beta_0 + \\beta^T x_i ) \\geq m ||\\beta|| $. \n",
    "\n",
    "Thus the optimization problem is equivalent to minimizing\n",
    "> \\begin{equation}\n",
    "\\frac{1}{2} ||\\beta|| \\;\\;\\; \\mbox{subject to} \\;\\;\\; y_i\n",
    "  ( \\beta_0 + \\beta^T x_i ) \\geq 1 \\,\\,\\, \\forall \\, i.\n",
    "\\end{equation}\n",
    "\n",
    "This optimization  is a _quadratic programming_ problem (quadratic objective function with linear constraints).\n",
    "\n",
    "To make sure that we get through all the remaining classification algorithms, we'll skip over the mathematical details.  You can read about them in Ivezic $\\S$ 9.6 or in Karen Leighly's [classification lecture notes](http://seminar.ouml.org/lectures/classification/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For realistic data sets where the decision boundary is not obvious we relax the assumption that the classes are linearly separable.  This changes the minimization condition and puts bounds on the number of misclassifications (which we would obviously like to minimize)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Treating Scikit-Learn's agorithm as a black box, let's fit a Support Vector Machine Classifier to these points.\n",
    "\n",
    "The Scikit-Learn implementation of SVM classification is [`SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) which looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear',C=1.0)\n",
    "svm.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to better visualize what SVM is doing, let's create a convenience function that will plot the decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(clf, ax=None):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    u = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
    "    v = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)\n",
    "    yy, xx = np.meshgrid(v, u)\n",
    "\n",
    "    P = np.zeros_like(xx)\n",
    "    for i, ui in enumerate(u):\n",
    "        for j, vj in enumerate(v):\n",
    "            Xgrid = np.array([ui, vj])\n",
    "            P[i, j] = clf.decision_function(Xgrid.reshape(1,-1))\n",
    "    return ax.contour(xx, yy, P, colors='k',\n",
    "                      levels=[-1, 0, 1], alpha=0.5,\n",
    "                      linestyles=['--', '-', '--'])\n",
    "\n",
    "    #GTR: Not clear why we need the loops and can't just\n",
    "    # make the Xgrid array like we normally do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's plot the decision boundary and the support vectors, which are stored in the `support_vectors_` attribute of the classifier.  Try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(____.____[:, 0], ____.____[:, 1], s=200, edgecolor='k', facecolor='w'); #Add support vectors\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring', edgecolor='k')\n",
    "plot_svc_decision_function(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at what happens for data that have some overlap between the classes.   \n",
    "\n",
    "Execute the cell below to create overlapping data, then go back to the previous cell to plot the decision boundary and support vectors of the best-fit SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Source: https://github.com/jakevdp/ESAC-stats-2014/blob/master/notebooks/04.1-Classification-SVMs.ipynb\n",
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring' ,edgecolor='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our example above included a parameter, $C$, which I didn't explain.  According to the [SVC documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC), $C$ is a \"Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\"\n",
    "\n",
    "Thus this is the **inverse** of the $\\alpha$ regularization parameter we saw last week.  Let's see what happens when we change it.  Fit these data with `C=100` (less regularization) and `C=0.01` (more regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear',C=____)\n",
    "svm.____(X,y)\n",
    "plt.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], s=200, edgecolor='k', facecolor='w');\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring', edgecolor='k')\n",
    "plot_svc_decision_function(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We see that $C$ controls the number of \"margin violations\" that are allowed and the width of the \"street\" that runs between the data (i.e., the number of support vectors).\n",
    "\n",
    "Note that SVM *only* depends on the support vectors.  You can get rid of the rest of the data.  Smaller $C$ causes there to be more support vectors, while larger $C$ gives fewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below is an example using the same data set from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 9.10, edits by GTR\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from astroML.utils.decorators import pickle_results\n",
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples\n",
    "from astroML.utils import completeness_contamination\n",
    "from matplotlib import colors\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# get data and split into training & testing sets\n",
    "X, y = fetch_rrlyrae_combined()\n",
    "X = X[:, [1, 0, 2, 3]]  # rearrange columns for better 1-color results\n",
    "\n",
    "# SVM takes several minutes to run, and is order[N^2]\n",
    "#  truncating the dataset can be useful for experimentation.\n",
    "#X = X[::5]\n",
    "#y = y[::5]\n",
    "\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.75, 0.25],\n",
    "                                                     random_state=0)\n",
    "\n",
    "N_tot = len(y)\n",
    "N_st = np.sum(y == 0)\n",
    "N_rr = N_tot - N_st\n",
    "N_train = len(y_train)\n",
    "N_test = len(y_test)\n",
    "N_plot = 5000 + N_rr\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Fit SVM\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)\n",
    "\n",
    "\n",
    "@pickle_results('SVM_rrlyrae.pkl')\n",
    "def compute_SVM(Ncolors):\n",
    "    classifiers = []\n",
    "    predictions = []\n",
    "\n",
    "    for nc in Ncolors:\n",
    "        # perform support vector classification\n",
    "        clf = SVC(kernel='linear', class_weight='balanced')\n",
    "        clf.fit(X_train[:, :nc], y_train)\n",
    "        y_pred = clf.predict(X_test[:, :nc])\n",
    "\n",
    "        classifiers.append(clf)\n",
    "        predictions.append(y_pred)\n",
    "\n",
    "    return classifiers, predictions\n",
    "\n",
    "classifiers, predictions = compute_SVM(Ncolors)\n",
    "\n",
    "completeness, contamination = completeness_contamination(predictions, y_test)\n",
    "\n",
    "print(\"completeness\", completeness)\n",
    "print(\"contamination\", contamination)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute the decision boundary\n",
    "clf = classifiers[1]\n",
    "w = clf.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "yy = np.linspace(-0.1, 0.4)\n",
    "xx = a * yy - clf.intercept_[0] / w[1]\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(xx, yy, '-k')\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.Oranges, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "ax.set_xlim(0.7, 1.35)\n",
    "ax.set_ylim(-0.15, 0.4)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "# plot completeness vs Ncolors\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(Ncolors, completeness, 'o-k', ms=6)\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.set_ylabel('completeness')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# plot contamination vs Ncolors\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(Ncolors, contamination, 'o-k', ms=6)\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "\n",
    "ax.set_xlabel('N colors')\n",
    "ax.set_ylabel('contamination')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some comments on these results:\n",
    "\n",
    "- The median of a distribution is unaffected by large perturbations of outlying points, as long as those perturbations do not cross the boundary.\n",
    "- In the same way, once the support vectors are determined, changes to the positions or numbers of points beyond the margin will not change the decision boundary.  For this reason, SVM can be a very powerful tool for discriminative classification.\n",
    "- This is why there is a high completeness compared to the other methods: it does not matter that the background sources outnumber the RR Lyrae stars by a factor of $\\sim$200 to 1. It simply determines the best boundary between the small RR Lyrae clump and the large background clump.\n",
    "- This completeness, however, comes at the cost of a relatively large contamination level.\n",
    "\n",
    "Note that:\n",
    "- SVM is not scale invariant so it often worth rescaling the data to [0,1] or to whiten it to have a mean of 0 and variance 1 (remember to do this to the test data as well!)\n",
    "- The data don't need to be separable (we can put a constraint in minimizing the number of \"failures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Kernel Methods\n",
    "If the contamination is driven by non-linear effects (which isn't the case here), it may be worth implementing a non-linear decision boundary.  As before, we do that by *kernelization*.  \n",
    "\n",
    "Let's take a look at an example where the data are not linearly separable and where kernelization really makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, \n",
    "            cmap='spring', edgecolor='k')\n",
    "plot_svc_decision_function(clf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But we can make a transform of the data to *make* it linearly separable, for example with a simple **radial basis function** as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a new features that is related to the square of the input features\n",
    "# Points near and far from the origin will end up far away from each other.\n",
    "z = np.exp(-(X[:, 0] ** 2 + X[:, 1] ** 2))\n",
    "\n",
    "from ipywidgets import interact\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def plot_3D(elev=30, azim=30):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], z, c=y, s=50, cmap='spring', edgecolor='k')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "\n",
    "plot_3D()\n",
    "# GTR: Or even fancier with \n",
    "# interact(plot_3D, elev=[-90, 90], azip=(-180, 180));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can trivially separate these populations using `kernel='rbf'` and `C=10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clf = SVC(____=____,____=____)\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring', edgecolor='k')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=200, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the \"Moons\" example that gave us such a hard time with K-means clustering.  In addition to the $C$ parameter, we'll adjust $\\gamma$ which is the coefficient for the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Geron\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
    "\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#For plotting the boundaries in the Moons example\n",
    "#Geron\n",
    "def plot_predictions(clf, axes):\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X = np.c_[x0.ravel(), x1.ravel()]\n",
    "    y_pred = clf.predict(X).reshape(x0.shape)\n",
    "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Try two very different values for $\\gamma$ and $C$.  Of the four combinations, try to make one case where SVM is badly underfitting and one case where it is badly overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Geron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "gamma1, gamma2 = ____, ____\n",
    "C1, C2 = ____, _____\n",
    "hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n",
    "\n",
    "svm_clfs = []\n",
    "for gamma, C in hyperparams:\n",
    "    rbf_kernel_svm_clf = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n",
    "        ])\n",
    "    rbf_kernel_svm_clf.fit(X, y)\n",
    "    svm_clfs.append(rbf_kernel_svm_clf)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)\n",
    "\n",
    "for i, svm_clf in enumerate(svm_clfs):\n",
    "    plt.sca(axes[i // 2, i % 2])\n",
    "    plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5])\n",
    "    plot_dataset(X, y, [-1.5, 2.45, -1, 1.5])\n",
    "    gamma, C = hyperparams[i]\n",
    "    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=16)\n",
    "    if i in (0, 1):\n",
    "        plt.xlabel(\"\")\n",
    "    if i in (1, 3):\n",
    "        plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note also the `Pipeline` function used in that example. It can chain together lots of operations that one would do manually in sequence, but can be easily packaged together into a compact workflow. In this circumstance it scaled the data before performing SVM classification; as mentioned SVM often requires scaling the data beforehand since it is not scale-invariant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We won't do it here, but note that SVM can be used for regression too.  There you are trying to maximize the number of points that are **on** rather than off the \"street\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees  <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "A [**decision tree**](https://en.wikipedia.org/wiki/Decision_tree) is similar to the process of classification that you might do by hand: \n",
    "- define some criteria to separate the sample into 2 groups (not necessarily equal),\n",
    "- then take those sub-groups and do it again.  \n",
    "- keep going until you reach a stopping point such as not having a minimum number of objects to split again.\n",
    "\n",
    "The tree structure is as follows:\n",
    "- top node contains the entire data set\n",
    "- at each branch the data are subdivided into two child nodes \n",
    "- split is based on a predefined decision boundary (usually axis aligned)\n",
    "- splitting repeats, recursively, until we reach a predefined stopping criteria \n",
    "\n",
    "Below is a simple example of a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Source: Jake VanderPlas, https://github.com/LocalGroupAstrostatistics2015/MachineLearning/blob/master/fig_code/figures.py\n",
    "def plot_example_decision_tree():\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    ax = fig.add_axes([0, 0, 0.8, 1], frameon=False, xticks=[], yticks=[])\n",
    "    ax.set_title('Example Decision Tree: Animal Classification', size=24)\n",
    "\n",
    "    def text(ax, x, y, t, size=20, **kwargs):\n",
    "        ax.text(x, y, t,\n",
    "                ha='center', va='center', size=size,\n",
    "                bbox=dict(boxstyle='round', ec='k', fc='w'), **kwargs)\n",
    "\n",
    "    text(ax, 0.5, 0.9, \"How big is\\nthe animal?\", 20)\n",
    "    text(ax, 0.3, 0.6, \"Does the animal\\nhave horns?\", 18)\n",
    "    text(ax, 0.7, 0.6, \"Does the animal\\nhave two legs?\", 18)\n",
    "    text(ax, 0.12, 0.3, \"Are the horns\\nlonger than 10cm?\", 14)\n",
    "    text(ax, 0.38, 0.3, \"Is the animal\\nwearing a collar?\", 14)\n",
    "    text(ax, 0.62, 0.3, \"Does the animal\\nhave wings?\", 14)\n",
    "    text(ax, 0.88, 0.3, \"Does the animal\\nhave a tail?\", 14)\n",
    "\n",
    "    text(ax, 0.4, 0.75, \"> 1m\", 12, alpha=0.4)\n",
    "    text(ax, 0.6, 0.75, \"< 1m\", 12, alpha=0.4)\n",
    "\n",
    "    text(ax, 0.21, 0.45, \"yes\", 12, alpha=0.4)\n",
    "    text(ax, 0.34, 0.45, \"no\", 12, alpha=0.4)\n",
    "\n",
    "    text(ax, 0.66, 0.45, \"yes\", 12, alpha=0.4)\n",
    "    text(ax, 0.79, 0.45, \"no\", 12, alpha=0.4)\n",
    "\n",
    "    ax.plot([0.3, 0.5, 0.7], [0.6, 0.9, 0.6], '-k')\n",
    "    ax.plot([0.12, 0.3, 0.38], [0.3, 0.6, 0.3], '-k')\n",
    "    ax.plot([0.62, 0.7, 0.88], [0.3, 0.6, 0.3], '-k')\n",
    "    ax.plot([0.0, 0.12, 0.20], [0.0, 0.3, 0.0], '--k')\n",
    "    ax.plot([0.28, 0.38, 0.48], [0.0, 0.3, 0.0], '--k')\n",
    "    ax.plot([0.52, 0.62, 0.72], [0.0, 0.3, 0.0], '--k')\n",
    "    ax.plot([0.8, 0.88, 1.0], [0.0, 0.3, 0.0], '--k')\n",
    "    ax.axis([0, 1, 0, 1])\n",
    "\n",
    "plot_example_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<!-- For our RR Lyrae stars --> \n",
    "<! -- ![Ivezic, Figure 9.12](http://www.astroml.org/_images/fig_rrlyrae_treevis_1.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The **terminal (leaf) nodes** record the fraction of points that have one classification or the other in the training set.\n",
    "\n",
    "Application of the tree to classification is simple (a series of binary decisions). The  fraction of points from the training set classified as one class or the other (in the leaf node) defines the class associated with that leaf node.\n",
    "\n",
    "The binary splitting makes this extremely efficient. The trick is to ask the *right* questions.\n",
    "So, decision trees are simple to interpret (just a set of questions).\n",
    "\n",
    "Scikit-learn implements the [`DecisionTreeClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X = np.random.random((100,2))\n",
    "y = (X[:,0] + X[:,1] > 1).astype(int)\n",
    "dtree = DecisionTreeClassifier(max_depth=6)\n",
    "dtree.fit(X,y)\n",
    "y_pred = dtree.predict(X)\n",
    "print(dtree.get_depth())\n",
    "print(dtree.get_n_leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An example with our data set of RR Lyrae stars shows that it has moderately good completenees and contamination, but that, for this data set, it is not the optimal choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 9.13, edits by GTR\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples\n",
    "from astroML.utils import completeness_contamination\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# get data and split into training & testing sets\n",
    "X, y = fetch_rrlyrae_combined()\n",
    "X = X[:, [1, 0, 2, 3]]  # rearrange columns for better 1-color results\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.75, 0.25],\n",
    "                                                     random_state=0)\n",
    "N_tot = len(y)\n",
    "N_st = np.sum(y == 0)\n",
    "N_rr = N_tot - N_st\n",
    "N_train = len(y_train)\n",
    "N_test = len(y_test)\n",
    "N_plot = 5000 + N_rr\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Fit Decision tree\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)\n",
    "\n",
    "classifiers = []\n",
    "predictions = []\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)\n",
    "depths = [7, 12]\n",
    "\n",
    "for depth in depths:\n",
    "    classifiers.append([])\n",
    "    predictions.append([])\n",
    "    for nc in Ncolors:\n",
    "        clf = DecisionTreeClassifier(random_state=0, max_depth=depth,\n",
    "                                     criterion='entropy')\n",
    "        clf.fit(X_train[:, :nc], y_train)\n",
    "        y_pred = clf.predict(X_test[:, :nc])\n",
    "\n",
    "        classifiers[-1].append(clf)\n",
    "        predictions[-1].append(y_pred)\n",
    "\n",
    "completeness, contamination = completeness_contamination(predictions, y_test)\n",
    "\n",
    "print(\"completeness\", completeness)\n",
    "print(\"contamination\", contamination)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute the decision boundary\n",
    "\n",
    "clf = classifiers[1][1]\n",
    "xlim = (0.7, 1.35)\n",
    "ylim = (-0.15, 0.4)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 101),\n",
    "                     np.linspace(ylim[0], ylim[1], 101))\n",
    "\n",
    "Z = clf.predict(np.c_[yy.ravel(), xx.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(13, 6))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:],\n",
    "                s=4, lw=0, cmap=plt.cm.Oranges, zorder=2)\n",
    "im.set_clim(-0.5, 1)\n",
    "\n",
    "ax.contour(xx, yy, Z, [0.5], colors='k')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "ax.set_xlabel('$u-g$')\n",
    "ax.set_ylabel('$g-r$')\n",
    "\n",
    "ax.text(0.02, 0.02, \"depth = %i\" % depths[1],\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "# plot completeness vs Ncolors\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(Ncolors, completeness[0], 'o-k', ms=6, label=\"depth=%i\" % depths[0])\n",
    "ax.plot(Ncolors, completeness[1], '^--k', ms=6, label=\"depth=%i\" % depths[1])\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax.set_ylabel('completeness')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# plot contamination vs Ncolors\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(Ncolors, contamination[0], 'o-k', ms=6, label=\"depth=%i\" % depths[0])\n",
    "ax.plot(Ncolors, contamination[1], '^--k', ms=6, label=\"depth=%i\" % depths[1])\n",
    "ax.legend(loc='lower right',\n",
    "          bbox_to_anchor=(1.0, 0.79))\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "\n",
    "ax.set_xlabel('N colors')\n",
    "ax.set_ylabel('contamination')\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Splitting Criteria\n",
    "\n",
    "Now let's talk about the best ways to split the data.  This is actually a really hard problem, which you can read about more in Ivezic $\\S$ 9.7.1.  \n",
    "\n",
    "One way is to use the information content or entropy, $E(x)$, of the data\n",
    "\n",
    "$$ E(x) = -\\sum_i p_i(x) \\ln (p_i(x)),$$\n",
    "\n",
    "where $i$ is the class and $p_i(x)$ is the probability of that class\n",
    "given the training data.   We can define the **information gain** as the reduction in entropy due to the partitioning of the data (i.e. by partitioning the data you have reduced the disorder), which is sometimes also referred to as the **Kullback-Leibler (KL) Divergence.** \n",
    "\n",
    "The typical process for finding the optimal decision boundary is to perform trial splits along each feature one at a time, within which the value of the feature to split at is also trialed. The feature that allows for the maximum information gain is the one that is split at this level.\n",
    "\n",
    "Another commonly used \"loss function\" (especially for categorical classification) is the **Gini coefficient**:\n",
    "$$ G = \\sum_i^k p_i(1-p_i).$$\n",
    "\n",
    "It essentially estimates the probability of incorrect classification by choosing both a point and (separately) a class randomly from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Try changing the example above to use `criterion='gini'` and `min_samples_leaf=3`, see [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) for all of the parameters.\n",
    "\n",
    "I'll say more about this next time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Obviously in constructing a decision treee, if your choice of stopping criteria is too loose, further splitting just ends up adding noise.  So here is an example using cross-validation in order to optimize the depth of the tree (and to avoid overfitting -- that is, we are doing regularization).  \n",
    "\n",
    "Note that here we aren't classifying the objects into discrete categories, rather we are classifying them into a continuous category.  That is, we are doing regression.  In this particular case, we are using the colors of galaxies in order to predict their redshifts (distances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 9.14\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from astroML.datasets import fetch_sdss_specgals\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch data and prepare it for the computation\n",
    "data = fetch_sdss_specgals()\n",
    "\n",
    "# put magnitudes in a matrix\n",
    "mag = np.vstack([data['modelMag_%s' % f] for f in 'ugriz']).T\n",
    "z = data['z']\n",
    "\n",
    "# train on ~60,000 points\n",
    "mag_train = mag[::10]\n",
    "z_train = z[::10]\n",
    "\n",
    "# test on ~6,000 separate points\n",
    "mag_test = mag[1::100]\n",
    "z_test = z[1::100]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the cross-validation scores for several tree depths\n",
    "depth = np.arange(1, 21)\n",
    "rms_test = np.zeros(len(depth))\n",
    "rms_train = np.zeros(len(depth))\n",
    "i_best = 0\n",
    "z_fit_best = None\n",
    "\n",
    "for i, d in enumerate(depth):\n",
    "    clf = DecisionTreeRegressor(max_depth=d, random_state=0)\n",
    "    clf.fit(mag_train, z_train)\n",
    "\n",
    "    z_fit_train = clf.predict(mag_train)\n",
    "    z_fit = clf.predict(mag_test)\n",
    "    rms_train[i] = np.mean(np.sqrt((z_fit_train - z_train) ** 2))\n",
    "    rms_test[i] = np.mean(np.sqrt((z_fit - z_test) ** 2))\n",
    "\n",
    "    if rms_test[i] <= rms_test[i_best]:\n",
    "        i_best = i\n",
    "        z_fit_best = z_fit\n",
    "\n",
    "best_depth = depth[i_best]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.subplots_adjust(wspace=0.25,\n",
    "                    left=0.1, right=0.95,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "# first panel: cross-validation\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(depth, rms_test, '-k', label='cross-validation')\n",
    "ax.plot(depth, rms_train, '--k', label='training set')\n",
    "ax.set_xlabel('depth of tree')\n",
    "ax.set_ylabel('rms error')\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "ax.set_xlim(0, 21)\n",
    "ax.set_ylim(0.009,  0.04)\n",
    "ax.legend(loc=1)\n",
    "\n",
    "# second panel: best-fit results\n",
    "ax = fig.add_subplot(122)\n",
    "edges = np.linspace(z_test.min(), z_test.max(), 101)\n",
    "H, zs_bins, zp_bins = np.histogram2d(z_test, z_fit_best, bins=edges)\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto', \n",
    "           extent=[zs_bins[0], zs_bins[-1], zs_bins[0], zs_bins[-1]],\n",
    "           cmap=plt.cm.binary)\n",
    "ax.plot([-0.1, 0.4], [-0.1, 0.4], ':k')\n",
    "ax.text(0.04, 0.96, \"depth = %i\\nrms = %.3f\" % (best_depth, rms_test[i_best]),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "ax.set_xlabel(r'$z_{\\rm true}$')\n",
    "ax.set_ylabel(r'$z_{\\rm fit}$')\n",
    "\n",
    "ax.set_xlim(-0.02, 0.4001)\n",
    "ax.set_ylim(-0.02, 0.4001)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That's doing the Cross Validation by hand, let's try it automatically using data like the first example that we started with today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.20, random_state=42)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring',edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Do a `GridSearchCV` where you vary the `'max_depth'` between 1 and 21, using 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "drange = np.arange(____,____)  #Complete\n",
    "\n",
    "grid = ____(clf, param_grid={____: drange}, cv=____) #Complete\n",
    "grid.fit(X, y)\n",
    "\n",
    "best = grid.best_params_['max_depth']\n",
    "print(\"best parameter choice:\", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now plot the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=0, max_depth=best, criterion='entropy')\n",
    "dt.fit(X, y)\n",
    "yprob = dt.predict_proba(X) #If we wanted probabilities rather than the discrete classes\n",
    "\n",
    "xlim = (-1.5, 2.5)\n",
    "ylim = (-1, 1.5)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 51),\n",
    "                     np.linspace(ylim[0], ylim[1], 51))\n",
    "xystack = np.vstack([xx.ravel(),yy.ravel()])\n",
    "Xgrid = xystack.T\n",
    "\n",
    "Z = dt.predict(Xgrid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "#print(Z)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0,\n",
    "                    left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# left plot: data and decision boundary\n",
    "ax = fig.add_subplot(111)\n",
    "im = ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.spring, zorder=2, edgecolor='k')\n",
    "\n",
    "ax.contour(xx, yy, Z, [0.5], colors='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Note that Decision Trees are the basis for the rest of the material today.  So it is useful to consider some of the [advantages and disadvantages](https://scikit-learn.org/stable/modules/tree.html).\n",
    "\n",
    "For example Decision Trees are unstable to small changes in the data (unlike SVM).  If we rotated our moons example, we'd get a different answer.  We'll see how we can harness the power of Decision Trees to create even more robust classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble Learning  <a class=\"anchor\" id=\"five\"></a>\n",
    "\n",
    "You may have noticed that each of the classification methods that we have described so far has its strengths and weaknesses.  You might wonder if we could gain something by some sort of averaging of weighted \"voting\".  Such a process is what we call *ensemble learning*.  We'll discuss three such processes: \n",
    "- [**bagging**](https://en.wikipedia.org/wiki/Bootstrap_aggregating) \n",
    "- [**random forests**](https://en.wikipedia.org/wiki/Random_forest)\n",
    "- [**boosting**](https://www.wikiwand.com/en/Gradient_boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bagging\n",
    "\n",
    "**Bagging** (short for **bootstrap aggregation**--a name, unlike SVM, which actually makes some sense) can significantly improve the performance of decision trees.  In short, bagging averages the predictive results of a series of **bootstrap** samples.\n",
    "\n",
    "Remember that instead of splitting the sample into training and test sets that do not overlap, bootstrap says to draw from the observed data set with replacement.  So we select indices $j$ from the range $i=1,\\ldots,N$ and this is our new sample.  Some indices, $i$, will be repeated and we do this $B$ times.\n",
    "\n",
    "For a sample of $N$ points in a training set, bagging generates $B$ equally sized bootstrap samples from which to estimate the function $f_i(x)$. The final estimator for $\\hat{y}$, defined by bagging, is then\n",
    "$$\\hat{y} = f(x) = \\frac{1}{B} \\sum_i^B f_i(x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's compare how well a regular decision tree performs as compared to bagging applied to an ensemble of decision trees.  We'll use the same moons data from above, but a bit noisier to make it harder.  \n",
    "\n",
    "We'll also split into training and test sets so that both methods can be compared using a test set that neither have seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Following example from Geron\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring',edgecolor='k')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Regular Decision Tree\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Bagged Decision Tree\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, random_state=42, n_jobs=-1)\n",
    "%timeit bag_clf.fit(X_train, y_train)\n",
    "y_pred_bag = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note the `n_jobs=-1` parameter.  That says to use all the cores of your machine to do the job.  That's one of the benefits of bagging.  It can be made parallel trivially -- one bagging process has nothing to the with the others.  You just average them all together when you are done.  \n",
    "\n",
    "Try changing `n_jobs` to 1 and compare how much time it takes.  If you have a Mac, go to /Applications/Utilities/Activity Monitor, open that up and watch it spawn 4-8 Python 3.8 processes at once when you tell it to use all of the cores.\n",
    "\n",
    "OK, now let's compare the scores and we'll see that the bagging result beat the non-bagged result.  Then we'll make a plot and see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Now compare the scores\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred_tree))\n",
    "print(accuracy_score(y_test, y_pred_bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#From Geron for plotting the decision boundary\n",
    "from matplotlib.colors import ListedColormap\n",
    "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(tree_clf, X, y)\n",
    "plt.title(\"Decision Tree\", fontsize=14)\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(bag_clf, X, y)\n",
    "plt.title(\"Decision Trees with Bagging\", fontsize=14)\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forests\n",
    "\n",
    "**[Random forests](https://www.wikiwand.com/en/Random_forest)** extend bagging by generating decision trees from the bootstrap samples.  \n",
    "\n",
    "- In Random Forests, the ***splitting features on which to generate the tree are selected at random*** from the full set of features in the data.\n",
    "- The number of features selected per split level is typically the square root of the total number of features, $\\sqrt{D}$. \n",
    "- The final classification from the random forest is based on the averaging of the classifications of each of the individual decision trees. So, you can literally give it the kitchen sink (including attributes that you might not otherwise think would be useful for classification).\n",
    "- Random forests help to overcome some of the limitations of decision trees.\n",
    "\n",
    "As before, **cross-validation can be used to determine the optimal depth**.  Generally the number of trees, $n$, that are chosen is the number at which the cross-validation error plateaus.\n",
    "\n",
    "Below we give the same example as above for photometric estimation of galaxy redshifts, where Scikit-Learn's [`RandomForestClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) call looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X = np.random.random((100,2))\n",
    "y = (X[:,0] + X[:,1] > 1).astype(int)\n",
    "ranfor = RandomForestClassifier(10)\n",
    "ranfor.fit(X,y)\n",
    "y_pred = ranfor.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 9.15, edits by GTR\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from astroML.datasets import fetch_sdss_specgals\n",
    "from astroML.utils.decorators import pickle_results\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=12, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch and prepare the data\n",
    "data = fetch_sdss_specgals()\n",
    "\n",
    "# put magnitudes in a matrix\n",
    "mag = np.vstack([data['modelMag_%s' % f] for f in 'ugriz']).T\n",
    "z = data['z']\n",
    "\n",
    "# train on ~60,000 points\n",
    "mag_train = mag[::10]\n",
    "z_train = z[::10]\n",
    "\n",
    "# test on ~6,000 distinct points\n",
    "mag_test = mag[1::100]\n",
    "z_test = z[1::100]\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the results\n",
    "#  This is a long computation, so we'll save the results to a pickle.\n",
    "@pickle_results('photoz_forest.pkl')\n",
    "def compute_photoz_forest(depth):\n",
    "    rms_test = np.zeros(len(depth))\n",
    "    rms_train = np.zeros(len(depth))\n",
    "    i_best = 0\n",
    "    z_fit_best = None\n",
    "\n",
    "    for i, d in enumerate(depth):\n",
    "        clf = RandomForestRegressor(n_estimators=10,\n",
    "                                    max_depth=d, random_state=0)\n",
    "        clf.fit(mag_train, z_train)\n",
    "\n",
    "        z_fit_train = clf.predict(mag_train)\n",
    "        z_fit = clf.predict(mag_test)\n",
    "        rms_train[i] = np.mean(np.sqrt((z_fit_train - z_train) ** 2))\n",
    "        rms_test[i] = np.mean(np.sqrt((z_fit - z_test) ** 2))\n",
    "\n",
    "        if rms_test[i] <= rms_test[i_best]:\n",
    "            i_best = i\n",
    "            z_fit_best = z_fit\n",
    "\n",
    "    return rms_test, rms_train, i_best, z_fit_best\n",
    "\n",
    "\n",
    "depth = np.arange(1, 21)\n",
    "rms_test, rms_train, i_best, z_fit_best = compute_photoz_forest(depth)\n",
    "best_depth = depth[i_best]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.subplots_adjust(wspace=0.25,\n",
    "                    left=0.1, right=0.95,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "# left panel: plot cross-validation results\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(depth, rms_test, '-k', label='cross-validation')\n",
    "ax.plot(depth, rms_train, '--k', label='training set')\n",
    "ax.legend(loc=1)\n",
    "\n",
    "ax.set_xlabel('depth of tree')\n",
    "ax.set_ylabel('rms error')\n",
    "\n",
    "ax.set_xlim(0, 21)\n",
    "ax.set_ylim(0.009,  0.04)\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "\n",
    "# right panel: plot best fit\n",
    "ax = fig.add_subplot(122)\n",
    "\n",
    "edges = np.linspace(z_test.min(), z_test.max(), 101)\n",
    "H, zs_bins, zp_bins = np.histogram2d(z_test, z_fit_best, bins=edges)\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto', \n",
    "           extent=[zs_bins[0], zs_bins[-1], zs_bins[0], zs_bins[-1]],\n",
    "           cmap=plt.cm.binary)\n",
    "\n",
    "ax.plot([-0.1, 0.4], [-0.1, 0.4], ':k')\n",
    "ax.text(0.03, 0.97, \"depth = %i\\nrms = %.3f\" % (best_depth, rms_test[i_best]),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlabel(r'$z_{\\rm true}$')\n",
    "ax.set_ylabel(r'$z_{\\rm fit}$')\n",
    "\n",
    "ax.set_xlim(-0.02, 0.4001)\n",
    "ax.set_ylim(-0.02, 0.4001)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How many attributes/features is the code currently using?  Looking at [`RandomForestClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), print out how many features are being used and their relative importances\n",
    "\n",
    "Then rerun again forcing it to use 1 less feature than you found by adding a `max_features` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(n_estimators=20, max_depth=20, random_state=0)\n",
    "clf.fit(mag_train, z_train)\n",
    "print(clf.____)  #Number of Features\n",
    "print(clf.____) #Feature Importances\n",
    "print(clf.score(mag_test,z_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "**[Boosting](https://www.wikiwand.com/en/Gradient_boosting)** is an ensemble approach where many weak classifiers are combined and improved upon to make classification better. ***Boosting creates models that attempt to correct the errors of the ensemble so far.*** \n",
    "\n",
    "At the heart of boosting is the idea that we reweight the data based on how incorrectly the data were classified in the previous iteration.\n",
    "\n",
    "- We run the classification multiple times and each time reweight the data based on the previous performance of the classifier. \n",
    "- At the end of this procedure we allow the classifiers to vote on the final classification. \n",
    "\n",
    "The most popular form of boosting is that of ***adaptive boosting***. In this case we take a weak classifier, $h(x)$, and create a strong classifier, $f(x)$, as\n",
    "\n",
    "$$ f(x) = \\sum_m^B\\theta_m h_m(x),$$\n",
    "\n",
    "where $m$ is the number of iterations and $\\theta_m$ is the weight of the classifier in each iteration.  \n",
    "\n",
    "*If we chose $\\theta_m=1/B$, then we'd essentially have bagging.* For boosting the idea is to increase the weight of the misclassified data in each step.\n",
    "\n",
    "A fundamental limitation of the boosted decision tree is the computation time for large data sets (they rely on a chain of classifiers which are each dependent on the last), whereas random forests can be easily parallelized as we saw above.\n",
    "\n",
    "The example given below is actually Scikit-Learn's [`GradientBoostingClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) where we approximate the steepest descent criterion after each simple evaluation (more on that next time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "X = np.random.random((100,2))\n",
    "y = (X[:,0] + X[:,1] > 1).astype(int)\n",
    "gradboost = GradientBoostingClassifier()\n",
    "gradboost.fit(X,y)\n",
    "y_pred = gradboost.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 9.16\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from astroML.datasets import fetch_sdss_specgals\n",
    "from astroML.utils.decorators import pickle_results\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=12, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch and prepare the data\n",
    "data = fetch_sdss_specgals()\n",
    "\n",
    "# put magnitudes in a matrix\n",
    "mag = np.vstack([data['modelMag_%s' % f] for f in 'ugriz']).T  # X\n",
    "z = data['z'] # y\n",
    "\n",
    "# train on ~60,000 points\n",
    "mag_train = mag[::10] #X_train\n",
    "z_train = z[::10] #y_train\n",
    "\n",
    "# test on ~6,000 distinct points\n",
    "mag_test = mag[1::100] #X_test\n",
    "z_test = z[1::100] #y_test\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the results\n",
    "#  This is a long computation, so we'll save the results to a pickle.\n",
    "@pickle_results('photoz_boosting.pkl')\n",
    "def compute_photoz_forest(N_boosts):\n",
    "    rms_test = np.zeros(len(N_boosts))\n",
    "    rms_train = np.zeros(len(N_boosts))\n",
    "    i_best = 0\n",
    "    z_fit_best = None\n",
    "\n",
    "    for i, Nb in enumerate(N_boosts):\n",
    "        try:\n",
    "            # older versions of scikit-learn\n",
    "            clf = GradientBoostingRegressor(n_estimators=Nb, learn_rate=0.1,\n",
    "                                            max_depth=3, random_state=0)\n",
    "        except TypeError:\n",
    "            clf = GradientBoostingRegressor(n_estimators=Nb, learning_rate=0.1,\n",
    "                                            max_depth=3, random_state=0)\n",
    "        clf.fit(mag_train, z_train)\n",
    "\n",
    "        z_fit_train = clf.predict(mag_train)\n",
    "        z_fit = clf.predict(mag_test)\n",
    "        rms_train[i] = np.mean(np.sqrt((z_fit_train - z_train) ** 2))\n",
    "        rms_test[i] = np.mean(np.sqrt((z_fit - z_test) ** 2))\n",
    "\n",
    "        if rms_test[i] <= rms_test[i_best]:\n",
    "            i_best = i\n",
    "            z_fit_best = z_fit\n",
    "\n",
    "    return rms_test, rms_train, i_best, z_fit_best\n",
    "\n",
    "N_boosts = (10, 100, 200, 300, 400, 500)\n",
    "rms_test, rms_train, i_best, z_fit_best = compute_photoz_forest(N_boosts)\n",
    "best_N = N_boosts[i_best]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.subplots_adjust(wspace=0.25,\n",
    "                    left=0.1, right=0.95,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "# left panel: plot cross-validation results\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(N_boosts, rms_test, '-k', label='cross-validation')\n",
    "ax.plot(N_boosts, rms_train, '--k', label='training set')\n",
    "ax.legend(loc=1)\n",
    "\n",
    "ax.set_xlabel('number of boosts')\n",
    "ax.set_ylabel('rms error')\n",
    "ax.set_xlim(0, 510)\n",
    "ax.set_ylim(0.009,  0.032)\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "\n",
    "ax.text(0.03, 0.03, \"Tree depth: 3\",\n",
    "        ha='left', va='bottom', transform=ax.transAxes)\n",
    "\n",
    "# right panel: plot best fit\n",
    "ax = fig.add_subplot(122)\n",
    "edges = np.linspace(z_test.min(), z_test.max(), 101)\n",
    "H, zs_bins, zp_bins = np.histogram2d(z_test, z_fit_best, bins=edges)\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto', \n",
    "           extent=[zs_bins[0], zs_bins[-1], zs_bins[0], zs_bins[-1]],\n",
    "           cmap=plt.cm.binary)\n",
    "\n",
    "ax.plot([-0.1, 0.4], [-0.1, 0.4], ':k')\n",
    "ax.text(0.04, 0.96, \"N = %i\\nrms = %.3f\" % (best_N, rms_test[i_best]),\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlabel(r'$z_{\\rm true}$')\n",
    "ax.set_ylabel(r'$z_{\\rm fit}$')\n",
    "\n",
    "ax.set_xlim(-0.02, 0.4001)\n",
    "ax.set_ylim(-0.02, 0.4001)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alright, alright but what the @#%! should I use?  <a class=\"anchor\" id=\"six\"></a>\n",
    "\n",
    "A convenient cop-out: no single model can be known in advance to be the best classifier!\n",
    "\n",
    "In general the level of accuracy increases for parametric models as:\n",
    "- <b>naive Bayes</b>, \n",
    "- linear discriminant analysis (LDA),\n",
    "- logistic regression, \n",
    "- linear support vector machines, \n",
    "- quadratic discriminant analysis (QDA),\n",
    "- linear ensembles of linear models. \n",
    "\n",
    "For non-parametric models accuracy increases as:\n",
    "- decision trees\n",
    "- $K$-nearest-neighbor, \n",
    "- neural networks\n",
    "- kernel discriminant analysis,\n",
    "- <b> kernelized support vector machines</b>\n",
    "- <b> random forests</b>\n",
    "- boosting\n",
    "\n",
    "See also Ivezic, Table 9.1.\n",
    "\n",
    "Naive Bayes and its variants are by far the easiest to compute. Linear support vector machines are more expensive, though several fast algorithms exist. Random forests can be easily parallelized. \n",
    "\n",
    "We saw before that Scikit-learn has tools for computing ROC curves, which is implemented as follows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "X = np.random.random((100,2))\n",
    "y = (X[:,0] + X[:,1] > 1).astype(int)\n",
    "\n",
    "gnb = GaussianNB().fit(X,y)\n",
    "y_prob = gnb.predict_proba(X)\n",
    "\n",
    "# Compute precision/recall curve\n",
    "pr, re, thresh = metrics.precision_recall_curve(y, y_prob[:,0])\n",
    "\n",
    "# Compute ROC curve\n",
    "tpr, fpr, thresh = metrics.roc_curve(y, y_prob[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's remember what they had to say:\n",
    "\n",
    "![Ivezic, Figure 9.17](http://www.astroml.org/_images/fig_ROC_curve_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's an example with a different data set.  Here we are trying to distinguish quasars (in black) from stars (in grey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 9.18\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from astroML.utils import split_samples\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import (LinearDiscriminantAnalysis,\n",
    "                                           QuadraticDiscriminantAnalysis)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from astroML.classification import GMMBayes\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=12, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch data and split into training and test samples\n",
    "from astroML.datasets import fetch_dr7_quasar\n",
    "from astroML.datasets import fetch_sdss_sspp\n",
    "\n",
    "quasars = fetch_dr7_quasar()\n",
    "stars = fetch_sdss_sspp()\n",
    "\n",
    "# Truncate data for speed\n",
    "quasars = quasars[::5]\n",
    "stars = stars[::5]\n",
    "\n",
    "# stack colors into matrix X\n",
    "Nqso = len(quasars)\n",
    "Nstars = len(stars)\n",
    "X = np.empty((Nqso + Nstars, 4), dtype=float)\n",
    "\n",
    "X[:Nqso, 0] = quasars['mag_u'] - quasars['mag_g']\n",
    "X[:Nqso, 1] = quasars['mag_g'] - quasars['mag_r']\n",
    "X[:Nqso, 2] = quasars['mag_r'] - quasars['mag_i']\n",
    "X[:Nqso, 3] = quasars['mag_i'] - quasars['mag_z']\n",
    "\n",
    "X[Nqso:, 0] = stars['upsf'] - stars['gpsf']\n",
    "X[Nqso:, 1] = stars['gpsf'] - stars['rpsf']\n",
    "X[Nqso:, 2] = stars['rpsf'] - stars['ipsf']\n",
    "X[Nqso:, 3] = stars['ipsf'] - stars['zpsf']\n",
    "\n",
    "y = np.zeros(Nqso + Nstars, dtype=int)\n",
    "y[:Nqso] = 1\n",
    "\n",
    "# split into training and test sets\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.9, 0.1],\n",
    "                                                     random_state=0)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute fits for all the classifiers\n",
    "def compute_results(*args):\n",
    "    names = []\n",
    "    probs = []\n",
    "\n",
    "    for classifier, kwargs in args:\n",
    "        print(classifier.__name__)\n",
    "        model = classifier(**kwargs)\n",
    "        model.fit(X, y)\n",
    "        y_prob = model.predict_proba(X_test)\n",
    "\n",
    "        names.append(classifier.__name__)\n",
    "        probs.append(y_prob[:, 1])\n",
    "\n",
    "    return names, probs\n",
    "\n",
    "LRclass_weight = dict([(i, np.sum(y_train == i)) for i in (0, 1)])\n",
    "\n",
    "names, probs = compute_results((GaussianNB, {}),\n",
    "                               (LinearDiscriminantAnalysis, {}),\n",
    "                               (QuadraticDiscriminantAnalysis, {}),\n",
    "                               (LogisticRegression,\n",
    "                                dict(class_weight=LRclass_weight)),\n",
    "                               (KNeighborsClassifier,\n",
    "                                dict(n_neighbors=10)),\n",
    "                               (DecisionTreeClassifier,\n",
    "                                dict(random_state=0, max_depth=12,\n",
    "                                     criterion='entropy')),\n",
    "                               (GMMBayes, dict(n_components=3, tol=1E-5,\n",
    "                                               covariance_type='full')))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot results\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, bottom=0.15, top=0.9, wspace=0.25)\n",
    "\n",
    "# First axis shows the data\n",
    "ax1 = fig.add_subplot(121)\n",
    "im = ax1.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=4,\n",
    "                 linewidths=0, edgecolors='none',\n",
    "                 cmap=plt.cm.binary)\n",
    "im.set_clim(-0.5, 1)\n",
    "ax1.set_xlim(-0.5, 3.0)\n",
    "ax1.set_ylim(-0.3, 1.4)\n",
    "ax1.set_xlabel('$u - g$')\n",
    "ax1.set_ylabel('$g - r$')\n",
    "\n",
    "labels = dict(GaussianNB='GNB',\n",
    "              LinearDiscriminantAnalysis='LDA',\n",
    "              QuadraticDiscriminantAnalysis='QDA',\n",
    "              KNeighborsClassifier='KNN',\n",
    "              DecisionTreeClassifier='DT',\n",
    "              GMMBayes='GMMB',\n",
    "              LogisticRegression='LR')\n",
    "\n",
    "# Second axis shows the ROC curves\n",
    "ax2 = fig.add_subplot(122)\n",
    "for name, y_prob in zip(names, probs):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "\n",
    "    fpr = np.concatenate([[0], fpr])\n",
    "    tpr = np.concatenate([[0], tpr])\n",
    "\n",
    "    ax2.plot(fpr, tpr, label=labels[name])\n",
    "\n",
    "ax2.legend(loc=4)\n",
    "ax2.set_xlabel('false positive rate')\n",
    "ax2.set_ylabel('true positive rate')\n",
    "ax2.set_xlim(0, 0.15)\n",
    "ax2.set_ylim(0.6, 1.01)\n",
    "ax2.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lastly for today, add a `precision_recall` plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, 9.18\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from astroML.utils import split_samples\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import _________ ####Complete\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.lda import LDA\n",
    "#from sklearn.qda import QDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from astroML.classification import GMMBayes\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fetch data and split into training and test samples\n",
    "from astroML.datasets import fetch_dr7_quasar\n",
    "from astroML.datasets import fetch_sdss_sspp\n",
    "\n",
    "quasars = fetch_dr7_quasar()\n",
    "stars = fetch_sdss_sspp()\n",
    "\n",
    "# Truncate data for speed\n",
    "quasars = quasars[::5]\n",
    "stars = stars[::5]\n",
    "\n",
    "# stack colors into matrix X\n",
    "Nqso = len(quasars)\n",
    "Nstars = len(stars)\n",
    "X = np.empty((Nqso + Nstars, 4), dtype=float)\n",
    "\n",
    "X[:Nqso, 0] = quasars['mag_u'] - quasars['mag_g']\n",
    "X[:Nqso, 1] = quasars['mag_g'] - quasars['mag_r']\n",
    "X[:Nqso, 2] = quasars['mag_r'] - quasars['mag_i']\n",
    "X[:Nqso, 3] = quasars['mag_i'] - quasars['mag_z']\n",
    "\n",
    "X[Nqso:, 0] = stars['upsf'] - stars['gpsf']\n",
    "X[Nqso:, 1] = stars['gpsf'] - stars['rpsf']\n",
    "X[Nqso:, 2] = stars['rpsf'] - stars['ipsf']\n",
    "X[Nqso:, 3] = stars['ipsf'] - stars['zpsf']\n",
    "\n",
    "y = np.zeros(Nqso + Nstars, dtype=int)\n",
    "y[:Nqso] = 1\n",
    "\n",
    "# split into training and test sets\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.9, 0.1],\n",
    "                                                     random_state=0)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute fits for all the classifiers\n",
    "def compute_results(*args):\n",
    "    names = []\n",
    "    probs = []\n",
    "\n",
    "    for classifier, kwargs in args:\n",
    "        print(classifier.__name__)\n",
    "        model = classifier(**kwargs)\n",
    "        model.fit(X, y)\n",
    "        y_prob = model.predict_proba(X_test)\n",
    "\n",
    "        names.append(classifier.__name__)\n",
    "        probs.append(y_prob[:, 1])\n",
    "\n",
    "    return names, probs\n",
    "\n",
    "LRclass_weight = dict([(i, np.sum(y_train == i)) for i in (0, 1)])\n",
    "\n",
    "names, probs = compute_results((GaussianNB, {}),\n",
    "                               (LDA, {}),\n",
    "                               (QDA, {}),\n",
    "                               (LogisticRegression,\n",
    "                                dict(class_weight=LRclass_weight)),\n",
    "                               (KNeighborsClassifier,\n",
    "                                dict(n_neighbors=10)),\n",
    "                               (DecisionTreeClassifier,\n",
    "                                dict(random_state=0, max_depth=12,\n",
    "                                     criterion='entropy')),\n",
    "                               (GaussianMixture, dict(n_components=3, tol=1E-5,\n",
    "                                               covariance_type='full')))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot results\n",
    "fig = plt.figure(figsize=(18, 7))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, bottom=0.15, top=0.9, wspace=0.25)\n",
    "\n",
    "# First axis shows the data\n",
    "ax1 = fig.add_subplot(131)\n",
    "im = ax1.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=4,\n",
    "                 linewidths=0, edgecolors='none',\n",
    "                 cmap=plt.cm.binary)\n",
    "im.set_clim(-0.5, 1)\n",
    "ax1.set_xlim(-0.5, 3.0)\n",
    "ax1.set_ylim(-0.3, 1.4)\n",
    "ax1.set_xlabel('$u - g$')\n",
    "ax1.set_ylabel('$g - r$')\n",
    "\n",
    "labels = dict(GaussianNB='GNB',\n",
    "              LinearDiscriminantAnalysis='LDA',\n",
    "              QuadraticDiscriminantAnalysis='QDA',\n",
    "              KNeighborsClassifier='KNN',\n",
    "              DecisionTreeClassifier='DT',\n",
    "              GaussianMixture='GMMB',\n",
    "              LogisticRegression='LR')\n",
    "\n",
    "# Second axis shows the ROC curves\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "for name, y_prob in zip(names, probs):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    precision, recall, thresholds2 = ____(____, ____) # Complete\n",
    "\n",
    "    fpr = np.concatenate([[0], fpr])\n",
    "    tpr = np.concatenate([[0], tpr])\n",
    "\n",
    "    precision = ___.___(___,___) # Complete\n",
    "    recall = ___.___(___,___) # Complete\n",
    "    \n",
    "    ax2.plot(fpr, tpr, label=labels[name])\n",
    "    ax3.plot(____, ____, label=labels[name]) # Complete\n",
    "\n",
    "ax2.legend(loc=4)\n",
    "ax2.set_xlabel('false positive rate')\n",
    "ax2.set_ylabel('true positive rate')\n",
    "ax2.set_xlim(0, 0.15)\n",
    "ax2.set_ylim(0.6, 1.01)\n",
    "ax2.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "ax3.set_xlim(0.5, 1.01)\n",
    "ax3.set_ylim(0.5, 1.01)\n",
    "ax3.set_xlabel('recall')\n",
    "ax3.set_ylabel('precision')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
