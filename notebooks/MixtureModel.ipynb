{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Mixture Models (GMM)\n",
    "\n",
    "Gordon Richards \n",
    "(2016, 2018, 2020)\n",
    "\n",
    "KDE centers each bin (or kernel rather) at each point.  In a [**mixture model**](https://en.wikipedia.org/wiki/Mixture_model) we don't use a kernel for each data point, but rather we fit for the *locations of the kernels*--in addition to the width.  So a mixture model is sort of a hybrid between a tradtional (fixed bin location/size) histogram and KDE.   Using lots of kernels (maybe even more than the BIC score suggests) may make sense if you just want to provide an accurate description of the data (as in density estimation).  Using fewer kernels makes mixture models more like clustering (later today), where the suggestion is still to use many kernels in order to divide the sample into real clusters and \"background\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gaussians are the most commonly used components for mixture models.  So, the pdf is modeled by a sum of Gaussians:\n",
    "$$p(x) = \\sum_{k=1}^N \\alpha_k \\mathscr{N}(x|\\mu_k,\\Sigma_k),$$\n",
    "where $\\alpha_k$ are the \"mixing coefficients\" with $0\\le \\alpha_k \\le 1$ and $\\sum_{k=1}^N \\alpha_k = 1$.\n",
    "\n",
    "We can solve for the parameters using maximum likelihood analyis as we have discussed previously.\n",
    "However, this can be complicated in multiple dimensions, requiring the use of [**Expectation Maximization (EM)**](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expectation Maximization (ultra simplified version)\n",
    "\n",
    "(Note: all explanations of EM are far more complicated than seems necessary for our purposes, so here is my overly simplified explanation.)\n",
    "\n",
    "This may make more sense in terms of our earlier Bayesian analyses if we write this as \n",
    "$$p(z=c) = \\alpha_k,$$\n",
    "and\n",
    "$$p(x|z=c) = \\mathscr{N}(x|\\mu_k,\\Sigma_k),$$\n",
    "where $z$ is a \"hidden\" variable related to which \"component\" each point is assigned to.\n",
    "\n",
    "In the Expectation step, we hold $\\mu_k, \\Sigma_k$, and $\\alpha_k$ fixed and compute the probability that each $x_i$ belongs to component, $c$.  \n",
    "\n",
    "In the Maximization step, we hold the probability of the components fixed and maximize $\\mu_k, \\Sigma_k,$ and $\\alpha_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that $\\alpha$ is the relative weight of each Gaussian component and not the probability of each point belonging to a specific component.  (Can think of as a 1-D case with 2 Gaussian and 1 background components.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use the following animation to illustrate the process.  \n",
    "\n",
    "We start with a 2-component GMM, where the initial components can be randomly determined.\n",
    "\n",
    "The points that are closest to the centroid of a component will be more probable under that distribution in the \"E\" step and will pull the centroid towards them in the \"M\" step.  Iteration between the \"E\" and \"M\" step eventually leads to convergence.\n",
    "\n",
    "In this particular example, 3 components better describes the data and similarly converges.  Note that the process is not that sensitive to how the components are first initialized.  We pretty much get the same result in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"B36fzChfyGU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A typical call to the [Gaussian Mixture Model](http://scikit-learn.org/stable/modules/mixture.html) algorithm looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X = np.random.normal(size=(1000,2)) #1000  points in 2D\n",
    "gmm = GaussianMixture(3) #three components\n",
    "gmm.fit(X)\n",
    "log_dens = gmm.score(X)\n",
    "BIC = gmm.bic(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start with the 1-D example given using eruption data from \"Old Faithful\" geyser at Yellowstone National Park.  \n",
    "[http://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat](http://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#eruptions: Eruption time in mins\n",
    "#waiting: Waiting time to next eruption\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/faithful.dat', delim_whitespace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Make two \"fancy\" histograms illustrating the distribution of `df['eruptions']` and `df['waiting']` times.  Use `bins=\"freedman\"` and `histtype=\"step\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as ____\n",
    "from astropy.visualization import hist as ____\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "ax = fig.add_subplot(121)\n",
    "fancyhist(___,___,___)\n",
    "plt.xlabel('Eruptions')\n",
    "plt.ylabel('N')\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "fancyhist(___,___,___)\n",
    "plt.xlabel('Waiting')\n",
    "plt.ylabel('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Fit Gaussian Mixtures, first in 1-D\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#First fit Eruptions\n",
    "gmm1 = GaussianMixture(n_components=2) # 2-component gaussian mixture model\n",
    "gmm1.fit(df['eruptions'][:,None]) # Fit step\n",
    "xgrid1 = np.linspace(0, 8, 1000) # Make evaluation grid\n",
    "logprob1 = gmm1.score_samples(xgrid1[:,None]) # Compute log likelihoods on that grid\n",
    "pdf1 = np.exp(logprob1)\n",
    "resp1 = gmm1.predict_proba(xgrid1[:,None]) \n",
    "pdf_individual1 = resp1 * pdf1[:, np.newaxis] # Compute posterior probabilities for each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then fit waiting\n",
    "gmm2 = GaussianMixture(n_components=___)\n",
    "gmm2.fit(___)\n",
    "xgrid2 = np.linspace(30, 120, 1000)\n",
    "logprob2 = gmm2.score_samples(___)\n",
    "pdf2 = np.exp(logprob2)\n",
    "resp2 = gmm2.predict_proba(___)\n",
    "pdf_individual2 = ___ * ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Make plots\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "ax = fig.add_subplot(121)\n",
    "plt.hist(df['eruptions'], bins=6, density=True, histtype='step')\n",
    "plt.plot(xgrid1, pdf_individual1, '--', color='blue')\n",
    "plt.plot(xgrid1, pdf1, '-', color='gray')\n",
    "plt.xlabel(\"Eruptions\")\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "plt.hist(____, bins=9, ____, ___)\n",
    "plt.plot(___, ____, '--', color='blue')\n",
    "plt.plot(___, ____, '-', color='gray')\n",
    "plt.xlabel(\"Waiting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's now do a more complicated 1-D example (Ivezic, Figure 6.8), which compares a Mixture Model to KDE.\n",
    "[Note that the version at astroML.org has some bugs!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# Ivezic, Figure 6.8\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from astropy.visualization import hist\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate our data: a mix of several Cauchy distributions\n",
    "#  this is the same data used in the Bayesian Blocks figure\n",
    "np.random.seed(0)\n",
    "N = 10000\n",
    "mu_gamma_f = [(5, 1.0, 0.1),\n",
    "              (7, 0.5, 0.5),\n",
    "              (9, 0.1, 0.1),\n",
    "              (12, 0.5, 0.2),\n",
    "              (14, 1.0, 0.1)]\n",
    "true_pdf = lambda x: sum([f * stats.cauchy(mu, gamma).pdf(x)\n",
    "                          for (mu, gamma, f) in mu_gamma_f])\n",
    "x = np.concatenate([stats.cauchy(mu, gamma).rvs(int(f * N))\n",
    "                    for (mu, gamma, f) in mu_gamma_f])\n",
    "np.random.shuffle(x)\n",
    "x = x[x > -10]\n",
    "x = x[x < 30]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "fig.subplots_adjust(bottom=0.08, top=0.95, right=0.95, hspace=0.1)\n",
    "N_values = (500, 5000)\n",
    "subplots = (211, 212)\n",
    "k_values = (10, 100)\n",
    "\n",
    "for N, k, subplot in zip(N_values, k_values, subplots):\n",
    "    ax = fig.add_subplot(subplot)\n",
    "    xN = x[:N]\n",
    "    t = np.linspace(-10, 30, 1000)\n",
    "\n",
    "    kde = KernelDensity(0.1, kernel='gaussian')\n",
    "    kde.fit(xN[:, None])\n",
    "    dens_kde = np.exp(kde.score_samples(t[:, None]))\n",
    "\n",
    "    # Compute density via Gaussian Mixtures\n",
    "    # we'll try several numbers of clusters\n",
    "    n_components = np.arange(3, 20)\n",
    "    gmms = [GaussianMixture(n_components=n).fit(xN[:,None]) for n in n_components]\n",
    "    BICs = [gmm.bic(xN[:,None]) for gmm in gmms]\n",
    "    i_min = np.argmin(BICs)\n",
    "    t = np.linspace(-10, 30, 1000)\n",
    "    logprob = gmms[i_min].score_samples(t[:,None])\n",
    "\n",
    "    # plot the results\n",
    "    ax.plot(t, true_pdf(t), ':', color='black', zorder=3,\n",
    "            label=\"Generating Distribution\")\n",
    "    ax.plot(xN, -0.005 * np.ones(len(xN)), '|k', lw=1.5)\n",
    "    ax.plot(t, np.exp(logprob), '-', color='gray',\n",
    "            label=\"Mixture Model\\n(%i components)\" % n_components[i_min])\n",
    "    ax.plot(t, dens_kde, '-', color='black', zorder=3,\n",
    "            label=\"Kernel Density $(h=0.1)$\")\n",
    "\n",
    "    # label the plot\n",
    "    ax.text(0.02, 0.95, \"%i points\" % N, ha='left', va='top',\n",
    "            transform=ax.transAxes)\n",
    "    ax.set_ylabel('$p(x)$')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    if subplot == 212:\n",
    "        ax.set_xlabel('$x$')\n",
    "\n",
    "    ax.set_xlim(0, 20)\n",
    "    ax.set_ylim(-0.01, 0.4001)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot the BIC values and see why it picked that many components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.scatter(n_components,BICs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What do the individual components look like?  Make a plot of those.  Careful with the shapes of the arrays!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(gmms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# See Ivezic, Figure 4.2 for help: http://www.astroml.org/book_figures/chapter4/fig_GMM_1D.html\n",
    "# The index \"10\" is choosing the instance with 13 components.\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "print(len(gmms[10].weights_))\n",
    "logprob =  gmms[10].score_samples(t[:,None])\n",
    "pdf = np.exp(logprob) # Sum of the individual component pdf\n",
    "resp = gmms[10].predict_proba(t[:,None]) # Array of \"responsibilities\" for each component\n",
    "plt.plot(t,resp*pdf[:,None])\n",
    "plt.xlim((0,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's look at the Old Faithful data again, but this time in 2-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.scatter(df['eruptions'],df['waiting'])\n",
    "plt.xlabel('Eruptions')\n",
    "plt.ylabel('Waiting')\n",
    "plt.xlim([1.5,5.3])\n",
    "plt.ylim([40,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we'll fit both features at the same time (i.e., the $x$ and $y$ axes above).  Note that Scikit-Learn can handle Pandas DataFrames without further conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gmm3 = GaussianMixture(n_components=2)\n",
    "gmm3.fit(df[['eruptions','waiting']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once the components have been fit, we can plot the location of the centroids and the \"error\" ellipses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kludge to fix the bug with draw_ellipse in v1.0\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def draw_ellipse(mu, C, scales=[1, 2, 3], ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # find principal components and rotation angle of ellipse\n",
    "    sigma_x2 = C[0, 0]\n",
    "    sigma_y2 = C[1, 1]\n",
    "    sigma_xy = C[0, 1]\n",
    "\n",
    "    alpha = 0.5 * np.arctan2(2 * sigma_xy,\n",
    "                             (sigma_x2 - sigma_y2))\n",
    "    tmp1 = 0.5 * (sigma_x2 + sigma_y2)\n",
    "    tmp2 = np.sqrt(0.25 * (sigma_x2 - sigma_y2) ** 2 + sigma_xy ** 2)\n",
    "\n",
    "    sigma1 = np.sqrt(tmp1 + tmp2)\n",
    "    sigma2 = np.sqrt(tmp1 - tmp2)\n",
    "\n",
    "    for scale in scales:\n",
    "        ax.add_patch(Ellipse((mu[0], mu[1]),\n",
    "                             2 * scale * sigma1, 2 * scale * sigma2,\n",
    "                             alpha * 180. / np.pi,\n",
    "                             **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#See cell above\n",
    "#from astroML.plotting.tools import draw_ellipse\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(df['eruptions'],df['waiting'])\n",
    "plt.xlabel('Eruptions')\n",
    "plt.ylabel('Waiting')\n",
    "plt.xlim([1.5,5.3])\n",
    "plt.ylim([40,100])\n",
    "\n",
    "ax.scatter(gmm3.means_[:,0], gmm3.means_[:,1], marker='s', c='red', s=80)\n",
    "for mu, C, w in zip(gmm3.means_, gmm3.covariances_, gmm3.weights_):\n",
    "    draw_ellipse(mu, 2*C, scales=[1], ax=ax, fc='none', ec='k') #2 sigma ellipses for each component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ivezic, Figure 6.6 shows another 2-D example.  In the first panel, we have the raw data.  In the second panel we have a density plot (essentially a 2-D histogram).  We then try to represent the data with a series of Gaussians.  We allow up to 14 Gaussians and use the AIC/BIC to determine the best choice for this number.  This is shown in the third panel.  Finally, the fourth panel shows the chosen Gaussians with their centroids and 1-$\\sigma$ contours.\n",
    "\n",
    "In this case 7 components are required for the best fit.  While it looks like we could do a pretty good job with just 2 components, there does appear to be some \"background\" that is a high enough level to justify further components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# Ivezic, Figure 6.6\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "#from sklearn.mixture import GMM\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from astroML.datasets import fetch_sdss_sspp\n",
    "from astroML.decorators import pickle_results\n",
    "#See cells above\n",
    "#from astroML.plotting.tools import draw_ellipse\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get the Segue Stellar Parameters Pipeline data\n",
    "data = fetch_sdss_sspp(cleaned=True)\n",
    "\n",
    "# Note how X was created from two columns of data\n",
    "X = np.vstack([data['FeH'], data['alphFe']]).T\n",
    "\n",
    "# truncate dataset for speed\n",
    "X = X[::5]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute GMM models & AIC/BIC\n",
    "N = np.arange(1, 14)\n",
    "\n",
    "#@pickle_results(\"GMM_metallicity.pkl\")\n",
    "def compute_GMM(N, covariance_type='full', n_iter=1000):\n",
    "    models = [None for n in N]\n",
    "    for i in range(len(N)):\n",
    "        #print N[i]\n",
    "        models[i] = GMM(n_components=N[i], n_iter=n_iter, covariance_type=covariance_type)\n",
    "        #models[i] = GaussianMixture(n_components=N[i], max_iter=n_iter, covariance_type=covariance_type)\n",
    "        models[i].fit(X)\n",
    "    return models\n",
    "\n",
    "models = compute_GMM(N)\n",
    "\n",
    "AIC = [m.aic(X) for m in models]\n",
    "BIC = [m.bic(X) for m in models]\n",
    "\n",
    "i_best = np.argmin(BIC)\n",
    "gmm_best = models[i_best]\n",
    "print(\"best fit converged:\", gmm_best.converged_)\n",
    "print(\"BIC: n_components =  %i\" % N[i_best])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute 2D density\n",
    "FeH_bins = 51\n",
    "alphFe_bins = 51\n",
    "H, FeH_bins, alphFe_bins = np.histogram2d(data['FeH'], data['alphFe'], (FeH_bins, alphFe_bins))\n",
    "\n",
    "Xgrid = np.array(map(np.ravel,\n",
    "                     np.meshgrid(0.5 * (FeH_bins[:-1]\n",
    "                                        + FeH_bins[1:]),\n",
    "                                 0.5 * (alphFe_bins[:-1]\n",
    "                                        + alphFe_bins[1:])))).T\n",
    "log_dens = gmm_best.score(Xgrid).reshape((51, 51))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "fig.subplots_adjust(wspace=0.45, bottom=0.25, top=0.9, left=0.1, right=0.97)\n",
    "\n",
    "# plot data\n",
    "ax = fig.add_subplot(141)\n",
    "ax.scatter(data['FeH'][::10],data['alphFe'][::10],marker=\".\",color='k',edgecolors='None')\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.text(0.93, 0.93, \"Input\",\n",
    "        va='top', ha='right', transform=ax.transAxes)\n",
    "\n",
    "# plot density\n",
    "ax = fig.add_subplot(142)\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.text(0.93, 0.93, \"Density\",\n",
    "        va='top', ha='right', transform=ax.transAxes)\n",
    "\n",
    "# plot AIC/BIC\n",
    "ax = fig.add_subplot(143)\n",
    "ax.plot(N, AIC, '-k', label='AIC')\n",
    "ax.plot(N, BIC, ':k', label='BIC')\n",
    "ax.legend(loc=1)\n",
    "ax.set_xlabel('N components')\n",
    "plt.setp(ax.get_yticklabels(), fontsize=7)\n",
    "\n",
    "# plot best configurations for AIC and BIC\n",
    "ax = fig.add_subplot(144)\n",
    "ax.imshow(np.exp(log_dens),\n",
    "          origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "\n",
    "ax.scatter(gmm_best.means_[:, 0], gmm_best.means_[:, 1], c='w')\n",
    "for mu, C, w in zip(gmm_best.means_, gmm_best.covars_, gmm_best.weights_):\n",
    "    draw_ellipse(mu, C, scales=[1], ax=ax, fc='none', ec='k')\n",
    "\n",
    "ax.text(0.93, 0.93, \"Converged\",\n",
    "        va='top', ha='right', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.set_ylim(alphFe_bins[0], alphFe_bins[-1])\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That said, I'd say that there are *too* many components here.  So, I'd be inclined to explore this a bit further if it were my data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Talk about how to use this to do outlier finding.  Convolve with errors of unknown object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lastly, let's look at a 2-D case where we are using GMM more to characterize the data than to find clusters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# Ivezic, Figure 6.7\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.mixture import GMM\n",
    "from astroML.datasets import fetch_great_wall\n",
    "from astroML.decorators import pickle_results\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# load great wall data\n",
    "X = fetch_great_wall()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Create a function which will save the results to a pickle file\n",
    "#  for large number of clusters, computation will take a long time!\n",
    "#@pickle_results('great_wall_GMM.pkl')\n",
    "def compute_GMM(n_clusters, n_iter=1000, min_covar=3, covariance_type='full'):\n",
    "    clf = GMM(n_clusters, covariance_type=covariance_type,\n",
    "              n_iter=n_iter, min_covar=min_covar)\n",
    "    clf.fit(X)\n",
    "    print \"converged:\", clf.converged_\n",
    "    return clf\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute a grid on which to evaluate the result\n",
    "Nx = 100\n",
    "Ny = 250\n",
    "xmin, xmax = (-375, -175)\n",
    "ymin, ymax = (-300, 200)\n",
    "\n",
    "Xgrid = np.vstack(map(np.ravel, np.meshgrid(np.linspace(xmin, xmax, Nx),\n",
    "                                            np.linspace(ymin, ymax, Ny)))).T\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the results\n",
    "#\n",
    "# we'll use 100 clusters.  In practice, one should cross-validate\n",
    "# with AIC and BIC to settle on the correct number of clusters.\n",
    "clf = compute_GMM(n_clusters=100)\n",
    "log_dens = clf.score(Xgrid).reshape(Ny, Nx)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.subplots_adjust(hspace=0, left=0.08, right=0.95, bottom=0.13, top=0.9)\n",
    "\n",
    "ax = fig.add_subplot(211, aspect='equal')\n",
    "ax.scatter(X[:, 1], X[:, 0], s=1, lw=0, c='k')\n",
    "\n",
    "ax.set_xlim(ymin, ymax)\n",
    "ax.set_ylim(xmin, xmax)\n",
    "\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "plt.ylabel(r'$x\\ {\\rm (Mpc)}$')\n",
    "\n",
    "ax = fig.add_subplot(212, aspect='equal')\n",
    "ax.imshow(np.exp(log_dens.T), origin='lower', cmap=plt.cm.binary,\n",
    "          extent=[ymin, ymax, xmin, xmax])\n",
    "ax.set_xlabel(r'$y\\ {\\rm (Mpc)}$')\n",
    "ax.set_ylabel(r'$x\\ {\\rm (Mpc)}$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that this is very different than the non-parametric density estimates.  The advantage is that we now have a *model*.  This model can be stored very compactly with just a few numbers, unlike the KDE or KNN maps which require a floating point number for each grid point.  \n",
    "\n",
    "One thing that you might imagine doing with this is subtracting the model from the data and looking for interesting things among the residuals."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
